{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "extracting_pdf_text.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOt0tGsohPKgJVokDOTNg9H",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhavya2810/Bhavya_INFO5731/blob/master/extracting_pdf_text.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRHUG86sXA8H",
        "outputId": "d2be6b93-69bd-4eb4-e9a5-fccad0cbbbb4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "pip install pdfminer"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pdfminer\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/a3/155c5cde5f9c0b1069043b2946a93f54a41fd72cc19c6c100f6f2f5bdc15/pdfminer-20191125.tar.gz (4.2MB)\n",
            "\u001b[K     |████████████████████████████████| 4.2MB 2.8MB/s \n",
            "\u001b[?25hCollecting pycryptodome\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2b/6f/7e38d7c97fbbc3987539c804282c33f56b6b07381bf2390deead696440c5/pycryptodome-3.9.9-cp36-cp36m-manylinux1_x86_64.whl (13.7MB)\n",
            "\u001b[K     |████████████████████████████████| 13.7MB 307kB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pdfminer\n",
            "  Building wheel for pdfminer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pdfminer: filename=pdfminer-20191125-cp36-none-any.whl size=6140077 sha256=eb20af7bffe5d649ef8e22be7442680b89529a79e2e73b39f31f5c9e21504abd\n",
            "  Stored in directory: /root/.cache/pip/wheels/e1/00/af/720a55d74ba3615bb4709a3ded6dd71dc5370a586a0ff6f326\n",
            "Successfully built pdfminer\n",
            "Installing collected packages: pycryptodome, pdfminer\n",
            "Successfully installed pdfminer-20191125 pycryptodome-3.9.9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JxqCZVbGXByw"
      },
      "source": [
        "import io\n",
        "from pdfminer.converter import TextConverter\n",
        "from pdfminer.pdfinterp import PDFPageInterpreter\n",
        "from pdfminer.pdfinterp import PDFResourceManager\n",
        "from pdfminer.pdfpage import PDFPage\n",
        "def extract_text_by_page(pdf_path):\n",
        "    with open(pdf_path, 'rb') as fh:\n",
        "        for page in PDFPage.get_pages(fh, \n",
        "                                      caching=True,\n",
        "                                      check_extractable=True):\n",
        "            resource_manager = PDFResourceManager()\n",
        "            fake_file_handle = io.StringIO()\n",
        "            converter = TextConverter(resource_manager, fake_file_handle)\n",
        "            page_interpreter = PDFPageInterpreter(resource_manager, converter)\n",
        "            page_interpreter.process_page(page)\n",
        "            text = fake_file_handle.getvalue()\n",
        "            yield text\n",
        "            # close open handles\n",
        "            converter.close()\n",
        "            fake_file_handle.close()\n",
        "\n",
        "def extract_text(pdf_path):\n",
        "    text = \"\"\n",
        "    for page in extract_text_by_page(pdf_path):\n",
        "         text += page\n",
        "    return text\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  pdf_text = extract_text('/content/2020.acl-main.1.pdf')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVEWlJh8XjqO",
        "outputId": "cce6346b-3745-4dc4-a4c0-decb65c742ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        }
      },
      "source": [
        "pdf_text"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Proceedingsofthe58thAnnualMeetingoftheAssociationforComputationalLinguistics,pages1–6July5-10,2020.c(cid:13)2020AssociationforComputationalLinguistics1LearningtoUnderstandChild-directedandAdult-directedSpeechLiekeGelderloosTilburgUniversityl.j.gelderloos@uvt.nlGrzegorzChrupałaTilburgUniversityg.chrupala@uvt.nlAfraAlishahiTilburgUniversitya.alishahi@uvt.nlAbstractSpeechdirectedtochildrendiffersfromadult-directedspeechinlinguisticaspectssuchasrepetition,wordchoice,andsentencelength,aswellasinaspectsofthespeechsignalit-self,suchasprosodicandphonemicvaria-tion.Humanlanguageacquisitionresearchindicatesthatchild-directedspeechhelpslan-guagelearners.Thisstudyexplorestheef-fectofchild-directedspeechwhenlearningtoextractsemanticinformationfromspeechdi-rectly.Wecomparethetaskperformanceofmodelstrainedonadult-directedspeech(ADS)andchild-directedspeech(CDS).Weﬁndin-dicationsthatCDShelpsintheinitialstagesoflearning,buteventually,modelstrainedonADSreachcomparabletaskperformance,andgeneralizebetter.Theresultssuggestthatthisisatleastpartiallyduetolinguisticratherthanacousticpropertiesofthetworegisters,asweseethesamepatternwhenlookingatmodelstrainedonacousticallycomparablesyntheticspeech.1IntroductionSpeechdirectedtochildren(CDS)differsfromadult-directedspeech(ADS)inmanyaspects.Lin-guisticdifferencesincludethenumberofwordsperutterance,withutterancesinCDSbeingconsider-ablyshorterthanutterancesinADS,andrepetition,whichismorecommoninchild-directedspeech.Therearealsoparalinguistic,acousticfactorsthatcharacterizechild-directedspeech:peoplespeak-ingtochildrentypicallyuseahigherpitchandexaggeratedintonation.IthasbeenarguedthatthepropertiesofCDShelpperceptionorcomprehension.Kuhletal.(1997)proposethatCDSisoptimizedforlearn-ability.Optimallearnabilitymay,butdoesnotnecessarilyalignwithoptimizationforperceptionorcomprehension.Althoughspeechwithlowervariabilitymaybeeasiesttolearntounderstand,highervariabilitymayprovidemorelearningoppor-tunities,leadingtomorecompletelanguageknowl-edge.Inthispaper,weexplorehowlearningtoextractmeaningfromspeechdifferswhenlearningfromCDSandADS.Wediscusstaskperformanceonthetrainingregisteraswellasgeneralizationacrossregisters.Toteaseaparttheeffectofacousticandlinguisticdifferences,wealsoreportonmodelstrainedonsynthesizedspeech,inwhichlinguisticdifferencesbetweentheregistersareretained,buttheacousticpropertiesaresimilar.2Relatedwork2.1ChilddirectedspeechandlearnabilityThecharacteristicsofchild-directedspeechareamajortopicofstudyinlanguageacquisitionre-search.Foracomprehensiveoverview,seeSoder-strom(2007)andClark(2009,Ch.2,p.32-41).Withregardstoacoustics,CDSisreportedtohaveexaggeratedintonationandaslowerspeechrate(Fernaldetal.,1989).Kuhletal.(1997)showthatCDScontainsmore‘extreme’realizationsofvowels.McMurrayetal.(2013)showthattheseincreasedmeansareaccompaniedbyincreasedvariance,andarguethatanylearningadvantageofCDSduetoextremevowelrealizationsiscoun-teractedbyincreasedvariance.However,ithasalsobeenarguedthatincreasedvariancemaybebeneﬁcialtolearninginthelongrun,asitgivesthelearneramorecompletesetofexamplesforacategory,whichhelpsgeneralization.Guevara-Rukozetal.(2018)showthatwordformsinchild-directedspeechareacousticallymorediverse.Attheutterancelevel,child-directedlanguagecon-sistsofshortersentencesandsimplersyntax(New-portetal.,1977;Fernaldetal.,1989),andwordsmoreoftenappearinisolation(RatnerandRooney,2001).\\x0c2Studiesonhomerecordingsshowthattheavail-abilityofCDSinputaccountsfordifferencesinvocabularygrowthbetweenlearners,whereasover-heardspeechisunrelated(Hoff,2003;WeislederandFernald,2013).ThisdoesnotnecessarilymeanthatitiseasiertolearnfromCDS.PsycholinguisticresearchhasshownthatinfantsacrosstheworldshowaCDSpreference,payingmoreattentiontoitthantoADS(ManyBabiesConsortium,2020).LearningadvantagesofCDSinchildrenmaythere-foresimplybebecausetheygrantitmoreattention,ratherthantopropertiesofCDSthatareadvanta-geousforlearning.Computationalmodels,how-ever,havenochoiceinwheretheyallocateatten-tion.AnylearningadvantagesweﬁndofeitherADSorCDSincomputationalstudiesmustbeduetopropertiesthatmakespeechinthatregistermorelearnabletothemodel.Therehasbeensomecomputationalworkcom-paringlearningfromADSandCDSatthelevelofwordlearningandphoneticlearning.Studiesonsegmentabilityusealgorithmsthatlearntoidentifywordunits,withsomestudiesreportinghigherseg-mentabilityforCDS(Batchelder,2002;DalandandPierrehumbert,2011),whileCristiaetal.(2019)re-portmixedresults.KirchhoffandSchimmel(2005)trainHMM-basedspeechrecognitionsystemsonCDSandADS,andtestonmatchedandcrossedtestsets.TheyﬁndthatbothADSandCDStrainedsystemsperformbestonthematchingtestset,butCDStrainedsystemsperformbetteronADSthansystemstrainedonADSpeformonCDS.TheyshowthatthisislikelycausedbyphoneticclasseshavelargeroverlapsinCDS.Totheauthors’knowledge,thecurrentworkistheﬁrsttocomputationallyexplorelearnabilitydif-ferencesbetweenADSandCDSconsideringtheprocessofspeechcomprehensionasawhole:fromaudiotosemanticinformation.2.2Speechrecognitionwithnon-linguisticsupervisionInrecentyears,severalstudieshaveworkedonmachinelearningtasksinwhichmodelsdirectlyextractsemanticinformationfromspeech,withoutfeedbackontheword,character,orphonemelevel.Mostprominently,workon‘weaklysupervised’speechrecognitionincludesworkinwhichaccom-panyingvisualinformationisusedasaproxyforsemanticinformation.Bygroundingspeechinvi-sualinformationaccompanyingit,modelscanlearntoextractvisuallyrelevantsemanticinformationfromspeech,withoutneedingsymbolicannotation(Harwathetal.,2016;HarwathandGlass,2017;Chrupałaetal.,2017;Merkxetal.,2019).Thetopicisofinterestforautomaticspeechrecognition,asitprovidespotentialwaysoftrain-ingspeechrecognitionwithouttheneedforvastamountsofannotation.Theutilizationofnon-linguisticinformationassupervisionisparticularlyusefulforlow-resourcelanguages.Forthepurposeofthisstudy,however,weareinterestedinthissetofproblemsbecauseoftheparalleltohumanlanguageacquisition.Alanguagelearningchilddoesnotreceiveexplicitfeedbackonthewordsorphonemesitperceives.Rather,theylearntoinferthesestructuralpropertiesoflanguage,withattheirdisposalonlythespeechsignalitselfanditsweakandmessylinkstotheouterworld.3TaskThetaskistomatchspeechtoasemanticrepre-sentationofthelanguageitcontains,intuitively‘grounding’ittothesemanticcontext.Thedesignofthistaskisinspiredbyworkinvisualgrounding.However,theavailabilityofCDSdataaccompa-niedbyvisualdataisverylimited.Insteadofvisualrepresentation,weusesemanticsentenceembed-dingsofthetranscriptions.Ratherthantrainingourmodeltoimaginethevisualcontextaccompanyinganutterance,asinvisualgrounding,wetrainittoimaginethesemanticcontent.Notethatsincethesemanticembeddingsarebasedonthetranscrip-tionsofthesentencesthemselves,theyhaveamuchcloserrelationtothesentencesthanvisualcontextrepresentationswouldhave.Thesemanticsentencerepresentationswereob-tainedusingSBERT,aBERT-basedarchitecturethatyieldssentenceembeddings,whichwasﬁne-tunedontheSTSbenchmarkofSemEval(ReimersandGurevych,2019).ThisparticularencodingwaschosenbecauseitharnessesthesemanticstrengthofBERT(Devlinetal.,2019)inanencodingofthesentenceasawhole.SpeechisconvertedMel-frequencycepstrumcoefﬁcients.4Data4.1Naturalspeech:NewmanRatnercorpusSinceweareinterestedintheeffectoflearningfromchild-versusadultdirectedspeech,wese-lectdatathatdiffersinregister,butisotherwiseascomparableaspossible.TheNewmanRatner\\x0c3DatasetCDSADSVocabularysize3,1705,665Totalnr.ofwords97,118203,084Type/tokenratio.033.028Wordsperutterance4.529.46Utterancelengthinseconds3.373.46Wordspersecond1.342.74Table1:Descriptivestatisticsofthedatacorpuscontainsannotatedrecordingsofcaregiversinconversationwiththeirchildrenandwithexper-imenters(Newmanetal.,2016).Thisdatasetissuitabletoourset-up,asitcontainsareasonableamountoftranscribedCDSandADSbythesamespeakers,whichisrare;anditisinEnglish,forwhichpretrainedstate-of-the-artlanguagemodelssuchas(S)BERT(Devlinetal.,2019;ReimersandGurevych,2019)arereadilyavailable.Child-directedspeechintheNewmanRatnercor-pustakesplaceinfreeplaybetweencaregiverandchild,whereasadult-directedspeechisutteredinthecontextofaninterview.Stretchesofspeechhavebeentranscribedcontainingoneormoreutter-ances.Weselectedonlyutterancesbycaregiversandexcludedsegmentswithmultiplespeakers.AstheCDSportionofthecorpusislargerthantheADSportion,werandomlyselected21,465CDSsegments,matchingthenumberofADSsegmentsbycaregivers.Validationandtestsetsof1,000seg-mentswereheldout,whiletheremaining19,465segmentswereusedfortraining.Table1listssomecharacteristicstatisticsoftheCDSandADSsamplesthatwereused.TheADSsamplecontainsalargervocabularythantheCDSsample.Onaverage,ADSsegmentscontainmorethantwiceasmanywords,althoughtheyareonly88millisecondslongeronaverage.Therefore,thenumberofwordspersecondistwiceashighinADSasitisinCDS.4.2SyntheticspeechToteaseaparteffectsoftheacousticpropertiesofspeechandpropertiesofthelanguageitself,werepeattheexperimentusingsynthesizedversionoftheADSandCDScorpora.Forthisvariant,wefeedthetranscriptionstotheGoogletext2speechAPI,usingthe6availableUSEnglishWaveNetvoices(vandenOordetal.,2016).Notethatthesyntheticspeechismuchcleanerthanthenaturalspeech,whichwasrecordedusingamicrophoneattachedtoclothingofthecaregiver,andcontainsalotofsilence,noise,andﬂuctuationsinvolumeofthespeech.SincesyntheticspeechforADSandCDSisgen-eratedusingthesamepipeline,theacousticproper-tiesofthesesamplesarecomparable,butlinguisticdifferencesbetweenthemareretained.Differencesremaininthevocabularysize,numberofwordsperutteranceandtypetokenratio,butthenumberofwordspersecondisnowcomparable.ThismeansthelengthofutterancesismuchlargerforsyntheticADSsentences,sincetheaverageADSsentencecontainsapproximatelytwiceasmanywordsastheaverageCDSsentence.5ModelThemodelandtrainingset-upisbasedonMerkxetal.(2019).Thismodelissuitedtoourtask,asitallowstolearntoextractsemanticinformationfromspeechbygroundingitinanothermodality,withoutrequiringthespeechtobesegmented.Thespeechencodercomprisesaconvolutionalﬁlteroverthespeechinput,feedingintoastackof4bidirectional-GRUlayersfollowedbyanattentionoperator.Thedifferenceinourset-upistheuseofSBERTsentenceembeddingsinsteadofvisualfeaturevectors.Usingamarginloss,themodelistrainedtomakethecosinedistancebetweentruepairsofspeechsegmentsandSBERTembeddingssmallerthanthatbetweenrandomcounterparts.Wetrainfor50epochsandfollowingMerkxetal.(2019)weuseacycliclearningrateschedule.16Results6.1PerformanceTrainedmodelsareevaluatedbyrankingallSBERTembeddingsinthetestsetbycosinedistancetospeechencodings.Reportedmetricsarerecall@1,recall@5,andrecall@10,whicharetheproportionofcasesinwhichthecorrectSBERTembeddingisamongthetop1,5,or10mostsimilarones;andthemedianrankofthecorrectSBERTembedding.Testresultsarereportedforthetrainingepochforwhichrecall@1ishighestonvalidationdata.Wehavetrained3differentlyrandomlyinitializedrunsforallfourdatasets,andreporttheaveragescoresonthetestsplitofthedatasetthemodelwastrainedon,aswellasitsCDSorADScounterpart,anda1CodeisavailablethroughGithub:https://github.com/lgelderloos/cdsads\\x0c4ModeltrainedonCDSTestsetMed.r.R@1R@5R@10CDS4.67.28.52.61ADS52.50.08.19.26Combined30.67.15.30.37ModeltrainedonADSTestsetMed.r.R@1R@5R@10CDS37.83.10.24.33ADS5.00.29.51.61Combined20.83.17.32.40Table2:TestperformanceofmodelstrainedonnaturalspeechModeltrainedonsyntheticCDSTestsetMed.r.R@1R@5R@10CDS1.00.82.96.99ADS1.00.59.79.86Combined1.00.68.85.90ModeltrainedonsyntheticADSTestsetMed.r.R@1R@5R@10CDS1.00.70.89.95ADS1.00.84.94.97Combined1.00.74.89.93Table3:Testperformanceofmodelstrainedonsyn-theticspeechcombinedtestset,whichissimplytheunionofthetwo.Ascanbeobservedintable2,onthecom-binedtestset,modelstrainedonadultdirectedspeechslightlyoutperformmodelstrainedonchild-directedspeech.However,modelsinthetworeg-istersperformverysimilarlywhenwetestthemonthetestsetinthesameregister,withADShav-inghigherrecall@1,butCDSscoringbetterontheothermetrics.WhenwetestADSmodelsonCDS,performanceislowerthanthatofmodelsthathavebeentrainedonCDS.However,thedroponADSbetweenmodelstrainedonADSandmodelstrainedonCDSisevenlarger.Thebetterperfor-manceonthecombinedtestset,then,seemstocomefromADSmodelsgeneralizingbettertoCDSthantheotherwayaround.Generalperformanceofallmodelstrainedandtestedonsyntheticspeech,whichismuchcleanerthanthenaturalspeechandmoresimilaracrossreg-isters,ismuchhigherthanperformanceonnaturalspeech(seetable3).However,thesamepatterncanbeobserved:onthecombinedtestset,ADS02468100.00.20.40.60.81.0ADSCDSRecall@1Recall@5Recall@10Figure1:Validationperformanceinearlytrainingonnaturalspeech02468100.00.20.40.60.81.0ADSCDSRecall@1Recall@5Recall@10Figure2:ValidationperformanceinearlytrainingonsyntheticspeechmodelsperformbetterthanCDSmodels.Whentestedontheregistertheyweretrainedon,themod-elsperformsimilarly,butmodelstrainedonADSperformbetterwhentestedonCDSthantheotherwayaround.Tosummarize,modelstrainedonADSandCDSreachcomparablescoreswhenevaluatedonthesameregistertheyaretrainedon.However,trainingonADSleadstoknowledgethatgeneralizesbetterthantrainingonCDSdoes.Thispatternholdsevenwhentrainingandevaluatingonsyntheticspeech,whenthetworegistersareacousticallysimilar.6.2LearningtrajectoriesLearnabilityisnotjustabouteventualattainment:itisalsoabouttheprocessoflearningitself.Al-thoughADSandCDSmodelseventuallyperformsimilarly,thisisnotnecessarilythecaseduringthetrainingprocess.Figures1and2showthetra-jectoryofrecallperformanceonthevalidationsetaftertheﬁrst10epochsoftraining.Duringtheseearlystagesoflearning,themodelstrainedonADS(dottedlines)areoutperformedbythosetrainedon\\x0c5CDS(solidlines).Thispatternismorepronouncedinthemodelstrainedonsyntheticspeech,butalsopresentformodelstrainedonnaturalspeech.Afterﬁveepochsoftraining,averagerecall@1is0.12forCDSmodelsand0.09forADSmodels.Formodelstrainedonsyntheticspeech,averagerecall@1onvalidationdatais0.51forADSmodelsand0.59forCDSmodels.Inlaterstagesoftraining,modelstrainedonADSoutperformCDSmodelsonvalida-tiondata.Atepoch40,closetotheoptimallyper-formingepochformostmodels,averagerecall@1is0.31forADSmodelsand0.28forCDSmodels,and0.86and0.81forthesyntheticcounterparts,respectively.Althoughmodelstrainedonadult-directedspeecheventuallycatchupwithmodelstrainedonchild-directedspeech,CDSmodelslearnmorequicklyatthestart.7DiscussionWeﬁndindicationsthatlearningtoextractmean-ingfromspeechisinitiallyfasterwhenlearningfromchild-directedspeech,butlearningfromadult-directedspeecheventuallyleadstosimilartaskper-formanceonthetrainingregister,andbettergener-alizationtotheotherregister.Theeffectispresentbothinmodelstrainedonnaturalspeechandinmodelstrainedonsyntheticspeech,suggestingthatitisatleastpartlyduetodifferencesinthelanguageitself,ratherthanacousticpropertiesofthespeechregister.OurﬁndingthatmodelstrainedonADSgen-eralizebettertoCDSthantheotherwayaroundcontrastswiththeﬁndingsofKirchhoffandSchim-mel(2005).OurresultsareincontrasttotheideathatCDSisoptimizedforleadingtothemostvalu-ableknowledge,asitisthemodelstrainedonADSthatleadtobettergeneralization.OurﬁndingthatlearningisinitiallyfasterforCDSismoreinlinewiththeideaoflearnabilityas‘easytolearn’.ThebettergeneralizationofmodelstrainedonADSmaybeduetoADShavinghigherlexicalandsemanticvariability,reﬂectedinthelargervocab-ularyandhighernumberofwordsperutterance.Sincethereissimplymoretolearn,learningtoper-formthetaskismoredifﬁcultonADS,butitleadstomorevaluableknowledge.ItisalsopossiblethatSBERTisbettersuitedtoencodethesemanticcontentofADS,asADSuterrancesarelikelytobemoresimilartothesentencesSBERTwastrainedonthanCDSutterancesare.Wemustbeprudentindrawingconclusionsfromtheapparenteffectsweseeinthisstudy,asthere-sultsondifferentdatasetscannotbeinterpretedasbeingonthesamescale.Althoughallmetricsarebasedonarankofthesamenumberofcompeti-tors,thedistributionofsimilaritiesanddifferencesbetweenthesemanticrepresentationsofthesecom-petitorsmaydifferacrossdatasets.Thecombinedtestsetscoresaremoredirectlycomparable,butideally,wewouldliketocomparethegeneraliza-tionofbothmodelsonanindependenttestset.Infuturework,weintendtocurateatestsetwithdatafromseparatesources,whichcanserveasabenchmarkforthemodelswestudy.Wein-tendtoexplorehowacurriculumofCDSfollowedbyADSaffectslearningtrajectoriesandoutcomes.Wealsointendtousetoolsforinterpretingtheknowledgeencodedinneuralnetworks(suchasdi-agnosticclassiﬁersandrepresentationalsimilarityanalysis)toinvestigatetheemergentrepresentationoflinguisticunitssuchasphonemesandwords.ReferencesEleanorO.Batchelder.2002.Bootstrappingthelexi-con:Acomputationalmodelofinfantspeechseg-mentation.Cognition,83(2):167–206.GrzegorzChrupała,LiekeGelderloos,andAfraAl-ishahi.2017.Representationsoflanguageinamodelofvisuallygroundedspeechsignal.InPro-ceedingsofthe55thAnnualMeetingoftheAssocia-tionforComputationalLinguistics(Volume1:LongPapers),pages613–622.EveV.Clark.2009.Firstlanguageacquisition,2ndedition.CambridgeUniversityPress,Cambridge.AlejandrinaCristia,EmmanuelDupoux,NanBern-steinRatner,andMelanieSoderstrom.2019.Seg-mentabilitydifferencesbetweenchild-directedandadult-directedspeech:Asystematictestwithaneco-logicallyvalidcorpus.OpenMind,3:13–22.RobertDalandandJanetB.Pierrehumbert.2011.Learningdiphone-basedsegmentation.Cognitivescience,35(1):119–155.JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova.2019.BERT:Pre-trainingofdeepbidirectionaltransformersforlanguageunder-standing.InProceedingsofthe2019ConferenceoftheNorthAmericanChapteroftheAssociationforComputationalLinguistics:HumanLanguageTech-nologies,Volume1(LongandShortPapers),pages4171–4186.AnneFernald,TrauteTaeschner,JudyDunn,MechthildPapousek,B´en´edictedeBoysson-Bardies,andIkukoFukui.1989.Across-language\\x0c6studyofprosodicmodiﬁcationsinmothers’andfathers’speechtopreverbalinfants.Journalofchildlanguage,16(3):477–501.AdrianaGuevara-Rukoz,AlejandrinaCristia,Bog-danLudusan,RolandThiolli`ere,AndrewMartin,ReikoMazuka,andEmmanuelDupoux.2018.Arewordseasiertolearnfrominfant-thanadult-directedspeech?aquantitativecorpus-basedinvestigation.CognitiveScience,42(5):1586–1617.DavidHarwathandJamesGlass.2017.Learningword-likeunitsfromjointaudio-visualanalysis.InPro-ceedingsofthe55thAnnualMeetingoftheAssocia-tionforComputationalLinguistics(Volume1:LongPapers),pages506–517.DavidHarwath,AntonioTorralba,andJamesGlass.2016.Unsupervisedlearningofspokenlanguagewithvisualcontext.InAdvancesinNeuralInfor-mationProcessingSystems29,pages1858–1866.ErikaHoff.2003.Thespeciﬁcityofenvironmentalin-ﬂuence:Socioeconomicstatusaffectsearlyvocabu-larydevelopmentviamaternalspeech.ChildDevel-opment,74(5):1368–1378.KatrinKirchhoffandStevenSchimmel.2005.Sta-tisticalpropertiesofinfant-directedversusadult-directedspeech:Insightsfromspeechrecognition.TheJournaloftheAcousticalSocietyofAmerica,117(4):2238–2246.PatriciaK.Kuhl,JeanE.Andruski,InnaA.Chistovich,LudmillaA.Chistovich,ElenaV.Kozhevnikova,ViktoriaL.Ryskina,ElviraI.Stolyarova,UllaSund-berg,andFranciscoLacerda.1997.Cross-languageanalysisofphoneticunitsinlanguageaddressedtoinfants.Science,277(5326):684–686.TheManyBabiesConsortium.2020.Quantifyingsourcesofvariabilityininfancyresearchusingtheinfant-directed-speechpreference.AdvancesinMethodsandPracticesinPsychologicalScience,3(1):24–52.BobMcMurray,KristineA.Kovack-Lesh,DresdenGoodwin,andWilliamMcEchron.2013.Infantdi-rectedspeechandthedevelopmentofspeechpercep-tion:Enhancingdevelopmentoranunintendedcon-sequence?Cognition,129(2):362–378.DannyMerkx,StefanL.Frank,andMirjamErnes-tus.2019.LanguageLearningUsingSpeechtoIm-ageRetrieval.InProceedingsofInterspeech2019,pages1841–1845.RochelleS.Newman,MeredithL.Rowe,andNanBernsteinRatner.2016.Inputanduptakeat7monthspredictstoddlervocabulary:theroleofchild-directedspeechandinfantprocessingskillsinlanguagedevelopment.JournalofChildLanguage,43(5):1158–1173.ElissaNewport,HenryGleitman,andLilaGleitman.1977.Mother,I’dratherdoitmyself:Someef-fectsandnoneffectsofmaternalspeechstyle.InCatherineE.SnowandCharlesA.Ferguson,editors,Talkingtochildren:Languageinputandacquisition,pages109–149.CambridgeUniversityPress,Cam-bridge.AronvandenOord,SanderDieleman,HeigaZen,KarenSimonyan,OriolVinyals,AlexanderGraves,NalKalchbrenner,AndrewSenior,andKorayKavukcuoglu.2016.WaveNet:Agenerativemodelforrawaudio.arXiv:1609.03499.NanBernsteinRatnerandBeckyRooney.2001.HowaccessibleisthelexiconinMotherese?InJ¨urgenWeissenbornandBarbaraH¨ohle,editors,Ap-proachestoBootstrapping:Phonological,lexical,syntacticandneurophysiologicalaspectsofearlylanguageacquisition,volume23ofLanguageAc-quisitionandLanguageDisorders,pages71–78.JohnBenjaminsPublishingCompany.NilsReimersandIrynaGurevych.2019.Sentence-BERT:SentenceembeddingsusingSiameseBERT-networks.InProceedingsofthe2019ConferenceonEmpiricalMethodsinNaturalLanguageProcessingandthe9thInternationalJointConferenceonNatu-ralLanguageProcessing(EMNLP-IJCNLP),pages3982–3992.MelanieSoderstrom.2007.Beyondbabytalk:Re-evaluatingthenatureandcontentofspeechin-puttopreverbalinfants.DevelopmentalReview,27(4):501–532.AdrianaWeislederandAnneFernald.2013.Talk-ingtochildrenmatters:Earlylanguageexperiencestrengthensprocessingandbuildsvocabulary.Psy-chologicalScience,24(11):2143–2152.\\x0c'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mvsR5PptXmGq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}