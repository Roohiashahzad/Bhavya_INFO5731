Contribution,Annotation 1,Annotation 2
Crowdsourcing a large paraphrasing corpus of questions which are answerable using the data from EHR,Applications,Applications
The creation and annotation of a newspaper dataset for political bias detection,Dataset Creation,Dataset Creation
this paper puts forward a new thinking direction of enriching training dataset for the CGED task.,New Algorithm/ Method,New Algorithm/ Method
"We conduct a comparison and evaluation of our findings with other URE techniques, to ascertain the important features in URE. We conclude that entity types provide a strong inductive bias for URE",Performance Evaluation,Performance Evaluation
"we address the task of machine reading the time of historical events, compile datasets for the task, and develop a model for tackling it.",Theory Proposal,Theory Proposal
"we explore the implicit event argument detection task, which studies event arguments beyond sentence boundaries",Algorithm/Method Optimization,Algorithm/Method Optimization
We demonstrate the effectiveness of our approach with state-of-the-art accuracy on the unsupervised Story Cloze task and with promising results on larger-scale next sentence prediction tasks,New Algorithm/ Method,New Algorithm/ Method
"In this paper we explore the use of synthetic data for the English shallow task. We analyse the effects of synthetic data, and
we argue that its use should be encouraged
rather than prohibited so that future research
efforts continue to explore systems that can
take advantage of such data.",Applications,Applications
"In this paper, we present a novel image captioning architecture to better explore semantics available in captions and leverage that to enhance both image representation and caption generation",New Algorithm/ Method,New Algorithm/ Method
"In this paper, I take a broad linguistic perspective, looking at how well current models can deal with various semantic challenges.",New Algorithm/ Method,Theory Proposal
"we outline several lessons that transfer to QA research: removing ambiguity, identifying better QA agents, and adjudicating disputes.",Performance Evaluation,Performance Evaluation
"In this paper, we will attempt to uncover potential reasons for this.",Theory Proposal,Theory Proposal
"Here we reflect on parsing MRLs in that decade, highlight the solutions and lessons learned for the architectural, modeling and lexical challenges in the pre-neural era, and argue that similar challenges re-emerge in neural architectures for MRLs",Theory Proposal,Theory Proposal
we compare the structural probe to a more traditional parser with an identical lightweight parameterisation,Performance Evaluation,Performance Evaluation
"We review motivations, definition, approaches, and methodology for unsupervised crosslingual learning and call for a more rigorous position in each of them.",Algorithm/Method Optimization,Algorithm/Method Optimization
"We propose an approach to solve this task as a link prediction problem, using Deep Convolutional Graph Neural Networks. This paper also analyses how different baselines perform in this task and shows that a graph structure can provide higher F1-score, especially when considering multi-hop premise selection",New Algorithm/ Method,New Algorithm/ Method
"This paper provides the first study of how these explanations can be generated automatically based on available claim context, and how this task can be modelled jointly with veracity prediction",Theory Proposal,Theory Proposal
"This paper presents Kernel Graph Attention Network (KGAT), which conducts more finegrained fact verification with kernel-based attentions",Theory Proposal,Model Proposal
we propose a multi-source meta transfer (MMT) for low-resource MCQA.,Model Proposal,Model Proposal
We evaluate stateof-the-art cross-lingual models and machinetranslation-based baselines on MLQA.,Performance Evaluation,Performance Evaluation
"In this paper we present DoQA, a task and associated dataset for accessing domain-specific FAQs via conversational QA",New Algorithm/ Method,New Algorithm/ Method
we devise a novel bootstrapping framework based on self-supervision to obtain a dataset of clarification questions from various domains of stackexchange,Model Proposal,Model Proposal
"In this paper, we develop simple approaches to semi-supervised contextualized text normalization",New Algorithm/ Method,New Algorithm/ Method
We create a ten-year Reddit corpus as a benchmark for MFEP and evaluate a number of baselines on this benchmark,New Algorithm/ Method,New Algorithm/ Method
we propose a model that can disambiguate between mappings and convert between the two scripts.,Model Proposal,Model Proposal
"Our method is based on repeated training of linear classifiers that predict a certain property we aim to remove, followed by projection of the representations on their null-space",Algorithm/Method Optimization,Algorithm/Method Optimization
"This paper contributes a sober view of the problem, a survey of techniques to address it, novel techniques, and extensions to the model. To establish a ranking of techniques, we perform a systematic comparison using Bayesian optimisation and find that many techniques perform reasonably similar, given enough resources",Theory Proposal,Theory Proposal
"In this paper, we investigate the feasibility of training monolingual Transformer-based language models for other languages, taking French as an example and evaluating our language models on part-of-speech tagging, dependency parsing, named entity recognition and natural language inference tasks",Performance Evaluation,Performance Evaluation
"We propose a novel large-scale referring expression recognition dataset, Refer360°, consisting of 17,137 instruction sequences and ground-truth actions for completing these instructions in 360° scenes.",Dataset Creation,Dataset Creation
"This paper proposes a new neural network architecture for VQA based on the recent Graph Network (GN) (Battaglia et al., 2018).",New Algorithm/ Method,New Algorithm/ Method
we propose a novel dual channel graph convolutional network (DC-GCN) for better combining visual and textual advantages.,Model Proposal,Theory Proposal
we propose to explicitly segment target text into fragment units and align them with their data correspondences,New Algorithm/ Method,New Algorithm/ Method
"In this paper, we propose the Heterogeneous Graph Transformer to independently model the different relations in the individual subgraphs of the original graph, including direct relations, indirect relations and multiple possible relations between nodes.",New Algorithm/ Method,New Algorithm/ Method
"In this paper, we propose a novel attentional sequence-to-sequence(Seq2seq) model that dynamically exploits the relevance of each output word to the target stylefor unsupervised style transfer.",Model Proposal,Model Proposal
we propose a neural co-generation model that generates dialogue acts and responses concurrently,Model Proposal,Model Proposal
we propose a multi-task learning model with a simple yet effective utterance tagging technique and a bidirectional language model as an auxiliary task for task-oriented dialogue state generation.,Model Proposal,Model Proposal
"In this paper, we propose a Meta-Reinforced MultiDomain State Generator (MERET). Our first contribution is to improve the DST accuracy. We enhance a neural model based DST generator with a reward manager, which is built on policy gradient reinforcement learning (RL) to fine-tune the generator",New Algorithm/ Method,New Algorithm/ Method
"In this paper, we propose a Chinese multi-domain knowledge-driven conversation dataset, KdConv, which grounds the topics in multi-turn conversations to knowledge graphs",Dataset Creation,Dataset Creation
"In this paper, we propose a new perspective to diversify dialogue generation by leveraging non-conversational text",New Algorithm/ Method,New Algorithm/ Method
"we aim to identify, from among a set of speeches on the same topic and with an opposing stance, the ones that directly counter it. We provide a large dataset of 3685 such speeches (in English), annotated for this relation, which hopefully would be of general interest to the NLP community.",Performance Evaluation,Performance Evaluation
"In this paper, we model debaters’ prior beliefs, interests, and personality traits based on their previous activity, without dependence on explicit user profiles or questionnaires. Using a dataset of over 60,000 argumentative discussions, comprising more than three million individual posts collected from the subreddit r/ChangeMyView, we demonstrate that our modeling of debater’s characteristics enhances the prediction of argument persuasiveness as well as of debaters’ resistance to persuasion.",Model Optimization,Model Optimization
"In this paper, we formulate the data augmentation as a conditional generation task: generating a new sentence while preserving the original opinion targets and labels. We propose a masked sequence-to-sequence method for conditional augmentation of aspect term extraction.",Model Optimization,Model Optimization
"In order to further test the capabilities of these powerful neural networks on a harder NLP problem, we propose a transition system that, thanks to Pointer Networks, can straightforwardly produce labelled directed acyclic graphs and perform semantic dependency parsing.",Algorithm/Method Optimization,Algorithm/Method Optimization
"We apply, extend and evaluate different meta-embedding methods from the word embedding literature at the sentence level, including dimensionality reduction (Yin and Schutze ¨ , 2016), generalized Canonical Correlation Analysis (Rastogi et al., 2015) and cross-view auto-encoders (Bollegala and Bao, 2018).",Applications,Applications
"In this paper, we present an improved crowdsourcing protocol for complex semantic annotation, involving worker selection and training, and a data consolidation phase.",Dataset Creation,Dataset Creation
" In this paper, we propose N 3 (Neural Networks from Natural Language) - a new paradigm of synthesizing task-specific neural networks from language descriptions and a generic pre-trained model.",New Algorithm/ Method,New Algorithm/ Method
"We therefore present a novel domain-agnostic Human-In-The-Loop annotation approach: we use recommenders that suggest potential concepts and adaptive candidate ranking, thereby speeding up the overall annotation process and making it less tedious for users. We evaluate our ranking approach in a simulation on difficult texts and show that it greatly outperforms a strong baseline in ranking accuracy.",Model Proposal,Model Proposal
"We present a method for incorporating model and data uncertainty estimates into natural language processing models for automatic rumour verification. We show that these estimates can be used to filter out model predictions likely to be erroneous, so that these difficult instances can be prioritised by a human fact-checker. We propose two methods for uncertainty-based instance rejection, supervised and unsupervised. We also show how uncertainty estimates can be used to interpret model performance as a rumour unfolds.",New Algorithm/ Method,New Algorithm/ Method
"In this paper, we present CorefQA, an accurate and extensible approach for the coreference resolution task. We formulate the problem as a span prediction task",New Algorithm/ Method,New Algorithm/ Method
"In this paper, we close this gap by reporting concept extraction performance on automatically anonymized data and investigating joint models for de-identification and concept extraction.",Performance Evaluation,Performance Evaluation
" We propose uncertainty-aware curriculum learning, which is motivated by the intuition that: 1) the higher the uncertainty in a translation pair, the more complex and rarer the information it contains; and 2) the end of the decline in model uncertainty indicates the completeness of current training stage. Specifically, we serve cross-entropy of an example as its data difficulty and exploit the variance of distributions over the weights of the network to present the model uncertainty",Model Proposal,Model Proposal
"We present the first thorough investigation of gender bias in speech translation, contributing with: i) the release of a benchmark useful for future studies, and ii) the comparison of different technologies (cascade and end-to-end) on two language directions (English-Italian/French).",Performance Evaluation,Performance Evaluation
"In this work, we present CLASSYMAP, a classification-based approach to self-learning, yielding a more robust and a more effective induction of projection-based CLWEs. Unlike prior self-learning methods, our approach allows for integration of diverse features into the iterative process. We show the benefits of CLASSYMAP for bilingual lexicon induction: we report consistent improvements in a weakly supervised setup (500 seed translation pairs) on a benchmark with 28 language pairs.",New Algorithm/ Method,New Algorithm/ Method
"In this work, we introduce a class of hyperbolic KG embedding models that simultaneously capture hierarchical and logical patterns.",New Algorithm/ Method,New Algorithm/ Method
we introduce a gated component self-dependency units (SDU) that incorporates LSTM-styled gating units to replenish internal semantic importance within the multi-dimensional latent space of individual representations.,New Algorithm/ Method,New Algorithm/ Method
"We introduce a parametric family of entropy regularizers, which includes label smoothing as a special case, and use it to gain a better understanding of the relationship between the entropy of a trained model and its performance on language generation tasks.",New Algorithm/ Method,New Algorithm/ Method
"In this paper, we develop a deep, endto-end model that learns to effectively classify mismatches and to generate hard mismatched examples to improve the classifier. We train the model end-to-end by introducing a latent variable into the cross-entropy loss that alternates between using the real and generated samples.",New Algorithm/ Method,New Algorithm/ Method
"In this paper, we compare the performance of both QT and DT using the traditional SMT and state-of-the-art NMT methods trained on the same data to make the comparison as fair as possible. We present a novel approach for NMT model selection that is optimized towards CLIR performance and investigate the effect of morphological pre- and post-processing on the performance on CLIR.",Performance Evaluation,Performance Evaluation
"In this paper, we propose a method FGS2EE to inject fine-grained semantic information into entity embeddings to reduce the distinctiveness and facilitate the learning of contextual commonality.",New Algorithm/ Method,New Algorithm/ Method
"In this paper, we propose FLAT: Flat-LAttice Transformer for Chinese NER, which converts the lattice structure into a flat structure consisting of spans.",New Algorithm/ Method,New Algorithm/ Method
"We present an effective TSE method which is based on querying large, pre-trained masked language models (MLMs).",New Algorithm/ Method,New Algorithm/ Method
"In this work, we propose a structural-aware model at both the encoder and decoder phase to integrate the structural information, where graph attention network (GAT) is exploited for effectively modeling.",Model Proposal,Model Proposal
"we propose a two-stage semantic parsing framework, where the first stage utilizes an unsupervised paraphrase model to convert an unlabeled natural language utterance into the canonical utterance.",Model Proposal,Model Proposal
We propose an approach to semi-supervised learning of semantic dependency parsers based on the CRF autoencoder framework.,New Algorithm/ Method,New Algorithm/ Method
" we formulate the task based on the divergence between literal and intended meanings. We combine the complementary strengths of English Resource Grammar, a linguistically-precise hand-crafted deep grammar, and TLE, an existing manually annotated ESL UD-TreeBank with a novel reranking model.",New Algorithm/ Method,New Algorithm/ Method
We demonstrate the effectiveness of this new perspective by developing a new state-of-the-art semantic parser for Minimal Recursion Semantics.,New Algorithm/ Method,New Algorithm/ Method
"In this paper, we introduce a new model, called RikiNet, which reads Wikipedia pages for natural question answering.",Model Proposal,Model Proposal
"In this paper, we study machine reading comprehension (MRC) on long texts, where a model takes as inputs a lengthy document and a question and then extracts a text span from the document as an answer",Theory Proposal,Theory Proposal
"We present a reliable, crowdsourced framework for scalably annotating RC datasets with derivations. We create and publicly release the R4C dataset, the first, quality-assured dataset consisting of 4.6k questions, each of which is annotated with 3 reference derivations (i.e. 13.8k derivations).",Dataset Creation,Dataset Creation
we propose to learn the model with the help of a large scale of unlabeled data that is much easier to obtain.,Theory Proposal,Theory Proposal
"In this work, we introduce two approaches to improve unsupervised QA. First, we harvest lexically and syntactically divergent questions from Wikipedia to automatically construct a corpus of question-answer pairs (named as REFQA)",New Algorithm/ Method,New Algorithm/ Method
"we present a novel multi-grained machine reading comprehension framework that focuses on modeling documents at their hierarchical nature, which are different levels of granularity: documents, paragraphs, sentences, and tokens. We utilize graph attention networks to obtain different levels of representations so that they can be learned simultaneously",Model Proposal,Model Proposal
"We propose the task of unsupervised morphological paradigm completion. Given only raw text and a lemma list, the task consists of generating the morphological paradigms, i.e., all inflected forms, of the lemmas",New Algorithm/ Method,New Algorithm/ Method
"Here, we investigate the strength of those clues. More specifically, we operationalize “strength” as measuring how much information, in bits, we can glean about declension class from knowing the form and meaning of nouns",Performance Evaluation,Performance Evaluation
"In this paper, we propose a new framework that incorporates typological awareness by explicitly modeling different morphological patterns including suffixation, prefixation, infixation, and reduplication.",Model Proposal,Model Proposal
we develop a sentence-level training procedure to perform noise reduction and maximum utilization of the source domain information.,New Algorithm/ Method,New Algorithm/ Method
"We generate data from a finite state transducer to train an encoderdecoder model. We improve the model by “hallucinating” missing linguistic structure into the training data, and by resampling from a Zipf distribution to simulate a more natural distribution of morphemes.",Dataset Creation,Dataset Creation
"we propose a modification to contextual representation fine-tuning which, during inference, allows for an early (and fast) “exit” from neural network calculations for simple instances, and late (and accurate) exit for hard instances.",Algorithm/Method Optimization,Algorithm/Method Optimization
we present a general approach to learn both intra-cell and inter-cell architectures (call it ESS).,Theory Proposal,Theory Proposal
"In this paper, we make use of a multi-task objective, i.e., the models simultaneously predict words as well as ground truth parse trees in a form called “syntactic distances”, where information between these two separate objectives shares the same intermediate representation",Model Optimization,Model Optimization
"we show that adversarial examples also exist in dependency parsing: we propose two approaches to study where and how parsers make mistakes by searching over perturbations to existing texts at sentence and phrase levels, and design algorithms to construct such examples in both of the black-box and white-box settings",Performance Evaluation,Performance Evaluation
"We propose Differentiable Window, a new neural module and general purpose component for dynamic window selection. While universally applicable, we demonstrate a compelling use case of utilizing Differentiable Window to improve standard attention modules by enabling more focused attentions over the input regions. We propose two variants of Differentiable Window, and integrate them within the Transformer architecture in two novel ways. We evaluate our proposed approach on a myriad of NLP tasks, including machine translation, sentiment analysis, subject-verb agreement and language modeling. Our experimental results demonstrate consistent and sizable improvements across all tasks.",New Algorithm/ Method,New Algorithm/ Method
we propose a dependency graph enhanced dual-transformer network (named DGEDT) by jointly considering the flat representations learnt from Transformer and graphbased representations learnt from the corresponding dependency graph in an iterative interaction manner.,New Algorithm/ Method,New Algorithm/ Method
we propose the mixture of attentive experts model (MAE). MAE is trained using a block coordinate descent algorithm that alternates between updating (1) the responsibilities of the experts and (2) their parameters,Performance Evaluation,Performance Evaluation
"We critically examine RefCOCOg, a standard benchmark for this task, using a human study and show that 83.7% of test instances do not require reasoning on linguistic structure, i.e., words are enough to identify the target object, the word order doesn’t matter",Algorithm/Method Optimization,Algorithm/Method Optimization
"We propose a video span localizing network (VSLNet), on top of the standard span-based QA framework, to address NLVL. The proposed VSLNet tackles the differences between NLVL and span-based QA through a simple and yet effective query-guided highlighting (QGH) strategy",Performance Evaluation,Performance Evaluation
We present a simple experiment on language grounding that highlights the great potential of top-down processing even for very common words with a lot of visual instances: we learn to ground colour terms in visual representations of real-world objects and show that model predictions improve strongly when incorporating prior knowledge and assumptions about the object itself.,Model Proposal,Model Proposal
"In this paper, we propose to tackle this issue of large-scale image-caption consistency using a coherence-aware approach inspired by the framework of discourse coherence theory (Hobbs, 1978; Phillips, 1977).",Resources,Resources
"In this paper, we explore AspectOpinion Pair Extraction (AOPE) task, which aims at extracting aspects and opinion expressions in pairs",New Algorithm/ Method,New Algorithm/ Method
"In this paper, we propose a teacher-student learning method to address such limitations, where NER models in the source languages are used as teachers to train a student model on unlabeled data in the target language",New Algorithm/ Method,New Algorithm/ Method
"In this paper, we present a novel approach to the task of extracting structured information from formlike documents using a learned representation of an extraction candidate.",New Algorithm/ Method,New Algorithm/ Method
In this work we annotate a test set with ground-truth sentence-level explanations to evaluate the quality of explanations afforded by the relation extraction models. We demonstrate that replacing the entity mentions in the sentences with their fine-grained entity types not only enhances extraction accuracy but also improves explanation. We also propose to automatically generate “distractor” sentences to augment the bags and train the model to ignore the distractors.,Performance Evaluation,Performance Evaluation
"We present Neighborhood Matching Network (NMN), a novel sampling-based entity alignment framework. NMN aims to capture the most informative neighbors and accurately estimate the similarities of neighborhoods between entities in different KGs",Model Proposal,Model Proposal
"In this paper, we use ideas from graph-based dependency parsing to provide our model a global view on the input via a biaffine model (Dozat and Manning, 2017).",Model Optimization,Model Optimization
We then propose a Medical Information Extractor (MIE) towards medical dialogues.,New Algorithm/ Method,New Algorithm/ Method
"In this study, we develop models possessing interpretable inference process for structured prediction. Specifically, we present a method of instance-based learning that learns similarities between spans.",Model Proposal,Model Proposal
"In this paper, we refer this phenomenon particularly to rare entity problem. It is different from other types of data sparsity problems such as the lack of training data for lowresource language (Lin et al., 2018), as this rare entity problem is more related to a mismatch of entity distributions between training and test, rather than the size of training data. We present an example of the problem in Table",Theory Proposal,Theory Proposal
we introduce episodic memory activation and reconsolidation (EMAR) to continual relation learning in this paper,Theory Proposal,Theory Proposal
"In this paper, we propose a novel approach for KG entity typing which is trained by jointly utilizing local typing knowledge from existing entity type assertions and global triple knowledge from KGs.",New Algorithm/ Method,New Algorithm/ Method
"In this paper, we propose a novel bipartite flatgraph network (BiFlaG) for nested named entity recognition (NER), which contains two subgraph modules: a flat NER module for outermost entities and a graph module for all the entities located in inner layers.",New Algorithm/ Method,New Algorithm/ Method
"In this paper, we argue that incorporation of multimodal cues can improve the automatic identification of PPI",Theory Proposal,Theory Proposal
"In this paper, we justify from both computational and perceptive points-of-view that the top-down architecture is more suitable for textlevel DRS parsing. On the basis, we propose a top-down neural architecture toward text-level DRS parsing.",Performance Evaluation,Performance Evaluation
"In this paper, we propose an automatic evaluation model based on that idea and learn the model parameters from an unlabeled conversation corpus.",Model Proposal,Model Proposal
We propose a Dialogue State Tracker with Slot Attention and Slot Information Sharing (SAS) to reduce redundant information’s interference and improve long dialogue context tracking.,Model Optimization,Model Optimization
"In this paper, we present that efficiently learns dialogue policy from demonstrations through policy shaping and reward shaping.",Algorithm/Method Optimization,Algorithm/Method Optimization
"we propose a novel Dynamic Fusion Network (DFNet) which automatically exploit the relevance between the target domain and each domain. Results show that our model outperforms existing methods on multi-domain dialogue, giving the state-of-the-art in the literature",Model Optimization,Model Optimization
"In this paper, we propose a data manipulation framework to proactively reshape the data distribution towards reliable samples by augmenting and highlighting effective learning samples as well as reducing the effect of inefficient samples simultaneously",New Algorithm/ Method,New Algorithm/ Method
"In this paper, we propose to enhance the DST through employing a contextual hierarchical attention network to not only discern relevant information at both word level and turn level but also learn contextual representations",Algorithm/Method Optimization,Algorithm/Method Optimization
"In this paper we intend to give a thorough chronology of the major interplay between corpus progression and query tool evolution, with a strong focus on the latter",Theory Proposal,Theory Proposal
"In this paper, we trace the history of neural networks applied to natural language understanding tasks, and identify key contributions which the nature of language has made to the development of neural network architectures.",Theory Proposal,Theory Proposal
"In this paper we look at the relation between the types of languages, resources, and their representation in NLP conferences to understand the trajectory that different languages have followed over time.",Theory Proposal,Theory Proposal
"This work proposes an approach to representation and learning based on the tenets of embodied cognitive linguistics (ECL). According to ECL, natural language is inherently executable (like programming languages), driven by mental simulation and metaphoric mappings over hierarchical compositions of structures and schemata learned through embodied interaction",Theory Proposal,Theory Proposal
"In this paper, we analyze three different instances of sample bias that are prevalent in QE datasets, which affect the generalization that models trained on them can achieve.",Performance Evaluation,Performance Evaluation
" we propose an algorithm that reduces parsing to tagging, where all tags are predicted in parallel using a standard model architecture such as BERT (Devlin et al., 2019).",New Algorithm/ Method,New Algorithm/ Method
"In this paper, we propose a multi-granularity interaction network for extractive and abstractive multi-document summarization, which jointly learn semantic representations for words, sentences, and documents.",New Algorithm/ Method,New Algorithm/ Method
"In this paper, we develop a neural abstractive multidocument summarization (MDS) model which can leverage well-known graph representations of documents such as similarity graph and discourse graph, to more effectively process multiple input documents and produce abstractive summaries.",Model Optimization,Model Optimization
"In this paper, we propose to ease the cross-lingual summarization training by jointly learning to align and summarize. We design relevant loss functions to train this framework and propose several methods to enhance the isomorphism and cross-lingual transfer between languages.",New Algorithm/ Method,New Algorithm/ Method
"In this paper, we present a heterogeneous graph-based neural network for extractive summarization (HETERSUMGRAPH), which contains semantic nodes of different granularity levels apart from sentences.",Model Proposal,Model Proposal
"In this paper, we focus on extractive summarization since it usually generates semantically and grammatically correct sentences (Dong et al., 2018; Nallapati et al., 2017) and computes faster.",Theory Proposal,Theory Proposal
"In this paper, we argue that elementary discourse unit (EDU) is a more appropriate textual unit of content selection than the sentence unit in abstractive summarization",New Algorithm/ Method,New Algorithm/ Method
"In this paper, we study the challenging problem of automatic generation of citation texts in scholarly papers",New Algorithm/ Method,New Algorithm/ Method
we present a method suitable for reasoning about the semantic-level structure of evidence.,New Algorithm/ Method,New Algorithm/ Method
"In this paper, we propose two efficient neural mixed counting models, i.e., the Negative BinomialNeural Topic Model (NB-NTM) and the Gamma Negative Binomial-Neural Topic Model (GNB-NTM) for dispersed topic discovery.",Model Proposal,Model Proposal
"we propose neural graph matching networks, a novel sentence matching framework capable of dealing with multi-granular input information",New Algorithm/ Method,New Algorithm/ Method
"we propose a neural network model, called NeuInfer, to conduct both simple and flexible knowledge inference on n-ary facts",Model Proposal,Model Proposal
"we propose to incorporate paraphrase knowledge into question generation(QG) to generate human-like questions. Specifically, we present a two-hand hybrid model leveraging a self-built paraphrase resource, which is automatically conducted by a simple back-translation method.",Applications,Applications
" we propose an approach that automatically finds evidence for an event from a large text corpus, and leverages the evidence to guide the generation of inferential texts",New Algorithm/ Method,New Algorithm/ Method
"In this paper, we introduce a method for evaluating whether neural models can learn systematicity of monotonicity inference in natural language, namely, the regularity for performing arbitrary inferences with generalization on composition",New Algorithm/ Method,Performance Evaluation
"In this paper, we draw inspiration from similar ideas, and propose our approach 6096 for arranging a curriculum when learning NLU tasks",Theory Proposal,Theory Proposal
we propose a novel decoding algorithm that integrates constrained decoding using positive/negative examples during inference: this demonstrates the potential of our dataset to enable work at the intersection of NLP and program synthesis.,New Algorithm/ Method,New Algorithm/ Method
"In this paper, we propose a novel attack model, which incorporates the sememebased word substitution method and particle swarm optimization-based search algorithm to solve the two problems separately.",Model Proposal,Model Proposal
"we propose LogicalFactChecker, a neural network approach capable of leveraging logical operations for fact checking",New Algorithm/ Method,New Algorithm/ Method
we explore the effectiveness of incorporating two varieties of external knowledge into NL-to-code generation: automatically mined NL-code pairs from the online programming QA forum StackOverflow and programming language API documentation.,Performance Evaluation,Performance Evaluation
we propose a novel speed-tunable FastBERT with adaptive inference time.,New Algorithm/ Method,New Algorithm/ Method
"In this paper, we present the first detailed empirical study of the effects of different masked lan- guage modeling (MLM) pretraining regimes on cross-lingual transfer. Our first set of experiments is a detailed ablation study on a range of zero-shot cross-lingual transfer tasks",Algorithm/Method Optimization,Algorithm/Method Optimization
"In this paper, we present an effective retrievalbased approach to paraphrase generation by proposing a novel editor module.",New Algorithm/ Method,New Algorithm/ Method
"In this paper, we use both frequency changes and word semantic shifts to measure document influence by developing a neural network based framework",New Algorithm/ Method,New Algorithm/ Method
"In this paper, we apply pre-trained word embedding as the intermediate level in the multitask ST model. We propose to constrain the hidden states of the decoder of the recognition part to be close to the pre-trained word embedding",Model Proposal,Model Proposal
"In this paper, we show that neural machine translation (NMT) systems trained on large back-translated data overfit some of the characteristics of machine-translated texts",Theory Proposal,Theory Proposal
we propose to use extracted templates from tree structures as soft target templates to guide the translation procedure.,Theory Proposal,Theory Proposal
"In this work, we investigate the effect of future sentences as context by comparing the performance of a contextual NMT model trained with the future context to the one trained with the past context.",Performance Evaluation,Performance Evaluation
"In this paper, we propose a new adversarial augmentation method for Neural Machine Translation (NMT).",New Algorithm/ Method,New Algorithm/ Method
"In this work, we propose a simple but effective method for incorporating the word lexicon into the character representations.",New Algorithm/ Method,New Algorithm/ Method
"In this paper, we consider the problem of shifted label distribution, which is caused by the inconsistency between the noisy-labeled training set subject to external knowledge graph and the human-annotated test set, and exacerbated by the pipelined entity-then-relation extraction manner with noise propagation",New Algorithm/ Method,New Algorithm/ Method
"In this paper, we take the benefits of ConvE and KBGAT together and propose a Relation-aware Inception network with joint local-global structural information for knowledge graph Embedding (ReInceptionE). Specifically, we first explore the Inception network to learn query embedding, which aims to further increase the interactions between head and relation embeddings",Model Proposal,Model Proposal
"In this paper, we propose a novel layered model called Pyramid for nested NER. The model consists of a stack of inter-connected layers. Each layer l predicts whether a text region of certain length l, i.e. an l-gram, is a complete entity mention",Model Optimization,Model Optimization
We investigate a multi-cell compositional LSTM structure to deal with the above challenges by separately and simultaneously considering the possibilities of all entity types for each word when processing a sentence.,Performance Evaluation,Performance Evaluation
"In this paper, we consider bridging multi-task learning (MTL) (Caruana, 1993; Ruder, 2017) and pretraining (Peters et al., 2018; Devlin et al., 2019) to leverage training signals of an auxiliary task that has a sufficiently large number of labeled data. We present a joint
model that supports multi-class classification
and introduce a simple variant of self-attention
that allows the model to learn scaling factors.",Model Optimization,Model Optimization
"In the paper, we empower the model with external knowledge called Open-Domain Trigger Knowledge to provides extra semantic support on unseen/sparsely labeled trigger words and improve trigger identification",Model Optimization,Model Optimization
"We present IMOJIE, an extension to CopyAttention, which produces the next extraction conditioned on all previously extracted tuples. we design the first neural OpenIE
system that uses sequential decoding of tuples conditioned on previous tuples.",Resources,Resources
"We propose a simple, effective transition-based model with generic neural encoding for discontinuous NER.We propose an end-to-end
transition-based model with generic neural
encoding that allows us to leverage specialized
actions and attention mechanism to determine
whether a span is the component of a discontinuous
mention or not.",Model Proposal,Model Proposal
"In this paper, we propose a unified framework that is capable of handling both flat and nested NER tasks",New Algorithm/ Method,New Algorithm/ Method
"In this paper, we leverage the power of pre-trained language models for improving video-grounded dialogue, which is very challenging and involves complex features of different dynamics: (1) Video features which can extend across both spatial and temporal dimensions; and (2) Dialogue features which involve semantic dependencies over multiple dialogue turns. We propose a framework by extending GPT-2 models to tackle these challenges by formulating videogrounded dialogue tasks as a sequence-tosequence task, combining both visual and textual representation into a structured sequence, and fine-tuning a large pre-trained GPT-2 network.",Algorithm/Method Optimization,Algorithm/Method Optimization
"In this paper, we propose an algorithm that can customize a unique dialogue model for each task in the few-shot setting",New Algorithm/ Method,New Algorithm/ Method
"In this work, we introduce a three-stage framework that employs a generate-delete-rewrite mechanism to delete inconsistent words from a generated response prototype and further rewrite it to a personality-consistent one. We carry out evaluations by both human and automatic metrics. Experiments on the Persona-Chat dataset show that our approach achieves good performance",Model Optimization,Model Optimization
"this paper proposes a novel commonsense knowledge-aware dialogue generation model, ConKADI. We design a Felicitous Fact mechanism to help the model focus on the knowledge facts that are highly relevant to the context; furthermore, two techniques, Context-Knowledge Fusion and Flexible Mode Fusion are proposed to facilitate the integration of the knowledge in the ConKADI",Performance Evaluation,Performance Evaluation
"We investigate the following research questions: (i) what is the effect of integrating preprocessing techniques earlier into word embedding models, instead of later on in a downstream classification models? (ii) which preprocessing techniques yield the most benefit in affective tasks? (iii) does preprocessing of word embeddings provide any improvement over stateof-the-art pretrained word embeddings? and if yes, how much?",Performance Evaluation,Performance Evaluation
"We present OPINIONDIGEST, an abstractive opinion summarization framework, which does not rely on gold-standard summaries for training",Theory Proposal,Theory Proposal
we propose a novel Entity-aware Dependencybased Deep Graph Attention Network (ED-GAT) for comparative preference classification. We represent a sentence by its dependency graph,Model Proposal,Model Proposal
"We present an efficient annotation framework for argument quality, a feature difficult to be measured reliably as per previous work.",Model Optimization,Model Optimization
"In this paper, we investigate an extreme scenario of cross-lingual sentiment classification, in which the low-resource language does not have any labels or parallel corpus. We propose an unsupervised cross-lingual sentiment classification model named multi-view encoder-classifier (MVEC) that leverages an unsupervised machine translation (UMT) system and a language discriminator",Performance Evaluation,Performance Evaluation
"in this paper, we introduce a new research problem, stance polarity and intensity prediction in a responsive relationship between posts, which aims to predict a text’s stance polarity and intensity which we combine into a single continuous agreement value",Performance Evaluation,Performance Evaluation
"In this paper, we present the first comprehensive categorization of essential commonsense knowledge for answering the Winograd Schema Challenge (WSC).",New Algorithm/ Method,New Algorithm/ Method
. In this paper we demonstrate that the combination of a consistent answer structure with span annotations opens the door for new approaches to automatic verification of annotations and enables new types of analyses for reading comprehension.,Theory Proposal,Theory Proposal
"we provide best practices and guidelines tailored towards NLP research, as well as an easy-to-use package called HyBayes for Bayesian assessment of hypotheses,1 complementing existing tools.",Theory Proposal,Theory Proposal
We introduce a novel approach to transformers that learns hierarchical representations in multiparty dialogue.,New Algorithm/ Method,New Algorithm/ Method
"In this paper, we introduce the Cascade Transformer, a simple yet effective technique to adapt transformer-based models into a cascade of rankers.",New Algorithm/ Method,New Algorithm/ Method
"we propose the setting of selective question answering under domain shift, in which a QA model is tested on a mixture of in-domain and out-of-domain data, and must answer (i.e., not abstain on) as many questions as possible while maintaining high accuracy",Performance Evaluation,Performance Evaluation
"In this paper, we present SCDE, a dataset of sentence-level cloze questions sourced from public school examinations.",Model Proposal,Model Proposal
"In this paper, we demonstrated that the choice of probability space and interpretation of the distant supervision signal for document-level QA have a large impact, and that they interact.",Performance Evaluation,Performance Evaluation
"we show that diversity-promoting QG indeed provides better QA training than likelihood maximization approaches such as beam search. We also show that standard QG evaluation metrics such as BLEU, ROUGE and METEOR are inversely correlated with diversity, and propose a diversity-aware intrinsic measure of overall QG quality that correlates well with extrinsic evaluation on QA.",Performance Evaluation,Performance Evaluation
"In this paper, we address the task of producing globally consistent and accurate predictions for comparison questions leveraging logical and symbolic knowledge for data augmentation and training regularization",Model Optimization,Model Optimization
"In this work, we propose to cross variational auto-encoders by generating questions with aligned answers and generating answers with aligned questions.",New Algorithm/ Method,New Algorithm/ Method
"In this work, we study the benefits of collecting intermediate reasoning supervision along with the answer during data collection.",Theory Proposal,Theory Proposal
"In this work, we extend this selective rationalization approach to text matching, where the goal is to jointly select and align text pieces, such as tokens or sentences, as a justification for the downstream prediction",New Algorithm/ Method,New Algorithm/ Method
" we propose and conduct a systematic evaluation of the intermediate outputs of NMNs on NLVR2 and DROP, two datasets which require composing multiple reasoning steps.",Algorithm/Method Optimization,Algorithm/Method Optimization
we build hierarchical explanations by detecting feature interactions.,New Algorithm/ Method,New Algorithm/ Method
"we present an unsupervised analysis method that provides evidence mBERT learns representations of syntactic dependency labels, in the form of clusters which largely agree with the Universal Dependencies taxonomy.",New Algorithm/ Method,New Algorithm/ Method
we develop a new quantitative measure based on influence functions that can reveal artifacts in training data,Theory Proposal,Theory Proposal
"In this paper, we evaluated five explanation methods through simulation tests with text and tabular data.",Performance Evaluation,Performance Evaluation
"In this paper, we introduce the Cross-Linguistic Assessment of Models on Syntax (CLAMS) data set, which extends the subject-verb agreement component of the Marvin and Linzen (2018) challenge set to French, German, Hebrew and Russian",Dataset Creation,Dataset Creation
"In this paper, we find that this can be attributed to the inappropriate evaluation protocol used by them and propose a simple evaluation protocol to address this problem.",Theory Proposal,Theory Proposal
"In this paper, we investigate the presence of social biases in sentence-level representations and propose a new method, SENTDEBIAS, to reduce these biases",New Algorithm/ Method,New Algorithm/ Method
"In this paper, we present evidence of such undesirable biases towards mentions of disability in two different English language models: toxicity prediction and sentiment analysis",Theory Proposal,Model Proposal
"We introduce SOCIAL BIAS FRAMES, a new conceptual formalism that aims to model the pragmatic frames in which people project social biases and stereotypes onto others",New Algorithm/ Method,New Algorithm/ Method
"We survey 146 papers analyzing “bias” in NLP systems, fnding that their motivations are often vague, inconsistent, and lacking in normative reasoning, despite the fact that analyzing",Theory Proposal,Theory Proposal
"We propose a simple but effective technique, Double-Hard Debias, which purifies the word embeddings against such corpus regularities prior to inferring and removing the gender subspace.",New Algorithm/ Method,New Algorithm/ Method
we propose a novel regularization technique based on these explanations that encourages models to learn from the context of group identifiers in addition to the identifiers themselves.,New Algorithm/ Method,New Algorithm/ Method
"We propose to better explore their interaction by solving both tasks together, while the previous work treats them separately",Model Optimization,Model Optimization
"We propose PeTra, a memory-augmented neural network designed to track entities in its memory slots. PeTra is trained using sparse annotation from the GAP pronoun resolution dataset and outperforms a prior memory model on the task while using a simpler architecture",New Algorithm/ Method,New Algorithm/ Method
"In this paper, we revisit prior work to explicate the inconsistencies and propose an improved evaluation protocol to promote experimental rigor in future work.",Model Optimization,Model Optimization
we explore to what extent neural network sentence encoders can learn to predict the strength of scalar inferences,Theory Proposal,Theory Proposal
"we apply an existing theory of functional discourse structure for news articles that revolves around the main event and create a human-annotated corpus of 802 documents spanning over four domains and three media sources, we propose several documentlevel neural-network models to automatically construct news content structures",Applications,Applications
we present a new task and corpus for learning alignments between machine and human preferences,New Algorithm/ Method,New Algorithm/ Method
"In this paper, we take the first step towards a better understanding of these processes and the underlying dynamics that shape them, using data-driven methods. We build a new large-scale dataset, from multiple data sources, connecting state bills and legislator information, geographical information about their districts, and donations and donors’ information",Performance Evaluation,Performance Evaluation
"In this paper, we introduce the text-based ideal point model (tbip), an unsupervised probabilistic topic model that analyzes texts to quantify the political positions of its authors. We demonstrate the tbip with two types of politicized text data: U.S. Senate speeches and senator tweets.",Model Proposal,Model Proposal
"we collect and categorize applications with text as a causal confounder (Table 1 and §2), and we provide a flowchart of analysts’ decisions for this problem setting,we highlight recent work in representation learning in NLP (§4) and caution that this is still an open research area with questions of the sensitivity of effects to choices in representation,we summarize some of the most-used causal estimators that condition on confounders:matching, propensity score weighting, regression adjustment, doubly-robust methods, and causally-driven representation learning .",New Algorithm/ Method,New Algorithm/ Method
In this paper we explore connections between the language people use to describe their predictions and their forecasting skill.,Algorithm/Method Optimization,Algorithm/Method Optimization
" In this paper, we present a novel model that uses message-level attention to learn the relative weight of users’ social media posts for assessing their five factor personality traits. We demonstrate that models with message-level attention outperform those with word-level attention, and ultimately yield stateof-the-art accuracies for all five traits by using both word and message attention in combination with past approaches (an average increase in Pearson r of 2.5%).",New Algorithm/ Method,Model Proposal
"In this paper, we introduce HURRICANEEMO, an emotion dataset of 15,000 English tweets spanning three hurricanes: Harvey, Irma, and Maria. We present a comprehensive study of fine-grained emotions and propose classification tasks to discriminate between coarsegrained emotion groups.",Dataset Creation,Dataset Creation
we develop an unsupervised methodology to quantify how counselors manage this balance.,Theory Proposal,Theory Proposal
we demonstrate that certain attention heads of a visually grounded language model actively ground elements of language to image regions.,New Algorithm/ Method,New Algorithm/ Method
"In this paper, we propose a unifying predictive bias framework for NLP. We summarize the NLP literature and suggest general mathematical definitions of predictive bias.",New Algorithm/ Method,New Algorithm/ Method
we perform a large-scale study on the pretrained RoBERTa model with 110 intermediate–target task combinations,Model Proposal,Model Proposal
"In this paper, we describe the history, the current state, and the future directions of research in LegalAI. We illustrate the tasks from the perspectives of legal professionals and NLP researchers and show several representative applications in LegalAI. We conduct experiments and provide an indepth analysis of the advantages and disadvantages of existing works to explore possible future directions.",Theory Proposal,Theory Proposal
We advocate for supplementing or replacing PAID with paradigms that reward architectures that generalize as quickly and robustly as humans.,Model Optimization,Model Optimization
"In this paper, we describe work on examining the papers and their citations to identify broad trends within NLP research—overall, across paper types, across publication venues, over time, and across research areas within NLP. Notably, we explored questions such as: how well cited are papers of different types (journal articles, conference papers, demo papers, etc.)? how well cited are papers published in different time spans? how well cited are papers from different areas of research within NLP? etc",Theory Proposal,Theory Proposal
"In this position paper, we argue that a system trained only on form has a priori no way to learn meaning,We argue in this paper that genuine progress in our field — climbing the right hill,
not just the hill on whose slope we currently sit—
depends on maintaining clarity around big picture
notions such as meaning and understanding in task
design and reporting of experimental results.",Theory Proposal,Theory Proposal
"In this paper, we engage with an idea largely absent from discussions of meaning in natural language understanding—namely, that the way something is expressed reflects different ways of conceptualizing or construing the information being conveyed",New Algorithm/ Method,New Algorithm/ Method
"We define a generative model for a review collection which capitalizes on the intuition that when generating a new review given a set of other reviews of a product, we should be able to control the “amount of novelty” going into the new review or, equivalently, vary the extent to which it deviates from the input.",Model Proposal,Model Proposal
"This paper presents a novel unsupervised abstractive summarization method that generates summaries directly from source documents, without the aid of example summaries.",New Algorithm/ Method,New Algorithm/ Method
"This paper describes the Critical Role Dungeons and Dragons Dataset (CRD3) and related analyses. Critical Role is an unscripted, live-streamed show where a fixed group of people play Dungeons and Dragons, an openended role-playing game.",Resources,Resources
we develop a general framework where we evaluate the factual correctness of a generated summary by factchecking it automatically against its reference using an information extraction module,Performance Evaluation,Performance Evaluation
"We propose the use of dual encoders—a sequential document encoder and a graphstructured encoder—to maintain the global context and local characteristics of entities, complementing each other.",New Algorithm/ Method,New Algorithm/ Method
We present Deep Generalized Canonical Correlation Analysis (DGCCA).,New Algorithm/ Method,New Algorithm/ Method
we investigate a variety of features for automatically detecting plagiarized spoken responses in the context of a standardized assessment of English speaking proficiency.,Model Optimization,Model Optimization
"we recorded 2 hours of Chinese voice data from a female speaker, and Xiaomingbot learnt to speak in English and Japanese with the same voice.",Resources,Resources
"We propose an end-to-end, data-driven model for predicting depression from interview transcripts that leverages the contextual information provided by interviewer prompts",Model Proposal,Model Optimization
We conduct robust experiments to show that our model outperforms competitive baselines,Resources,Resources
"Our proposal rests on A light-weight query language that does not require in-depth familiarity with the underlying syntactic representation scheme, and instead lets the user specify their intent via a natural language example and lightweight markup.",Theory Proposal,Theory Proposal
"Our proposal rests on A fast, near-real-time response time due to efficient indexing, allowing for rapid experimentation",Performance Evaluation,Performance Evaluation
We describe how the cards were generated where each card corresponds to a Wikipedia page,Theory Proposal,Theory Proposal
"we adapted MRS techniques to create a conversational search portal that enable users to ask natural language questions to find precise answers and extract insights from the last 3 year papers published in top-tier NLP conferences, including ACL, NAACL, EMNLP and etc.",Applications,Applications
We developed a collaborative annotation toolkit that enable any researcher to contribute to this dataset so that more potential answers from these papers can be annotated.,Dataset Creation,Dataset Creation
we provided further evidence that the nature of LKBs impacts on system performance: the injection of syntagmatic relations – in the form of disambiguated pairs of co-occurring words – into an existing LKB biased towards paradigmatic knowledge enables knowledge-based systems to rival their supervised counterparts,Model Optimization,Model Optimization
"we introduce a Web interface and a RESTful API for SyntagRank, our multilingual WSD system, which applies the Personalized PageRank (PPR) algorithm (Haveliwala, 2002) to an LKB made up of WordNet, the Princeton WordNet Gloss Corpus (PWNG) and the lexical-semantic syntagmatic combinations available in the SyntagNet resource",New Algorithm/ Method,New Algorithm/ Method
"We introduce a broad-coverage, datadriven and linguistically sound set of transformations, that makes event-structure and many lexical relations explicit",Dataset Creation,Dataset Creation
"We present pyBART, an easy-to-use open-source Python library for converting English UD trees either to Enhanced UD graphs or to our representation",Theory Proposal,Model Optimization
"We build EVIDENCEMINER, a web-based system for textual evidence discovery for life sciences. EVIDENCEMINER is supported by novel methods for distantly supervised named entity recognition and pattern-based open information extraction.",New Algorithm/ Method,New Algorithm/ Method
"we describe a prototype system that facilitates interactive exploration and mapping of the evidence base, with an emphasis on answering the above questions",Theory Proposal,Theory Proposal
we introduce SyntaxGym: an online platform and open-source framework that makes targeted syntactic evaluations more accessible to experts in NLP and linguistics,Theory Proposal,New Algorithm/ Method
"Our system, GAIA enables seamless search of complex graph queries, and retrieves multimedia evidence including text, images and videos.",Theory Proposal,Theory Proposal
"We present easy-to-use retrieval focused multilingual sentence embedding models, made available on TensorFlow Hub",Theory Proposal,Model Optimization
"we developed a user-friendly workflow management platform, 96 BiomEdical Nlp TOolkits (BENTO), to facilitate the process of building and applying of clinical NLP pipelines.",Applications,Model Proposal
"we present BENTO, a workflow management platform with a graphic user interface (GUI) that is built on top of CodaLab, to facilitate the process of building clinical NLP pipelines",Theory Proposal,Model Optimization
"We introduce Stanza , a Python natural language processing toolkit supporting many human languages",Theory Proposal,New Algorithm/ Method
"We demonstrate that jiant reproduces published performance on a variety of tasks and models, including BERT and RoBERTa.",New Algorithm/ Method,Performance Evaluation
"We introduce MT-DNN, a comprehensive and easily-configurable open-source toolkit for building robust and transferable natural language understanding models.",Model Proposal,Model Optimization
we developed a web-based system LinggleWrite (f.linggle.com) with many assistive writing functions.,New Algorithm/ Method,New Algorithm/ Method
"We release CLIReval, 1 an open-source toolkit that evaluates the quality of MT outputs in the context of a CLIR system, without the need for any actual CLIR dataset.",Performance Evaluation,Performance Evaluation
"We demonstrate that CLIReval can perform as well as popular intrinsic MT metrics on recent WMT metrics shared task, without supervision from external datasets and domain-basedparameter tuning",Dataset Creation,Dataset Creation
we have developed an analysis tool and an interactive tool to assist researchers in diagnosing dialogue systems.,New Algorithm/ Method,New Algorithm/ Method
"We present a framework for bitext cleaning, OpusFilter, focusing on processing data collected in OPUS (Tiedemann, 2012), the world’s largest resource of openly available parallel corpora",Theory Proposal,Model Optimization
"We propose the Label Noise in Context system, or LNIC, which uses the neighborhood surrounding a suspicious example in the training set to improve both precision and explainability.",Theory Proposal,Theory Proposal
We describe LNIC’s nearest-neighbors-based algorithm to improve precision and explainability of automatically detected label noise,New Algorithm/ Method,New Algorithm/ Method
"we developed EXBERT, a tool that combines the advantages of static analyses with a dynamic and intuitive view into both the attentions and internal representations of the underlying model.",New Algorithm/ Method,Model Proposal
We demonstrate that EXBERT can replicate insights from the analysis by Clark et al. (2019) and easily extend it to other models,Performance Evaluation,Performance Evaluation
"We present a web-based system for diacritization of Hebrew text, which caters to both casual and expert users",Theory Proposal,Model Optimization
We provide a web interface for the user to input a text for diacritization and refine the resulting diacritized text,Model Optimization,Model Optimization
"We aim to provide a tool that is useful to casual users and language enthusiasts, but also to experts and professionals who may use it to set scientific editions of historical Hebrew texts.",Model Optimization,Model Optimization
"we present PHOTON, a modular, cross-domain NLIDB that adopts deep learning in its core components",Theory Proposal,Model Optimization
we present the prototype of a new task automation agent named SUGILITE,Theory Proposal,Model Optimization
"we unified the models of different implementation in a single codebase, implemented demos as top-level managers to access different models, and provide strategies to allow more organic integration across the models, including token probability interpolation, cross-mode scoring, latent interpolation, and unified hypothesis ranking",Model Optimization,Model Optimization
"We describe how we built an interactive visual explorer for this unified data, which we refer to as NLP Scholar",Theory Proposal,Theory Proposal
"we introduce Funlines1 , an online game for generating funny news headlines for humor research.",Theory Proposal,New Algorithm/ Method
"We explore and evaluate this fun, competitive way of motivating people to contribute creative text, addressing some of the special challenges of generating humor data mentioned above.",Performance Evaluation,Performance Evaluation
we optimize for efficiency with features that collapse common patterns observed in user testing and components designed for the iterative tuning of the semantic parser exemplars.,Performance Evaluation,Performance Evaluation
"We introduce DIALOGPT, a tunable gigawordscale neural network model for generation of conversational reponses, trained on Reddit data.",Model Proposal,Model Optimization
"we introduce a new version of ADVISER - previously a text-based, multi-domain dialog system toolkit (Ortega et al., 2019) - that supports multi-modal dialogs, including speech, text and vision information processing",Algorithm/Method Optimization,Algorithm/Method Optimization
we present Prta —the PRopaganda persuasion Techniques Analyzer.,Theory Proposal,Model Optimization
"we exploit dilated convolution and n-gram matching mechanism to extract implicit semantic features and explicit semantic features, respectively.",Theory Proposal,Theory Proposal
we develop a system to assist the professional coders in assigning the correct codes.,New Algorithm/ Method,New Algorithm/ Method
"We present ESPnet-ST, a toolkit that implements many of the recent models for E2E-ST, as well as the ASR and MT modules for Cascade-ST.",Theory Proposal,Model Optimization
", we extend ESPnet to ST tasks, providing code for building translation systems and recipes (i.e., scripts that encapsulate the entire training/inference procedure for reproducibility purposes) for a wide range of ST benchmarks.",Theory Proposal,Theory Proposal
"We describe and demonstrate Penman, a Python library and command-line utility for working with AMR data at both the tree and graph levels and for encoding and decoding these structures using PENMAN notation",Theory Proposal,Theory Proposal
"We introduce a web application for writing scientific text with integrated literature discovery, paper reading, and bibliography management capabilities.",Theory Proposal,New Algorithm/ Method
"we present MMPE, the first prototype to combine traditional input modes with pen, touch, and speech modalities for PE of MT.",Theory Proposal,Model Optimization
"We present MMPE, the first translation environment combining standard mouse & keyboard input with touch, pen, and speech interactions for PE of MT",Theory Proposal,Model Optimization
"We introduce Torch-Struct, a library for structured prediction designed to take advantage of and integrate with vectorized, auto-differentiation based frameworks",Theory Proposal,New Algorithm/ Method
we also include a number of general-purpose optimizations to provide cross-algorithm efficiency.,Performance Evaluation,Performance Evaluation
we present a novel approach to building dialog managers (DMs).,Theory Proposal,Model Optimization
"we showcase Conversation Learner, a machine teaching tool for building dialog managers",Theory Proposal,Theory Proposal
"We tackle the challenges involved with consuming vast quantities of news by leveraging modern techniques to semantically cluster stories, as well as innovative summarization methods to extract succinct, informational summaries for each cluster",New Algorithm/ Method,New Algorithm/ Method
"We leverage labeled datasets for DDI identification for supervision, and train a model that transfers to the related task of identifying supplement interactions.",Model Optimization,Dataset Creation
we design task specific architectures to incorporate the now captured explanations into training,Model Proposal,Model Proposal
"We designed, built, and evaluated a fully automated news chatbot that bases its content on a stream of news articles from a diverse set of English news sources",Performance Evaluation,Performance Evaluation
We perform interpretability analysis to learn how these approaches can enhance our understanding of attention behavior and adaptive approaches,Applications,Applications
We provide experimental results on the recent adaptive approaches for the multi-modal input sequences.,Model Optimization,Model Optimization
we propose methods to transfer text style on the story level.,New Algorithm/ Method,New Algorithm/ Method
we define style as the setting of the story which reveals time background and geographical information,Theory Proposal,Theory Proposal
"we describe our novel unsupervised method, which can be implemented without the need for labeled paraphasia data",New Algorithm/ Method,New Algorithm/ Method
We demonstrate the utility of our method as an essential first step in developing augmentative and alternative communication (AAC) devices for patients suffering from aphasia in any language.,New Algorithm/ Method,New Algorithm/ Method
"we propose a novel GCN (Kipf and Welling, 2016)-based MeSH term index model, HGCN4MeSH, which learns the co-occurrence representation of tags via a GCN-based mapping function",Model Proposal,Model Optimization
we design a novel data-driven adjacency matrix to guide the information propagation between nodes.,Model Proposal,Model Proposal
We confirm that selecting training data similar to the learners’ corpus instead of using randomly selected monolingual data improves the performance of the GEC model.,Applications,Applications
We show the effect of realistic pseudo errors by considering the types of errors typically made by language learners for the Russian GEC task.,Theory Proposal,Theory Proposal
"we study the relations among several popular embedding methods, including GloVe (Pennington et al., 2014), SGNS1 (Mikolov et al., 2013), Singular Value Decomposition (SVD) factorization of PMI matrix, and SVD factorization of log count (LC) matrix.",New Algorithm/ Method,New Algorithm/ Method
"we analyze the influence of training processes, i.e. hyperparameters (negative sampling), random initialization; and the influence of corpora towards word embeddings.",Resources,Resources
We propose a word attribute transfer method that obtains a vector with an inverted binary attribute without explicit knowledge.,New Algorithm/ Method,New Algorithm/ Method
The proposed method demonstrates more accurate word attribute transfer for words that have target attributes than other baselines without changing the words that do not have the target attributes.,New Algorithm/ Method,New Algorithm/ Method
"we propose a balancing procedure, based on the a priori ratio between topic capacities.",Theory Proposal,Theory Proposal
"We propose to modify the Transformer architecture (Vaswani et al., 2017) to combine the learned subword representations into word representations in the encoder block.",Theory Proposal,Theory Proposal
We demonstrate that the North KoreanEnglish translation model can be trained effectively on bilingual South Korean-English data by character-level tokenization and phonemelevel decomposition.,Performance Evaluation,Performance Evaluation
"we propose a method to tokenize South Korean input sentences at the character level and decompose them into phonemes to mitigate the grammatical differences between South Korean and North Korean, and demonstrate that the translation model from North Korean to English can be effectively learned using bilingual South Korean-English data.",Model Proposal,New Algorithm/ Method
we present an unsupervised deep learning framework (SCAR) for deletion-based sentence compression,New Algorithm/ Method,Model Optimization
We introduce a novel linkage loss that ties together the compressor and the reconstructor.,New Algorithm/ Method,Theory Proposal
"we propose a model that can identify and focus on abnormal findings more specifically and precisely, similar to the way that physicians would typically read, interpret, and write chest x-ray reports.",Model Proposal,Model Optimization
we investigate a method to exploit the monolingual data of the agglutinative language to enhance the representation ability of the encoder.,New Algorithm/ Method,New Algorithm/ Method
We experimentally compare the relevances produced by our method to those of other black-box and gradient-based explanation approaches.,New Algorithm/ Method,New Algorithm/ Method
We introduce the class zero-sum axiom for explanation methods.,New Algorithm/ Method,New Algorithm/ Method
We introduce NTUs as a novel research object that is capable of advancing our understanding of the interactive and rational aspects of social talk.,New Algorithm/ Method,Theory Proposal
We propose an annotation strategy for exploring NTUs in naturally occurring dialogues.,Theory Proposal,Theory Proposal
We propose to replace the rich linguistic feature templates used in the past approaches with a minimal feature function using contextual vector representations,Theory Proposal,Theory Proposal
We train a BERT model on the Telugu Wikipedia data and use vector representations from this model to train the parser.,New Algorithm/ Method,Model Optimization
we understand how models which perform well under pointwise evaluation may fail in practice and find better methods for evaluating paraphrase identification models,New Algorithm/ Method,New Algorithm/ Method
we present our efforts towards building efficient NMT systems between Indian languages (specifically Indo-Aryan languages) and English by exploiting parallel data from related languages,Performance Evaluation,Performance Evaluation
we propose an interpretable approach for event extraction (EE) that mitigates the tension between generalization and interpretability through multitask learning (MTL).,Theory Proposal,Theory Proposal
"We extend a subset of the BioNLP 2013 GENIA event extraction (Kim et al., 2013) dataset with a
set of rules designed to extract and explain three
of the GENIA biomedical events: protein phosphorylation, localization, and gene expression",Dataset Creation,Dataset Creation
We propose a dataset which maintains a broad scope but which addresses subjectivity,Theory Proposal,Theory Proposal
We propose an effective unsupervised alignment method to tackle the alignment problem.,New Algorithm/ Method,New Algorithm/ Method
we propose a strategy to supplement state-of-theart models with automatically extracted information using basic NLP tools to effectively handle rich morphology.,Model Proposal,Model Optimization
"We describe the first effort at establishing points of correspondence between disparate sentences. Without a clear understanding of points of correspondence, sentence fusion remains a daunting challenge that is only sparsely and sometimes incorrectly performed by abstractive summarizers.",Theory Proposal,Theory Proposal
We present a sizable dataset for sentence fusion containing human-annotated corresponding regions between pairs of sentences,Dataset Creation,Dataset Creation
We developed an uncertainty-aware automatic evaluation method for dialogue systems. Our method automates the human ratings required in ∆BLEU while keeping the performance.,New Algorithm/ Method,New Algorithm/ Method
We showed that integrating υBLEU into RUBER greatly improves RUBER’s performance by providing the robustness to evaluate responses with uncertainty,Performance Evaluation,Performance Evaluation
we present a preliminary morphological analyser for verbs in Nen,New Algorithm/ Method,Model Optimization
we outline a computational approach for modelling the linguistic phenomenon of distributed exponence.,Model Proposal,Model Proposal
"We propose a novel end-to-end Arabic document classification framework, Arabic document imagebased classifier (AraDIC), inspired by the work on image-based character embeddings. AraDIC consists of an image-based character encoder and a classifier",Theory Proposal,Theory Proposal
We propose to integrate label component information as embeddings into models. This procedure consists of two steps: (i) label decomposition and (ii) label embedding calculation.,Model Proposal,Model Optimization
we build a Japanese Wikipedia typo dataset (JWTD) from Japanese Wikipedia’s revision history.,Dataset Creation,Dataset Creation
We introduce a new task formulation of SAS that matches the actual usage,New Algorithm/ Method,Theory Proposal
we seek to develop models that bridge the gap between biological plausibility and linguistic competence,New Algorithm/ Method,Model Optimization
We propose a method to further align representations from such models into the cross-lingual space and use them to derive sentence embeddings.,Model Proposal,New Algorithm/ Method
we present a compositional semantics that maps various comparative constructions in English to semantic representations via Combinatory Categorial Grammar (CCG) parsers and combine it with an inference system based on automated theorem proving,New Algorithm/ Method,Model Optimization
"We introduce a set of new linguistic constraints (i.e. synonyms and antonyms) created with BabelNet for three languages: English, German and Italian.",New Algorithm/ Method,Theory Proposal
"We introduce an improved post-specialization method (dubbed WGAN-postspec), which demonstrates improved performance as compared to state-of-the-art DFFN (Vulic et al. ´ , 2018) and AuxGAN (Ponti et al., 2018) models",New Algorithm/ Method,New Algorithm/ Method
We show that the proposed approach achieves performance improvements on an intrinsic task (word similarity) as well as on a downstream task (dialog state tracking),Theory Proposal,Theory Proposal
We give a novel study of leveraging monolingual corpora of related and unrelated languages for NMT pre-training.,Model Proposal,Model Proposal
We make a comparison of existing and proposed techniques in a variety of corpora settings to verify our hypotheses,Theory Proposal,Theory Proposal
We propose a method that selects a better hypothesis giving high importance to distinct words generated from decoder without the usage of any language model or data,Model Proposal,New Algorithm/ Method
"We aggregate existing datasets into a large disaster dataset using a new annotation scheme. Furthermore, by utilizing a class-mask (elaborated in Section 4.1), we make use of both binary-classification data and multi-class classification data in the same training phase.",Dataset Creation,Dataset Creation
"We explore Manifold Mixup (Verma et al., 2019) in the natural language-based disaster domain. Manifold Mixup is a regularization technique originally introduced in computer vision tasks.",New Algorithm/ Method,Theory Proposal
"we evaluate the syntactic difference between the generated trees, randomly generated trees and gold reference trees produced by constituency parsers;",Performance Evaluation,Performance Evaluation
"we introduce a new dataset containing questions in tweets paired with their prior tweets to provide context. We create classification models to assess the difficulty of distinguishing rhetorical and information-seeking questions, and experiment with different properties of the prior context.",Dataset Creation,Model Optimization
We propose a single step transfer learning based classification method that identifies victim blaming language and labels it.,New Algorithm/ Method,New Algorithm/ Method
"we present various interactive visualization methods such as neuron activations (Karpathy et al., 2015; Dalvi et al., 2019), attention mechanisms (Bahdanau et al., 2014; Strobelt et al., 2018), and saliency measures (Li et al., 2016; Murdoch et al., 2018; Arras et al., 2017), including a walkthrough on how to build a simple attention visualization",New Algorithm/ Method,New Algorithm/ Method
"we learn how to anticipate how a developed technology could be repurposed for harmful or negative results, and designing systems so that they do not inadvertently cause harm.",New Algorithm/ Method,New Algorithm/ Method
"we focus on three main topic areas: 1) grounding in human-human communication; 2) grounding in dialogue systems; and 3) grounding in multi-modal interactive systems, including image-oriented conversations and humanrobot interactions",Theory Proposal,Theory Proposal
"we propose this tutorial on reviewing natural language processing research, focusing on conference submissions and various review forms used in the NLP community.",Theory Proposal,Theory Proposal
"we will introduce evaluation methods for style-conditioned text generation. We will present the current practice in the literature, involving both human evaluation and automatic metrics.",New Algorithm/ Method,New Algorithm/ Method
"we present approaches for information extraction (IE) from Web data that can be differentiated along two key dimensions: 1) the diversity in data modality that is leveraged, e.g. text, visual, XML/HTML, and 2) the thrust to develop scalable approaches with zero to limited human supervision",New Algorithm/ Method,Model Optimization
"we will (1) outline the various types of commonsense (e.g., physical, social), and (2) discuss techniques to gather and represent commonsense knowledge, while highlighting the challenges specific to this type of knowledge (e.g., reporting bias). We will also (3) discuss the types of commonsense knowledge captured by modern NLP systems (e.g., large pretrained language models), (4) review ways to incorporate commonsense knowledge into downstream task models, and (5) present various benchmarks used to measure systems’ commonsense reasoning abilities.",Resources,Resources
"We discuss two-stage retriever-reader frameworks for open-domain QA, pioneered by Chen et al. (2017): a retriever component finding documents that (might) contain an answer from a large collection of documents, followed by a reader component finding the answer in a given paragraph or a document.",Theory Proposal,Theory Proposal
"we investigate the effectiveness of extending ImageNet to Arabic using Arabic WordNet (AWN) by searching in AWN for all the synsets used in ImageNet. AWN was originally developed in 2006 (Black et al., 2006).",New Algorithm/ Method,Model Optimization
"we propose Entity Synset Alignment(ESA), which is a method to create a general scene graph by aligning various semantic knowledge efficiently to solve this bias problem.",Performance Evaluation,Performance Evaluation
"we introduce VQGR, a VQG system that is able to generate natural language questions when shown radiology images",New Algorithm/ Method,Theory Proposal
we propose two new metrics that evaluate how each question contributes to the goal,Performance Evaluation,Performance Evaluation
"We propose a novel alignment mechanism to deal with procedural reasoning on a newly released multimodal QA dataset, named RecipeQA",Theory Proposal,Theory Proposal
we propose a novel method for sentence boundary detection that takes it as a multi-class classification task under the endto-end pre-training framework.,New Algorithm/ Method,New Algorithm/ Method
we proposed a new adversarial training method to leverage target monolingual data to relieve the lowresource shortcoming of speech translation.,New Algorithm/ Method,New Algorithm/ Method
we propose a method to handle the two problems so as to generate robust translation to ASR errors.,New Algorithm/ Method,New Algorithm/ Method
"we propose a novel and effective Encoder-NAD-AD framework for NMT, in which the newly added non-autoregressive decoder (NAD) can provide target-side global information when autoregressive decoder (AD) translates, as illustrated in Figure 1. Briefly speaking, the encoder is first used to encode the source sequence into a sequence of vector representations.",Theory Proposal,Theory Proposal
"We propose a novel and efficient approach to explicitly exploit discourse structure information for documentlevel NMT. Particularly, our approach is applicable for any other context encoder of document-level NMT;",Performance Evaluation,Performance Evaluation
This paper describes our machine translation systems for the streaming Chinese-toEnglish translation task of AutoSimTrans 2020. We present a sentence length based method and a sentence boundary detection model based method for the streaming input segmentation,New Algorithm/ Method,New Algorithm/ Method
we evaluate the joint use of linguistic features and deep learning models. We achieve this fusion by simply taking the output of deep learning models as features themselves.,Performance Evaluation,Performance Evaluation
"we use simulated data to demonstrate that the rate of human-human agreement has a substantial effect on estimates of system performance, making it difficult to compare systems that are evaluated on different datasets",Performance Evaluation,Performance Evaluation
"we are interested in providing feedback specialized to the content of the essay, and specifically for the content areas required by the rubric. A key objective is that the feedback should be localized alongside the relevant essay text.",Resources,Resources
This paper examines one form of spoken language assessment; whether the response from the candidate is relevant to the prompt provided. This will be referred to as off-topic spoken response detection.,Model Optimization,Model Optimization
We propose a novel method for creating a tutoring dialogue collection that exhibits many of the properties needed for training a conversational tutor.,New Algorithm/ Method,New Algorithm/ Method
we employ a novel approach to advancing our understanding of the development of writing in English and German children across school grades using classification tasks.,Algorithm/Method Optimization,Model Optimization
We introduce an annotation scheme to capture the nature of sentence-level revisions of evidence use and reasoning (the ‘RER’ scheme) and apply it to 5th- and 6th-grade students’ argumentative essays.,New Algorithm/ Method,Applications
"we show how a deep-learning based system can outperform feature-based machine learning systems, as well as a string kernel system in scoring essay traits",Applications,Applications
we present an NLP-based approach for tracking the evolution of written language competence in L2 Spanish learners using a wide range of linguistic features automatically extracted from students’ written productions.,New Algorithm/ Method,Model Optimization
we build two datasets of MCQs for second-language learners with distractor selections annotated manually by human experts.,Dataset Creation,Dataset Creation
We develop and train models for automatic distractor selection that combine simple features with representations from pretrained models like BERT and ELMo,Algorithm/Method Optimization,Model Optimization
We annotate a small corpus of methodology sections drawn from Spanish information technology theses for the presence of steps and their logical order.,New Algorithm/ Method,New Algorithm/ Method
"We design a model to detect sentences that represent methodological steps, incorporating language model and verb taxonomy features, achieving 0.939 f-measure.",New Algorithm/ Method,New Algorithm/ Method
we set out methodological considerations of using automated speech recognition to build a corpus of teacher speech in an Indonesian language classroom,New Algorithm/ Method,New Algorithm/ Method
we describe a set of CR formative assessment items that call for students to express and integrate ideas across multiple dimensions of the NGSS.,Theory Proposal,Theory Proposal
"we present a novel learning-andassessment context where middle school students were asked to criticize an argument presented in the prompt, focusing on identifying and explaining the reasoning flaws.",New Algorithm/ Method,Model Optimization
"we investigate whether, in automated essay scoring (AES) research, deep neural models are an appropriate technological choice. We find that fine-tuning BERT produces similar performance to classical models at significant additional cost.",Performance Evaluation,Performance Evaluation
We develop custom g-transformations: token-level edits to perform (g)rammatical error corrections. Predicting g-transformations instead of regular tokens improves the generalization of our GEC sequence tagging system.,New Algorithm/ Method,New Algorithm/ Method
"We decompose the fine-tuning stage into two stages: fine-tuning on errorful-only sentences and further fine-tuning on a small, high-quality dataset containing both errorful and error-free sentences.",Dataset Creation,Dataset Creation
"We achieve superior performance by incorporating a pre-trained Transformer encoder in our GEC sequence tagging system. In our experiments, encoders from XLNet and RoBERTa outperform three other cutting-edge Transformer encoders (ALBERT, BERT, and GPT-2)",Applications,Applications
we propose a method for interpreting the weights of personalized neural CWI models.,Model Proposal,New Algorithm/ Method
"we propose an approach for automatically evaluating their appropriateness. Using neural machine translation, we generate correct-incorrect sentence pairs to serve as synthetic data in order to increase the amount and diversity of training data for our scoring model",Model Proposal,Model Optimization
we present work on automatically scoring student responses to constructed-response mathematics items where the response should contain both text and mathematical equations or expressions.,New Algorithm/ Method,Model Optimization
we explore whether approaches from the field of transfer learning may be useful for improving item parameter modeling.,Model Proposal,Model Proposal
"we provide a fair comparison of two methods for generating synthetic parallel data for GEC, using two evaluation datasets;",New Algorithm/ Method,New Algorithm/ Method
we look at the temporal change of gender bias in biomedical research.,Theory Proposal,Theory Proposal
"We answer the question; How has the usage of gender stereotypes changed in the last 60 years of biomedical research? Specifically, we look at the change in well-known gender stereotypes (e.g., Math vs Arts, Career vs Family, Intelligence vs Appearance, and occupations) in biomedical literature from 1960 to 2020.",Theory Proposal,Theory Proposal
"we are the first to employ a novel, completely unsupervised end-to-end neural attention-based document representation learning approach, using no external labels, in order to achieve the most meaningful term transfer between related documents, i.e. semantic tagging of documents, in a “pseudorelevance feedback”–based (Xu and Croft, 2000) setting for unsupervised query expansion.",New Algorithm/ Method,Model Optimization
"We present a search system that works in a paradigm which we call Extractive Search, and which allows rapid information seeking queries that are aimed at extracting facts, rather than documents.",New Algorithm/ Method,Model Proposal
"we adapt ESP to encode dependency paths, an approach we call Embedding of Structural Dependencies (ESD).",Applications,Applications
"We compare two methods for learning biomedical concept embeddings, the skip-gram with negative sampling (SGNS) algorithm (Mikolov et al., 2013a) and Embedding of Semantic Predications (ESP) (Cohen and Widdows, 2017), which adapts SGNS to encode concept-predicate-concept triples.",New Algorithm/ Method,New Algorithm/ Method
"we introduce in more detail the notion of spin, the types of spin that we address, and the information that is required to assess an article for spin",New Algorithm/ Method,Theory Proposal
"we describe our current algorithms, methods employed and provide their evaluation.",New Algorithm/ Method,New Algorithm/ Method
"we introduce construction of RadVisDial - the first publicly available dataset for visual dialog in radiology, derived from the MIMIC-CXR (Johnson et al., 2019) dataset",Dataset Creation,Dataset Creation
we compare several state-of-the-art models for VQA and VisDial applied to these images,Model Proposal,Model Proposal
"our model operates in the relation extraction setting, meaning it must distinguish between relations and nonrelations, as well as classifying by relation type.",Model Proposal,Model Proposal
"We introduce a pooled embedding for relational classification across long distances. Wang et al. (2019) focused on short-distance relations, but clinical CONTAINS relations often span multiple sentences, so a sequence-level embedding is necessary for such long-distance inference.",New Algorithm/ Method,Theory Proposal
we present an experimental evaluation of coding coverage in the MIMIC-III discharge summaries.,New Algorithm/ Method,Model Proposal
"we further propose to combine reinforcement learning (RL) to automatically extract out task-specific noisy text from the long documents, as we observe that many text segments do not contain predictive information such that removing these noise can potentially improve the performance.",Theory Proposal,Theory Proposal
"We model the noise extraction process as a sequential decision problem, which also aligns with the fact that clinical documents are received in time-sequential order",Model Proposal,Model Proposal
"we study the impact of a number of model design choices. First, following Reimers and Gurevych (2019), we study the impact of various pooling methods on STS, and find that convolution filters coupled with max and mean pooling outperform a number of alternative approaches",New Algorithm/ Method,New Algorithm/ Method
"We establish state-of-the-art benchmarks for EMR QA on a large clinical question answering dataset, emrQA",Dataset Creation,Dataset Creation
"We introduce and evaluate new models, achieving SOTA performance for this task.",Performance Evaluation,Performance Evaluation
we show that machine learningbased unsupervised clustering of and anomaly detection with linguistic biomarkers are promising approaches for intuitive visualization and personalized early stage detection of Alzheimer’s disease.,Theory Proposal,Theory Proposal
"we introduce BIOMRC, a new dataset for biomedical MRC that can be viewed as an improved version of BIOREAD.",Dataset Creation,Dataset Creation
"we propose a simple and intuitive neural model to reinstate migrating words that transpire in letter position dyslexia, a visual analysis deficit to the encoding of character order within a word",Model Proposal,Model Optimization
"We propose a document classification approach to determine the reason for administration of a given drug, with particular focus on domain adaptation from one drug to another, and instance selection to minimize annotation effort.",Theory Proposal,Theory Proposal
our work explores methods to improve the performance of classifying the indication for an antibiotic administration in veterinary records of dogs and cats.,New Algorithm/ Method,New Algorithm/ Method
"We make the embeddings, code, and other materials publicly available and outline several avenues of future work to facilitate progress in the field.",Performance Evaluation,Performance Evaluation
"We train five recent KGE models on SNOMED-CT and demonstrate their advantages over previous methods, making a case for the importance of leveraging the multirelational nature of knowledge graphs for biomedical knowledge representation.",New Algorithm/ Method,New Algorithm/ Method
"We investigate different types of errors that are penalized by exact F-score and identify a specific error type where there is high degrees of disagreement between the human user experience and what exact F-score measures: namely, errors where the extracted entity is correctly labeled, but the span only overlaps with the annotated entity rather than matching perfectly",Performance Evaluation,Performance Evaluation
"We demonstrate that the simple applications of this model under-perform and require knowledge base order-sensitive markings, ktag, to achieve state-of-the-art performance. This data encoding scheme captures the latent relation direction and provides a simple way to reduce noise in distant supervision.",Model Optimization,Model Optimization
we propose a new neural network model that combines multi-head attention mechanisms with a set of convolutions to provide global locality in biomedical event and relation extraction,Model Proposal,Model Optimization
we investigate the use of MTL with transformer-based models (BERT) on multiple biomedical and clinical NLP tasks,Performance Evaluation,Performance Evaluation
we present the dataset used for our experiments and their respective results,Dataset Creation,Dataset Creation
our paper introduces a multi-modal approach for fine-grained opinion mining,New Algorithm/ Method,Theory Proposal
"Our proposed model, at the time of writing, out-performs the state of the art on a benchmark dataset on a variety of accuracy and regression metrics",Model Proposal,Model Optimization
"we combine ideas from (Tsai et al., 2019) and (Liu et al., 2018) and explore the use of Transformer (Vaswani et al., 2017) based models for both aligned and unaligned signals without extensive over-parameterization of the models by using multiple modality-specific transformers.",Model Proposal,Model Proposal
"a cross-situational learning based grounding framework is proposed that allows grounding of words and phrases through corresponding percepts without human supervision and online, i.e. it does not require any explicit training phase, but instead updates the obtained mappings for every new encountered situation.",Theory Proposal,Theory Proposal
"We propose a multimodal analytical framework that analyzes the candidate in an interview scenario and provides feedback for predefined labels such as engagement, speaking rate, eye contact, etc.",Theory Proposal,Theory Proposal
"we discuss the benefits of a multimodal understanding of in-cabin utterances by incorporating verbal/language input together with the non-verbal/acoustic and visual cues, both from inside and outside the vehicle (e.g., passenger gestures and gaze from in-cabin video stream, referred objects outside of the vehicle from the road view camera stream).",Theory Proposal,Theory Proposal
"we recommend an AI sensing system that can semantically interpret the environmental conditions, objects, relations and activity carried out from the visual feed.",Resources,Resources
We analyze popular VQA models through the lens of attribution (input’s influence on predictions) to gain valuable insights,Resources,Resources
"We adopt the PU algorithm of Peng et al. (2019) to the domain of consumer electronic product descriptions, and evaluate its effectiveness on four entity types: Product, Component, Brand and Attribute",Performance Evaluation,Performance Evaluation
"we present SessionPath, a novel neural network model that improves facet suggestions on two counts: first, the model is able to leverage session embeddings to provide scalable personalization; second, SessionPath predicts facets by explicitly producing a probability distribution at each node in the taxonomy path.",New Algorithm/ Method,Model Proposal
we formulate the recommendation problem into a supervised product embedding learning process,Theory Proposal,Theory Proposal
"we target and use real-world data - service calls, which poses additional challenges with respect to the artificial datasets that have been typically used in the past in multimodal sentiment researches (Cambria et al., 2017), such as variability and noises.",Dataset Creation,Dataset Creation
We demonstrate the success of XLNet on finding product specifications that can help answering product related queries. It beats the baseline Siamese method by 0.14 − 0.31 points in HIT@1.,New Algorithm/ Method,New Algorithm/ Method
we present our work to improve the intent classification in the shopping assistant of Walmart company by using inter-utterance context. Our work also reduces the contextual disambiguation burden from the dialog manager,New Algorithm/ Method,Model Proposal
We propose a semi-supervised iterative approach to collect user complaints about a service from social media platforms,Theory Proposal,Theory Proposal
We evaluate the proposed approach for the problem of complaint detection for transportation services on Twitter.,Performance Evaluation,Performance Evaluation
"we propose a novel approach for item-based collaborative filtering, by leveraging the BERT model (Devlin et al., 2018) to understand item titles and model relevance between different items",Model Proposal,Model Optimization
"We proposed a semisupervised method, and successfully applied it to product reviews from different categories in the ecommerce platform",New Algorithm/ Method,New Algorithm/ Method
"We propose a Deep Hierarchical Classification framework, which incorporates the multi-scale hierarchical information in neural networks and introduces a representation sharing strategy according to the category tree.",Theory Proposal,Theory Proposal
we introduced SimsterQ - a clustering based system for answering questions that makes use of word vectors. Clustering was performed using cosine similarity scores between sentence vectors of reviews and questions,New Algorithm/ Method,Theory Proposal
"We propose a novel use of sentiment analysis by examining a key section of the quarterly and annual reports submitted to the SEC in two states: first, the unaltered report as filed with the SEC (X 0 ), and second, the report without selected NGMs (X). We then calculated the change in the tone or sentiment (as we use these terms interchangeable) as (X - X0 ) for each report and used it as an input to our prediction model",Model Proposal,Model Optimization
we study the applicability of Bayesian Parametric and Non-parametric methods for user clustering in an E-commerce search setting.,New Algorithm/ Method,New Algorithm/ Method
We propose a joint training setup in which sentence selection and claim verification are tackled by a single neural sequence matching model.,Model Proposal,Model Optimization
we simplify the training procedure and increase training efficiency for sentence selection and claim verification by merging redundant components and computation that exist when training the two tasks separately.,Performance Evaluation,Performance Evaluation
We describe the methodology for creating the corpus and the annotation process.,New Algorithm/ Method,New Algorithm/ Method
we propose a probabilistic graphical model which formulates fact extraction in a generative process.,Model Proposal,Model Optimization
"we leverage this implicit knowledge to create an effective end-to-end fact checker using a solely a language model, without any external knowledge or explicit retrieval components",Model Proposal,Model Proposal
We propose two measures for measuring the quality of constructed claims in the FEVER task. Annotating data for this task involves the creation of supporting and refuting claims over a set of evidence,Theory Proposal,Theory Proposal
"We propose, instead, a model-agnostic framework that consists of two modules: (1) a span extractor, which identifies the crucial information connecting claim and evidence; and (2) a classifier that combines claim, evidence, and the extracted spans to predict the veracity of the claim",Model Proposal,Model Optimization
"we report on the shared task on sarcasm detection that we conducted as part of the2nd Workshop on Figurative Language Processing
(FigLang 2020) at ACL 2020. The task aims to
study the role of conversation context for sarcasm
detection.",Resources,Resources
"We propose a new data augmentation technique that can successfully leverage the structural patterns of the conversational dataset. Our technique, called CRA(Contextual Response Augmentation), utilizes the conversational context of the unlabeled dataset to generate new training samples.",Theory Proposal,Theory Proposal
"We propose an architecture where the Transformer Encoder is stacked with BiLSTM (Schuster and Paliwal, 1997) and NeXtVLAD (Lin et al., 2018). We observe that NeXtVLAD, a differentiable pooling layer, proves more effective than simple nonparametric mean/max pooling methods.",New Algorithm/ Method,New Algorithm/ Method
"We present the shared task and provide a brief description of each of the participating systems, a comparative evaluation of the systems, and our observations about trends in designs and performance of the systems that participated in the shared task",New Algorithm/ Method,Model Proposal
we propose an end-to-end neural based method named DeepMet for detecting metaphor by transforming the token-level metaphor detection task into the reading comprehension task.,New Algorithm/ Method,New Algorithm/ Method
"We propose a novel approach (as detailed in Figure 1) wherein we first construct a dataset of realworld context–satirical headline pairs in which the context is constructed by procedurally retrieving and ranking real-world stories, events and information related to the entities that appear in the original satirical headline.",Theory Proposal,Theory Proposal
"we introduce a novel approach for modeling satirical news headlines as conditioned on a real-world context, and an information retrieval pipeline for constructing the real-world context for a given real satirical headline",Model Proposal,Model Optimization
"we explore the use of contextualized word embeddings for detecting sarcasm in the responses sampled from Reddit as well as Twitter. We outline the effect of adding contextual information, from previous dialogue turns, to the response, for both the datasets.",Algorithm/Method Optimization,Algorithm/Method Optimization
we propose using machine learning techniques with BERT and GloVe embeddings to detect sarcasm in tweets.,Theory Proposal,Theory Proposal
we present our study on the effectiveness of contextual information to decide if anutterance is sarcastic or not.,Theory Proposal,Model Proposal
"we propose to employ bidirectional encoder representations transformers (BERT), and aspect-based sentiment analysis approaches in order to extract the relation between context dialogue sequence and response and determine whether or not the response is sarcastic.",Theory Proposal,Theory Proposal
"we present traditional Machine learning approaches, Deep learning approach (RNN-LSTM) and BERT (Bidirectional Encoder Representations from Transformers) for identifying sarcasm.",Theory Proposal,Model Proposal
we present a deep neural architecture for sarcasm detection.,Theory Proposal,Model Proposal
"We investigate various pre-trained language representation models (PLRMs) like BERT, RoBERTa, etc. and fine-tune it on the Twitter dataset1",Model Optimization,Dataset Creation
we describe the work we performed for context aware sarcasm detection for both the data sets.,Dataset Creation,Dataset Creation
"We present different techniques and models, mostly based on transformer for Sarcasm Detection with Context.",Theory Proposal,Model Proposal
we propose a novel deep learning-based approach to detect whether an utterance is sarcastic or non-sarcastic by utilizing the given contexts in a hierarchical manner.,Theory Proposal,Theory Proposal
"We perform a comparative study of our different versions of BERT-based model with other variants of LSTM model and XLNet (Yang et al., 2019) (both using the estimated number of conversation sentences) and find out that BERT-based models outperformed them.",Applications,Applications
"we aim to detect token-level metaphors from plain texts by focusing on content words (Verbs, Nouns, Adjectives and Adverbs) of two corpora: VUA1 and TOFEL2",Theory Proposal,Theory Proposal
we design an ALBERTBiLSTM structure to recognize metaphorical words in TOEFL dataset,Dataset Creation,Dataset Creation
"we use concatenation of GloVe (Pennington et al., 2014) and ELMo (Peters et al., 2018) vectors augmented with character level features using CNN and highway network (Kim et al., 2016; Srivastava et al., 2015)",Theory Proposal,Theory Proposal
"We propose two models for metaphor detection1 with the input prepared as above - a vanilla BiLSTM model and a vanilla Transformer Encoder (Vaswani et al., 2017) model similar to BERT (Devlin et al., 2019) (but without pre-training).",Model Proposal,Model Optimization
This work explores the differences and similarities between neural image classifiers’ mis-categorisations and visually grounded metaphors - that we could conceive as intentional mis-categorisations,Theory Proposal,Theory Proposal
we introduce a gold standard data set of human x-phemism judgments and evaluate our models for this task.,Performance Evaluation,Performance Evaluation
we build our metaphor detection model upon RoBERTa to leverage its strength in capturing contextual information.,Model Proposal,Model Proposal
we are interested in relation-level metaphor identification focusing on the data availability for this level of processing.,Dataset Creation,Dataset Creation
we report preliminary results from applying this approach to two distinct scenarios: debates on gun rights and marriage equality,Applications,Resources
we describe computational ethnography studies to demonstrate how machine learning techniques can be utilized to exploit bias resident in language data produced by communities with online presence.,Theory Proposal,Model Proposal
"we set out a preliminary investigation of oxymorons based on naturally occurring data from Italian, with a view to contributing to the NLP-oriented research on figurative language by supplying an initial list of oxymorons and oxymoronic structures that can be used for further analyses and for evaluation tasks.",Dataset Creation,Dataset Creation
"Proposing the first model for humor style transfer, building a transformer model that “translates” from regular to humorous English1",Model Proposal,Model Proposal
we investigate whether Gao et al. (2018)’s findings can be replicated when detecting metaphors in TOEFL essays rather than the BNC,Performance Evaluation,Performance Evaluation
we present a novel resourceinexpensive architecture for metaphor detection based on a residual bidirectional long short-term memory and conditional random fields.,Theory Proposal,New Algorithm/ Method
we investigate the supervised disambiguation of potential occurrences of German VIDs,Performance Evaluation,Performance Evaluation
"we participate in the 2020 Metaphor Detection Shared Task (Leong et al., 2020).",Theory Proposal,Theory Proposal
we present the first neural metaphor processing architecture that models a broader discourse through the use of attention mechanisms,Theory Proposal,New Algorithm/ Method
"we present our results from the Second Shared Task on Metaphor Detection, hosted by the Second Workshop on Figurative Language Processing",Theory Proposal,New Algorithm/ Method
We propose using both BERT and XLNet language models to create contextualized embeddings and a bidirectional LSTM to identify whether a given word is a metaphor.,Model Proposal,Model Proposal
We present an ensemble approach for the detection of sarcasm in Reddit and Twitter responses in the context of The Second Workshop on Figurative Language Processing held in conjunction with ACL 20201 .,Theory Proposal,New Algorithm/ Method
we present a transformer-based sarcasm detection model that takes both the target utterance and its context and predicts if the target utterance involves sarcasm.,Theory Proposal,New Algorithm/ Method
"we explore teacher-student distillation as a means of increasing the efficiency of neural network systems used to undertake a core task in NLP, dependency parsing.",Performance Evaluation,Performance Evaluation
develops a novel approach to the problem based on general graph parsing techniques;,Algorithm/Method Optimization,Model Optimization
proposes and evaluates different ways of integrating ‘external’ grammatical information;,Performance Evaluation,Performance Evaluation
We propose an end-to-end variational autoencoding parsing (VAP) model for semisupervised graph-based projective dependency parsing,Model Proposal,Model Proposal
we define a neural-network left-corner parser with bounded stack memory for parsing and psycholinguistic prediction.,Theory Proposal,Theory Proposal
we consider a softer version of homomorphic encryption in the form of obfuscation for natural language,Model Optimization,Model Optimization
"we present a generalization of this concept: latent-variable semiring parsing. With our framework, any semiring weighted logic program can be latentified by transforming weights from scalar values of a semiring to rank-n arrays, or tensors, of semiring values, allowing the modeling of latent variables within the semiring parsing framework",Theory Proposal,New Algorithm/ Method
we are proposing a new nonterminal naming scheme for hybrid grammars. We hypothesize that these steps are complementary in improving the accuracy of the parsing model.,Model Proposal,Model Proposal
"we lay the theoretical foundations for a supertagging-based LCFRS parser. As LCFRS obtained from corpora such as the PTB are usually not lexical, we employ a lexicalization procedure",Theory Proposal,Theory Proposal
"we propose SS-PRPN, a semi-supervised extension of the UP architecture PRPN (Shen et al., 2018a), which can be trained jointly on language modeling and supervised parsing.",Model Proposal,Model Proposal
we try to improve both speed and accuracy of chart-based parsers,Theory Proposal,Theory Proposal
Development of a robust dependency parsing model using the latest transformer encoder.,Algorithm/Method Optimization,Model Proposal
development of a deep parser for Spanish that uses a HPSG grammar and returns trees that contain both syntactic and semantic information.,Algorithm/Method Optimization,Model Proposal
"we introduce the task of parsing into enhanced universal dependencies, describes the datasets used for training and evaluation, and evaluation metrics",Dataset Creation,Dataset Creation
We present the approach of the TurkuNLP group to the IWPT 2020 shared task on Multilingual Parsing into Enhanced Universal Dependencies.,Theory Proposal,New Algorithm/ Method
"we describe our system to predict enhanced dependencies for Universal Dependencies (UD) treebanks, which ranked 2nd in the Shared Task on Enhanced Dependency Parsing with an average ELAS of 82.60%. Our system uses a hybrid two-step approach.",Theory Proposal,Theory Proposal
we presents our parsing approach to the Shared Task on Enhanced Universal Dependencies at IWPT 202,Theory Proposal,New Algorithm/ Method
We present the system submission from the FASTPARSE team for the EUD Shared Task at IWPT 2020.,Theory Proposal,New Algorithm/ Method
"we exploit a hybrid approach, coupling an algorithmic graph transformation of the dependency tree with predictions made by a multitask machine learning model.",New Algorithm/ Method,New Algorithm/ Method
This paper presents the system used in our submission to the IWPT 2020 Shared Task. Our system is a graph-based parser with secondorder inference,Theory Proposal,New Algorithm/ Method
"In this paper, we present the submission of team CLASP to the IWPT 2020 Shared Task on parsing enhanced universal dependencies",Theory Proposal,Model Optimization
"We present Køpsala, the Copenhagen-Uppsala system for the Enhanced Universal Dependencies Shared Task at IWPT 2020.",Theory Proposal,New Algorithm/ Method
This paper presents our system at the IWPT 2020 Shared Task on Parsing into Enhanced Universal Dependencies,Theory Proposal,New Algorithm/ Method
"This paper describes the ON-TRAC Consortium translation systems developed for two challenge tracks featured in the Evaluation Campaign of IWSLT 2020, offline speech translation and simultaneous speech translation.",New Algorithm/ Method,New Algorithm/ Method
This paper describes KIT’s submissions to the IWSLT2020 Speech Translation evaluation campaign.,Performance Evaluation,Performance Evaluation
we build a simultaneous translation system for text-to-text(t2t) and speech-to-text(s2t) problems based on Transformer wait-k model,Model Proposal,Model Proposal
we describe the system submitted to the IWSLT 2020 Offline Speech Translation Task. We adopt the Transformer architecture coupled with the meta-learning approach to build our end-to-end Speechto-Text Translation (ST) system,Theory Proposal,Theory Proposal
Our system is an endto-end model based on an adaptation of the Transformer for speech data.,Applications,Applications
"This paper describes the submission to IWSLT 2020 End-to-End Speech Translation task by Samsung R&D Institute, Poland",Theory Proposal,Theory Proposal
We propose a few improvements to our previous system,Algorithm/Method Optimization,Algorithm/Method Optimization
"This paper describes the University of Helsinki Language Technology group’s participation in the IWSLT 2020 offline speech translation task, addressing the translation of English audio into German text",Theory Proposal,Theory Proposal
"This paper describes the LIT Team’s submission to the IWSLT2020 open domain translation task, focusing primarily on Japanese-toChinese translation direction.",Theory Proposal,Theory Proposal
"we demonstrate our system applied for the IWSLT 2020 open domain text translation task, which aims to translate Chinese from/to Japanese 1",Performance Evaluation,Performance Evaluation
This paper describes the University of Edinburgh’s neural machine translation systems submitted to the IWSLT 2020 open domain Japanese↔Chinese translation task.,Theory Proposal,Theory Proposal
"sambiguation (Tang et al., 2018a,b). In this paper, we describe our Transformer based neural machine translation system submitted to the IWSLT 2020 Chinese→Japanese and Japanese→Chinese open domain translation task",Theory Proposal,Model Optimization
establishing an efficient data pre-processing pipeline for large web-crawled corpora to train a transformer model for NMT and exploiting large amount of monolingual data with back-translation and language modeling.,Performance Evaluation,Performance Evaluation
we introduce University of Tsukuba’s submission to the IWSLT20 Open Domain Translation Task.,New Algorithm/ Method,Theory Proposal
"we describe the Xiaomi’s neural machine translation (NMT) systems evaluated at IWSLT 2020 (Ansari et al., 2020) shared open domain translation task in two directions, Chinese→Japanese (Zh→Ja) and Japanese→Chinese (Ja→Zh)",Performance Evaluation,Performance Evaluation
"This paper describes the neural machine translation (NMT) system of the Institute of Scientific and Technical Information of China (ISTIC) for the 17th International Conference on Spoken Language Translation (IWSLT 2020) (Ebrahim et al., 2020)",Theory Proposal,Theory Proposal
"we describe the data and training pipeline for building our NMT system. We start with the two datasets provided by the shared task organizers—the existing parallel (EP) 167 dataset that includes public, parallel sentences, as well as the Web crawled (WC) dataset created by crawling, aligning, and filtering JA-ZH parallel sentences from the Web",Dataset Creation,Dataset Creation
we propose a novel domain adaptation method through style transfer of out-of-domain data using unsupervised machine translation.,New Algorithm/ Method,New Algorithm/ Method
we propose a framework for disfluency removal that utilizes a simple noise induction technique for data augmentation using fluent monolingual text in the target language.,Theory Proposal,Theory Proposal
we present details of our system in the IWSLT Video Speech Translation evaluation.,Theory Proposal,Theory Proposal
"This paper describes our submission to Non-Native Speech Translation Task in IWSLT 2020 (Ansari et al., 2020).",Theory Proposal,Theory Proposal
"This paper describes the submission of the EU project ELITR (European Live Translator)1 to the non-native speech translation task at IWSLT 2020 (Ansari et al., 2020).",Theory Proposal,Theory Proposal
"we propose Speech Translation as an alternative to the template creation process. We experiment with cascade systems, i.e. pipelined ASR+MT architectures, and direct, end-to-end ST systems",Theory Proposal,Theory Proposal
We evaluate a combination of retranslation techniques that have not previously been studied together,Performance Evaluation,Performance Evaluation
"We provide the first empirical comparison of re-translation and streaming models, demonstrating that re-translation operating in a very low-revision regime can match or beat the quality-latency trade-offs of streaming models",Model Optimization,Model Optimization
"We test a 0-revision configuration of re-translation, and show that it is surprisingly competitive, due to the effectiveness of data augmentation with prefix pairs.",Performance Evaluation,Performance Evaluation
"We therefore propose the use of Adaptive Computation Time (Graves, 2016) for simultaneous machine translation",Theory Proposal,Theory Proposal
we integrate a source chunk boundary detection component into a bidirectional recurrent NMT model.,Model Proposal,Model Proposal
this work investigates ASR with output compression. We test our approaches on German TV subtitles,Performance Evaluation,Performance Evaluation
we present research work to enhance a STST pipeline in order to comply with the timing and rendering requirements posed by cross-lingual automatic dubbing of TED Talk videos,New Algorithm/ Method,New Algorithm/ Method
we highlight several of such language mixing phenomena related to the task of localization for translation and focus on two distance (miles to kilometers) and temperature (Fahrenheit to Celsius) conversion tasks.,Model Optimization,Model Optimization
we propose a solution for automatic punctuation that is both cost efficient and easy to train,Performance Evaluation,Performance Evaluation
we detail two non-invasive ways of detecting translationese,Theory Proposal,Theory Proposal
we compare translationese across human and machine translations from text and speech.,Theory Proposal,Theory Proposal
"we introduced a new shared task, organised by Duolingo, which encouraged models to produce as many correct translations as possible for a given input.",Model Proposal,Model Optimization
we propose a one-to-many text style transfer framework that can be trained using non-parallel text,Theory Proposal,Theory Proposal
"we have proposed a novel procedure for training encoder-decoder models, where the softmax function is applied to the output of each of the M decoder layers derived using the output of each of the N encoder layers",Model Proposal,Model Proposal
we propose an improved method of scaling the quantization centres,New Algorithm/ Method,New Algorithm/ Method
we propose a new approach that enables NMT systems to effectively adapt to a new domain using few-shots learning,Theory Proposal,Theory Proposal
"we collect, rank and evaluate a new publicly available headline paraphrase corpus (ParaPhraser Plus), and then perform text generation experiments with manual evaluation on automatically ranked corpora using the Universal Transformer architecture.",Performance Evaluation,Performance Evaluation
we propose to train an end-to-end XLS model to directly generate target language summaries given the source articles by matching the semantics of the predictions with the semantics of the source language summaries,Model Proposal,Model Proposal
"We propose a question type driven framework for AG-QG, which enables the model to generate diverse questions with high quality",Model Proposal,Model Proposal
We develop methods for producing text headings and section-level embeddings through a new task: generation of section titles for Wikipedia articles,New Algorithm/ Method,New Algorithm/ Method
"we explore unexpected and erroneous changes in the output of NMT models. Consider the simple example in Table 1 where the Transformer model (Vaswani et al., 2017) is used to translate very similar sentences.",Model Proposal,Model Proposal
Our approach can be used to mitigate problems commonly associated with language models,Model Proposal,Model Proposal
we perform a large-scale empirical analysis to attempt to discover best practices when using knowledge distillation in combination with domain adaptation,Applications,Applications
we introduce a system built for the Duolingo Simultaneous Translation And Paraphrase for Language Education (STAPLE) shared task at the 4th Workshop on Neural Generation and Translation,New Algorithm/ Method,New Algorithm/ Method
"we focus on two approaches, both based on VAE: one that attempts to achieve the diversity by generalizing the sentence representation produced by the encoder; and another which randomly perturbs the encoder’s output during the sentence generation.",New Algorithm/ Method,New Algorithm/ Method
"we demonstrate that even though we use a simple approach, it is possible to generate varied paraphrased transcriptions which do not simply replace one word with another, contrarily, it utilizes different styles, opposition, word order etc.",Performance Evaluation,Performance Evaluation
we propose a transferlearning-based simultaneous translation model by extending BART,Model Proposal,Model Proposal
"we experiment with various methods to improve the diversity of translations, while preserving their quality",New Algorithm/ Method,New Algorithm/ Method
we propose to address the STAPLE task primarily as a MT task to better understand the strengths and weaknesses of neural MT architectures for generating multiple learner-relevant translations,Model Optimization,Model Optimization
We find that stronger BLEU performance of the beam-search generated translation is not indicative of improvements on the task metric—weighted macro F1 of a set of hypotheses—and suggest this should encourage further research on how to train NMT models when n-best lists are needed (§7.1).,Applications,Applications
This paper describes the third place submission to the shared task Mayhew et al. (2020) on simultaneous translation and paraphrasing for language education at the 4th workshop on Neural Generation and Translation (WNGT) for ACL 2020.,Theory Proposal,Theory Proposal
"We also optimized the Transformer model decoding in engineering, such as caching the decoder’s attention results and using low precision data type.",Dataset Creation,Dataset Creation
"This paper describes the OpenNMT (Klein et al., 2017) submissions to the Workshop on Neural Generation and Translation 2020 efficiency shared task.",Performance Evaluation,Performance Evaluation
This paper describes the University of Edinburgh’s submissions to the Workshop on Neural Generation and Translation (WNGT) 2020 Efficiency Shared Task1 using the Marian machine translation toolki,Performance Evaluation,Performance Evaluation
we aim to demonstrate performance optimization in a particular domain by training document-level models on large out-of domain parallel corpus combined with small in-domain corpus using domain adaptation techniques.,Performance Evaluation,Performance Evaluation
we introduce a new task called Simultaneous Translation and Paraphrasing for Language Education (STAPLE).,New Algorithm/ Method,Theory Proposal
"We propose a novel KB-QA system, MULTIQUE, that combines information from curated and extracted knowledge bases to answer complex questions.",Theory Proposal,Theory Proposal
"Dataset for a task of classifying type of logical statement, gathered on MTurk platform and consisting of 851 sentences, belonging to six classes.",Dataset Creation,Dataset Creation
We provide novel analysis and selection of interpolation coefficients for combining global models with user-personalized models.,Performance Evaluation,Performance Evaluation
we address the problem of having insufficient data collection methodologies by proposing a novel approach that accelerates the data collection process for use in NL-to-QL models,Model Proposal,Model Proposal
we formulate text normalization and sanitization as a multi-task text generation approach and propose a neural pointer-generator network based on multihead attention,Theory Proposal,Theory Proposal
we explore the possibility of mitigating the problems related to ASR inconsistency and code-switching in our input data by using two alternate representations of text in our NLU model: ISO-15919 and IndicSOUNDEX. ISO-159191 was developed as a standardized Latin-based representation for Indic languages and scripts,Model Proposal,Model Proposal
we propose some simple alternatives and show that they lead to 13 better performance.,Model Optimization,Model Optimization
We introduce copy mechanism for BERTbased models with a unified encoder-decoder framework for question generation. We further extend this copy mechanism using selfattentions.,Model Proposal,Model Optimization
"we address the problems of training on a domain with effectively limitless possible vocabulary, and aim to create a DST system capable of scaling to unseen vocabulary at inference. We do this by first utilizing a language model (LM) based Transformer that is capable of handling any possible input and output in a textual manner, letting the same exact architecture scale to new intents, slots, and slot values, with no modifications needed",Model Proposal,Model Proposal
"we propose to use efficient dual sentence encoders such as Universal Sentence Encoder (USE) (Cer et al., 2018) and ConveRT (Henderson et al., 2019b) to support intent detection",Performance Evaluation,Performance Evaluation
"we present a vastly simplified, single-layer convolutional model (Kim, 2014; Bai et al., 2018) that is highly compressible but nonetheless achieves competitive results on task-oriented natural language understanding benchmarks",New Algorithm/ Method,New Algorithm/ Method
"we propose DLGNet, a transformer-based model for multi-turn dialogue modeling that addresses some of the highlighted problems above",Model Proposal,Model Proposal
"we propose a simple data augmentation method leveraging a confusionmatrix-based ASR error simulator (Fazel-Zarandi et al., 2019; Schatzmann et al., 2007).",New Algorithm/ Method,New Algorithm/ Method
we explore automating the creation of a template pool for a customer service chat application through clustering historical agent utterances and choosing representative utterances from each cluster,New Algorithm/ Method,New Algorithm/ Method
"We propose a two-stage training strategy. We first coarse-train the state tracking models on reading comprehension datasets, then finetune them on the target state tracking dataset",Model Proposal,Model Proposal
"we propose a multi-task setting to train the model. More specifically, our model is encouraged to explicitly ensure the two aforementioned effects of the contextual information for the task of SF.",Model Proposal,Model Proposal
"we propose the task of few-shot IC/SF, catering to domain adaptation in low resource scenarios, where there are only a handful of annotated examples available per intent and slot in the target domain",Theory Proposal,Theory Proposal
"We identify the annotation errors, inconsistencies, and ontology issues in MultiWOZ 2.1, and publish its improved version.",Model Optimization,Model Optimization
"We propose Sketch-Fill-A-R, a dialogue agent framework that can learn to generate fluent, consistent and engaging chit-chat responses. Our key motivation is the hypothesis that human-like chit-chat responses often 1) follow common conversational patterns with insertions of agent-specific traits, and 2) condition explicitly on those persona traits.",Theory Proposal,Theory Proposal
we propose a set of eight probing tasks to measure the conversational understanding of neural dialog models,Model Proposal,Model Proposal
"This paper introduces CODA-19, the COVID19 Research Aspect Dataset and presents the first outcome of our exploration in using non-expert crowds for large-scale scholarly article annotation",Dataset Creation,Dataset Creation
"In this paper, we present an information retrieval system on a corpus of scientific articles related to COVID-19.",Theory Proposal,theory Proposal
"We present COVID-Q, a dataset of 1,690 questions about COVID from 13 online sources.",Dataset Creation,Dataset Creation
we developed a Natural Language Processing (NLP) system to extract potential positive COVID-19 cases from clinical text within the Department of Veterans Affairs (VA).,New Algorithm/ Method,New Algorithm/ Method
"we present and make publicly available a high quality, ground truth text dataset of emotional responses to COVID-19.",Dataset Creation,Dataset Creation
We formulate the task of cross-lingual transfer learning for epidemiological outbreak alignment across countries.,Theory Proposal,Theory Proposal
We create a multi-label classifier based on transfer learning that can detect conspiracyladen comments. We find that misinformation videos contain a significantly higher proportion of conspiratorial comments.,Theory Proposal,Theory Proposal
"We developed NEMSI (Suendermann-Oeft et al., 2019), or the NEurological and Mental health Screening Instrument, to bridge this gap.",Algorithm/Method Optimization,Model Proposal
"We tested that for long document classification, a simple feature-based approach can work better than state-of-the-art models.",Performance Evaluation,Performance Evaluation
"We propose two novel training signals for FSL. These signals can remarkably improve the performance of existing FSL models. As these signals do not require any additional information (e.g. dependency tree or part-of-speech), they can be applied in any metric-based FSL models",Model Proposal,Model Proposal
"we investigate how an author’s and reader’s identity, as well as overall writing setup, influence how stories are written and rated. We introduce and release STORIESINTHEWILD, 1 containing 1,630 short stories written on a volunteerbased crowdsourcing platform, paired with author demographics and personality information",Theory Proposal,Theory Proposal
"we extend an existing scheme of annotation of events (Goud et al., 2019); we provide guidelines for annotation of mood of events (realis vs irrealis) and guidelines for annotation of event arguments,",Theory Proposal,Theory Proposal
" we explore how learning to extract
meaning from speech differs when learning from
CDS and ADS.",Theory Proposal,Theory Proposal
We discuss task performance on the training register as well as generalization across registers,Applications,applications
"emphasize that in order to capture the whole slot entity, it is pivotal for the model to share its parameters for all slot types in the source domains and learn the general pattern of slot entities",Model Proposal,Model Proposal
model can maintain good performances in crossdomain and low-resource settings.,Applications,Applications
DST-SC is designed with a slot connecting mechanism to establish the connection between the target slot and its source slot explicitly,Model Proposal,Model Proposal
We demonstrate that DST-SC is more effective for handling the related-slot problem and outperforms state-of-the-art baselines.,Performance Evaluation,Performance Evaluation
"We propose a recurrent knowledge interaction, which chooses knowledge dynamically among decoding steps, integrating multiple knowledge into the response coherently",Theory Proposal,Theory Proposal
"We use a knowledge-aware pointer network to do knowledge copy, which solves oov problem and keeps knowledge integrity, especially for long-text knowledge",Theory Proposal,Theory Proposal
"exploring the explicit guidance to help the variational response generator exploit persona information hidden in the nonstructured contents produced by the users, by utilizing intuitive characteristics of personalized conversations for model trainin",Model Proposal,Model Optimization
We introduce a new conversational task and demonstrate added value over traditional conversation modeling through both better control and response generatio,Model Proposal,Model Optimization
"We document the creation of a large, multiturn, multi-actor conversational dataset",Dataset Creation,Dataset Creation
" We demonstrate that by increasing model size from 117M to 8.3B parameters, human evaluations measuring preference of model gener- 68 ated samples over held out target distribution increase with respect to realism, style matching, grammar, and conversation coherency",Performance Evaluation,Performance Evaluation
"we propose a new method to tackle the above challenges, aiming to obtain a highquality pre-training model for dialogue generation.",New Algorithm/ Method,New Algorithm/ Method
we propose Iterative Rectification Network (IRN) to improve slot consistency for general NLG systems.,Theory Proposal,Theory Proposal
we will  focus on dialogues for transactions; other kinds of dialogues such as opinion sharing will have different model,Performance Evaluation,Performance Evaluation
" We explore the task of Chinese discourse parsing with a variety of strategies, and our parser achieves the state-of-the-art performance. Our robust dynamic-oracle procedure can be applied to other shift-reduce parsers",Applications,Applications
"We release the pre-trained, standalone, readyto-use parser as a resource for the research community.1",Theory Proposal,Theory Proposal
we propose a novel TransS-driven joint learning neural network framework that leverages the latent geometric structure information of argument-relation instance,Theory Proposal,Theory Proposal
"we adopt a multi-level encoder to further enrich the argument representations, which could obtain the deeper semantics of discourse",Model Optimization,Model Optimization
"We design a novel model, conditional masked prediction model with mix-attention (CoMMA), to measure the token dependency for sequence generation. •",Model Proposal,Model Proposal
"knowledge distillation and imposing source-target alignment constraint reduce the target-token dependency, and thus reduce the difficulty of training NAR models",Model Optimization,Model Optimization
"we conduct a study aimed at answering the following question: given a large annotated web-scale dataset such as Conceptual Captions (Sharma et al., 2018) in one language, and a baseline machine translation system",Dataset Creation,Dataset Creation
"We focus our study on the task of automatic image captioning, as a representative for cross-modal language generation where back-and-forth consistency cannot be leveraged in a straightforward manner",New Algorithm/ Method,New Algorithm/ Method
"we consider a new and specific setting of it, referred to as fact-based text editing, in which a draft text and several fact",Theory Proposal,Theory Proposal
"We propose the new research problem of fewshot NLG, which has great potential to benefit a wide range of real-world applications.",Theory Proposal,Theory Proposal
"To study different algorithms for our proposed problem, we create a multi-domain table-totext dataset",New Algorithm/ Method,New Algorithm/ Method
Our proposed algorithm can make use of the external resources as prior knowledge to significantly decrease human annotation effort,New Algorithm/ Method,New Algorithm/ Method
we develop SEQ2SEQ models that generate fluent and informative answer responses to conversational questions,Algorithm/Method Optimization,Model Proposal
we transform the answers from an existing QA dataset into fluent responses via data augmentation,Dataset Creation,Dataset Creation
"We propose a novel hierarchical variational framework for generating diverse QA pairs from a single context, which is, to our knowledge, the first probabilistic generative model for questionanswer pair generation",Model Proposal,Model Proposal
"We propose an InfoMax regularizer which effectively enforces the consistency between the generated QA pairs, by maximizing their mutual information. This is a novel approach in resolving consistency between QA pairs for QAG",Theory Proposal,Theory Proposal
We evaluate our framework on several benchmark datasets,Performance Evaluation,Performance Evaluation
We build a new dataset containing 7.2K passages and 81.9K questions from CoQA. It is the first dataset specially built for SQG,Dataset Creation,Dataset Creation
We perform semi-autoregressive SQG under dual-graph interaction,Applications,New Algorithm/ Method
We use extensive experiments to show that our model outperforms previous work by a substantial margin,Algorithm/Method Optimization,Algorithm/Method Optimization
We train and evaluate our approach on the largescale English paraphrase dataset,Performance Evaluation,Performance Evaluation
"we show that position embeddings provide a simple yet effective way to encode reordering information, and that the generated paraphrases exhibit high compliance with the desired reordering input.",Model Optimization,Model Optimization
"We propose a novel framework, PPVAE, for conditional text generation, which allows a separate training for a new condition without retraining the whole network",Theory Proposal,Theory Proposal
We conduct extensive experiments and analysis to verify the effectiveness of our proposed PPVAE. Our framework achieves state-of-the-art performance on conditionality in both automatic and human evaluations,Theory Proposal,Theory Proposal
We employ a simple uniform distribution of the masking ratio and name the model as u-PMLM. We prove that u-PMLM actually learns an autoregressive language model on random permutations of training sequences,Model Proposal,Model Optimization
We present a largescale analysis of generated text with a special focus on studying artifacts produced by large generative models.,Model Optimization,New Algorithm/ Method
We propose the new task of distinguishing between different fine-grained configurations 276 based on the generated text alone,Theory Proposal,Theory Proposal
"A new practical task, namely question generation from reviews without annotated instance, is proposed and it has good potential for multiple applications",Theory Proposal,Theory Proposal
A novel adaptive instance transfer and augmentation framework is proposed for handling the data lacking challenge in the task,Theory Proposal,Theory Proposal
we propose a Type Auxiliary Guiding (TAG) encoder-decoder framework,Theory Proposal,Theory Proposal
An adaptive Type-associated encoder which can summarize the information according to the node type,Applications,Applications
A Type-restricted decoder with a two-stage process to reduce the search space for the code comment generation,Theory Proposal,Theory Proposal
We propose the novel UPSA framework that addresses Unsupervised Paraphrasing by Simulated Annealing,Theory Proposal,Theory Proposal
We design a searching objective function for paraphrasing that not only considers language fluency,Model Proposal,Model Proposal
We propose a copy mechanism as one of our search actions of simulated annealing to address rare words,Theory Proposal,Theory Proposal
We achieve the state-of-the-art performance on four benchmark datasets,Dataset Creation,Dataset Creation
a model that performs segmentation and labeling jointly rather than separately,New Algorithm/ Method,New Algorithm/ Method
we introduce contextualized weak supervision to train a text classifier based on userprovided seed words.,New Algorithm/ Method,New Algorithm/ Method
We propose a novel framework enabling contextualized weak supervision for text classification,Theory Proposal,Theory Proposal
We develop an unsupervised method to automatically group word occurrences of the same word into an adaptive number of interpretations based on contextualized representations and userprovided seed information,New Algorithm/ Method,New Algorithm/ Method
We design a principled ranking mechanism to identify words that are discriminative and highly label-indicative,Model Proposal,Model Proposal
"We propose a new graph neural network for text classification, where each document is an individual graph and text level word interactions can be learned in i",Theory Proposal,Theory Proposal
"Our approach can generalise to new words that absent in training, and it is therefore applicable for inductive circumstances.",Theory Proposal,Theory Proposal
We demonstrate that our approach outperforms state-of-the-art text classification methods experimentally,New Algorithm/ Method,New Algorithm/ Method
"We propose a novel Bidirectional Adversarial Topic (BAT) model, which is, to our best knowledge, the first attempt of using bidirectional adversarial training in neural topic modeling",Model Proposal,Model Proposal
We extend BAT to incorporate the word relatedness information into the modeling process and propose the Bidirectional Adversarial Topic model with Gaussian,Model Proposal,Model Proposal
"Our multi-tasking learning model consistently outperforms the state-of-the-art model in terms of both single and multi-label classifications, sentence and document classifications, and classifications in three languages",Applications,Applications
") We encode the content words of the source sentence as a new source representation, and learn an additional content word context vector based on it to improve translation performance;",Model Optimization,Model Optimization
"we thereby make an initial attempt to measure explanation methods for NMT according to the second dimension of interpretability, which covers all target words",New Algorithm/ Method,New Algorithm/ Method
It presents an attempt at evaluating the explanation methods for neural machine translation from a new viewpoint of fidelity.,New Algorithm/ Method,New Algorithm/ Method
"It proposes a principled metric for evaluation, and to put it into practice it derives a simple yet efficient approach to approximately calculate the metric",Performance Evaluation,Performance Evaluation
It quantitatively compares several different explanation methods and evaluates their effects in terms of the proposed metric.,Performance Evaluation,Performance Evaluation
"While previous works only concentrate on manipulating the decoder, we illustrate and emphasize the importance of the encoder in NAT models and propose the encoder masking strategy to improve its training",Model Proposal,Model Proposal
We propose the consecutive masking strategy of the decoder input and the n-gram loss function to alleviate the problem of repetitive translations of NAT models.,Model Proposal,Model Proposal
We integrate the two parts above in the jointly masked sequence-to-sequence model which shows strong performance on benchmark machine translation datasets,Model Optimization,Model Optimization
" To address the large phrase table issue, we propose an attentive feature extraction model and generate phrase representation based on token representations.",Model Proposal,Model Proposal
"To the best of our knowledge, our work is the first to model phrase representations and incorporating them into the Transformer",Model Optimization,Model Proposal
"We empirically demonstrate that a simple modification made in the Transformer’s official implementation (Vaswani et al., 2018) which changes the computation order of residual connection and layer normalization can effectively ease its optimization",Performance Evaluation,Performance Evaluation
we show that recurrent models equipped with this new attention mechanism can extrapolate to longer sequences,Model Proposal,Model Optimization
"The deep MSC nets (with 72-layer encoders) bring great improvements on translation quality from increased depth, producing results that substantially better than existing systems",Theory Proposal,Theory Proposal
"we propose a novel norm-based criterion for the difficulty of a sentence, which takes advantage of both model-based and linguistically motivated difficulty features",Model Proposal,Model Proposal
"We observe that the norms of the word vectors trained on simple neural networks are expressive enough to model the two features, which are easy to obtain while possessing learning-dependent features",Model Optimization,Model Optimization
"We demonstrate the effectiveness of our proposed technique using fixed (Ma et al., 2019a) and adaptive (Zheng et al., 2019a) policies in both Chineseto-English and English-to-Chinese translation",Theory Proposal,Theory Proposal
"We compare the expressive power of rational and non-rational RNNs, distinguishing between state expressiveness (what kind and amount of information the RNN states can capture) and language expressiveness (what languages can be recognized when the state is passed to a classifier)",Theory Proposal,Theory Proposal
"A two-parameter generalization of the Zipf’s/power law is the Zipf-Mandelbrot law, where f ∝ (r + β) −α (Mandelbrot, 1965). Li et al. (2010) considered the reversed rank of rmax+1−r, where rmax is the maximum of ranking index, and proposed a two-parameter formulation of f ∝ r −α(rmax + 1 − r) β .",Theory Proposal,Theory Proposal
we propose to use dice loss in replacement of the standard cross-entropy objective for data-imbalanced NLP tasks,Theory Proposal,Theory Proposal
"we propose to replace CE or MLE with losses based on the Sørensen–Dice coefficient (Sorensen, 1948) or Tversky index (Tversky, 1977).",Performance Evaluation,Performance Evaluation
we argue that syntax can be inferred from a sample of natural language with very minimal supervision,Theory Proposal,Theory Proposal
We introduce an information theoretical definition of what constitutes syntactic information,Theory Proposal,Theory Proposal
we specifically focus on the Japanese language due to its complex and flexible word order,Theory Proposal,Theory Proposal
Discuss and validate the use of LMs as a tool for word order analysis as well as investigate the sensitivity of LMs against different word orders in non-European language,Performance Evaluation,Performance Evaluation
Find encouraging parallels between the results obtained with the LM-based method and those with the previously established method on various hypotheses of canonical word order of Japanese,New Algorithm/ Method,New Algorithm/ Method
Showcase the advantages of an LM-based method through analyzing linguistic phenomena that is difficult to explore with the previous data-driven methods,New Algorithm/ Method,New Algorithm/ Method
We study a novel and more realistic scenario of fake news detection on social media,Model Proposal,Model Proposal
"For accurate detection, we develop a new model, GCAN, to better learn the representations of user interactions, retweet propagation, and their correlation with source short text",Model Proposal,Model Proposal
"deals with fake news detection under a more realistic scenario on social media. We predict whether a source tweet story is fake, given only its short text content and its retweet sequence of users, along with user profiles",Theory Proposal,Theory Proposal
"We propose two novel GCN-based models, TPC-GCN and DTPC-GCN, for post-level controversy detection",Model Proposal,Model Proposal
"We build a Chinese dataset for controversy detection, consisting of 5,676 posts collected from Chinese Weibo, each of which are manually labeled as controversial or noncontroversial. To the best of our knowledge, this is the first released Chinese dataset for controversy detection",Dataset Creation,Dataset Creation
we propose to apply unsupervised stance detection to automatically tag a large number of Twitter users with their positions on specific topics,Theory Proposal,Theory Proposal
We use unsupervised stance detection to automatically determine the stance of Twitter users with respect to several polarizing topics,Theory Proposal,Theory Proposal
We then use distant supervision based on these discovered user stances to accurately characterize the political leaning of media outlets,Theory Proposal,Theory Proposal
We evaluate our approach by comparing its bias predictions for a number of news outlets against gold labels from Media Bias/Fact Check,Performance Evaluation,Performance Evaluation
"We propose a new and simple method for detecting usage change, that does not involve vector space alignment",New Algorithm/ Method,New Algorithm/ Method
"we use it to identify word usage changes in a variety of corpus pairs, reflecting different data division criteria",Dataset Creation,Dataset Creation
we propose a new framework for emotion-controllable response generation named Curriculum Dual Learning (CDL),Theory Proposal,Theory Proposal
"Enabling efficient DST, generating the values of a minimal subset of the slots by utilizing the previous dialogue state at each turn",Performance Evaluation,Performance Evaluation
Achieving state-of-the-art performance on MultiWOZ 2.0 and MultiWOZ 2.1 in an open vocabulary-based DST setting,Applications,Applications
Highlighting the potential of improving the state operating prediction accuracy in our proposed framework,Theory Proposal,Theory Proposal
We propose an effective model framework of five major layers on off-topic response detection task,Model Proposal,Model Proposal
"To explore the essence of our proposed model, we conduct visualization analysis from two perspectives: bi-attention visualization and semantic matching representation visualization to reveal important information on how our model works.",Model Proposal,Model Proposal
"To improve our result on unseen prompts further, we propose a novel negative sampling data augmentation method to enrich training data by shuffling words from the negative sample in off-topic response detection task",New Algorithm/ Method,New Algorithm/ Method
" To the best of our knowledge, this is the first study on applying meta-learning to retrieval-based end-to-end goal-oriented dialog systems;",Applications,Applications
we leverage the MAML algorithm to optimize a human-machine collaborative dialog system and show very promising results on the lowresource dialog tasks,New Algorithm/ Method,New Algorithm/ Method
we propose a new dataset and hope that can help bring forward the research in this area,Theory Proposal,Theory Proposal
We investigate and demonstrate the feasibility of applying lexical ontology to facilitate recognizing OOV words in the few-shot scenario,Performance Evaluation,Performance Evaluation
We propose a knowledge integration mechanism and use multi-level graph attention to model explicit lexical relations,Model Proposal,Model Proposal
"we propose Multi-Agent Dialog Policy Learning (MADPL), where the user is regarded as another dialog agent rather than a user simulator",Theory Proposal,Theory Proposal
We apply actor-critic based multi-agent reinforcement learning to learn the task-oriented dialog policy to facilitate pretraining and avoid explicitly building a user simulator,Applications,Applications
We propose Hybrid Value Network for reward decomposition to deal with the asymmetric role issue between the system agent and the user agent in the task-oriented dialog.,Theory Proposal,Theory Proposal
"We conduct in-depth experiments on the multidomain, multi-intent task-oriented dialog corpus to show the effectiveness, reasonableness and scalability of our algorithm",New Algorithm/ Method,New Algorithm/ Method
we propose to construct dialog paraphrases that consider dialog context in order to improve dialog generation quality,Theory Proposal,Theory Proposal
we propose a method to construct a response-anticipated memory to contain document information that is potentially more important in generating responses,New Algorithm/ Method,New Algorithm/ Method
"To the best of our knowledge, we are the first to approach semi-supervised dialogue policy learning",Theory Proposal,Theory Proposal
We propose a novel reward estimation approach to dialogue policy learning which relives the requirements of extensive annotations and promotes a stable learning of dialogue policy,Theory Proposal,Theory Proposal
We propose an action embedding learning technique to effectively train the reward estimator from either partially labeled or unlabeled dialogues,Theory Proposal,Theory Proposal
We conduct extensive experiments on the benchmark multi-domain dataset. Results show that our approach consistently outperforms strong baselines coupled with semi-supervised learning technique,Dataset Creation,Dataset Creation
"This paper proposes a general learning framework using the duality between NLU and NLG, where supervised and unsupervised learning can be flexibly incorporated for joint training.",Theory Proposal,Theory Proposal
This work is the first attempt to exploits the dual relationship between NLU and NLG towards unsupervised learning,Theory Proposal,Theory Proposal
The benchmark experiments demonstrate the effectiveness of the proposed framework,Theory Proposal,Theory Proposal
"a strongly-correlated, unsupervised and reference free metric is proposed for evaluating open-domain dialog systems",Theory Proposal,Theory Proposal
a thorough human quality annotation is carried out and is released1 to facilitate future benchmarking of dialog evaluation metrics,Performance Evaluation,Performance Evaluation
We introduce a group of discrete latent variables to model the underlying semantic components,Model Optimization,Model Optimization
"We also show that our model indeed learns meaningful and informative latent codes, and generates more precise and specific definitions",Model Optimization,Model Optimization
", we argue that this phenomenon is not model specific, but is due to the widely-used log loss: we demonstrate that log loss is not robust to noisy and invalid references",Theory Proposal,Theory Proposal
we show that optimizing for distinguishability is robust in the face of noisy and even invalid data,Dataset Creation,Dataset Creation
"We propose a novel graph-to-sequence model, which firstly uses the line graph to model the relationships between AMR edges",Model Proposal,Model Proposal
We integrate higher-order neighborhood information into graph encoders to model the relationships between indirectly connected nodes,Model Proposal,Model Proposal
We demonstrate that both higher-order neighborhood information and edge relations are important to graph-to-sequence modeling.,Performance Evaluation,Performance Evaluation
We propose to tackle a new challenging task: rigid formats controlled text generation. A pre-training and fine-tuning framework named SongNet is designed to address the problem,Theory Proposal,Theory Proposal
Sets of symbols are tailor-designed to improve the modeling performance. We improve the attention mechanism to impel the model to capture the future information to further enhance the sentence integrity,New Algorithm/ Method,New Algorithm/ Method
"To verify the performance of our framework SongNet, we collect two corpora, SongCi and Sonnet, in Chinese and English respectively. We design several automatic evaluation metrics and human evaluation metrics to conduct the performance evaluation",Resources,Resources
" Extensive experiments conducted on two collected corpora demonstrate that our proposed framework generates significantly better results given arbitrary formats, including the cold-start formats or even the formats newly defined by ourselves",Model Optimization,Model Optimization
"We evaluate our QG framework, Syn-QG against three QG systems on a mixture of Wikipedia and commercial text sentences outperforming existing approaches in grammaticality",Performance Evaluation,Performance Evaluation
"It allows processing each arriving short text in an online way. The online model is not only free of determining the optimal batch size, but also lends itself to handling large-scale data streams efficiently",Performance Evaluation,Performance Evaluation
"To the best of our knowledge, it is the first work to integrate semantic information for model-based online clustering, which is able to handle “term ambiguity"" problem effectively and finally support high-quality clustering",Model Proposal,Model Optimization
" Equipped with Poly Urn Scheme, the number of clusters (topics) are determined automatically in our cluster model",Model Proposal,Model Optimization
" to introduce correlations among the bits of hash codes, we propose to employ the distribution of Boltzmann machine as the variational posterior",Theory Proposal,Theory Proposal
"To obtain similarity-preserving hash codes, extensive efforts have been made to learn hash functions that can preserve the similarity information of original documents in the binary embedding space",Theory Proposal,Theory Proposal
" We formulate the interactive process of a term collection, which brings clarity to the problem to be solved",Theory Proposal,Theory Proposal
We develop a method that captures an analyst’s intention from a small number of samples with our formulation as the basis,New Algorithm/ Method,New Algorithm/ Method
we propose an automatic evaluation framework that provides a systematic assessment for interactive methods,New Algorithm/ Method,New Algorithm/ Method
"we propose a treestructured neural topic model (TSNTM), which is parameterized by neural networks and is trained using AEVB",Model Proposal,Model Proposal
"we overcome the aforementioned unsupervised gap, by using distant supervision to train neural models",Model Optimization,Model Optimization
"To overcome the lack of training data in the latter’s case, we further implement a novel weaksupervision approach using automatically generated question paraphrases, coupled with smart filtering to ensure high-quality paraphrases",Model Proposal,Model Proposal
"To the best of our knowledge, PCPR is the first work to jointly model contextualized word embeddings and pronunciation embeddings to recognize puns. Both contexts and phonological properties are beneficial to pun recognition",Model Proposal,Model Proposal
Extensive experiments are conducted on two benchmark datasets. PCPR significantly outperforms existing methods in both pun detection and pun location. In-depth analyses also verify the effectiveness and robustness of PCPR.,New Algorithm/ Method,New Algorithm/ Method
We release our implementations and pre-trained phoneme embeddings at https://github.com/ joey1993/pun-recognition to facilitate future research.,Theory Proposal,Theory Proposal
"in response to the above question, we propose a novel bidirectional language model named the Transformer-based Text Autoencoder (T-TA), which has a reduced computational complexity of O(n 2 ) when applying the model to unsupervised applications",Model Proposal,Model Proposal
"we propose a Fine-grained Interest Matching network (FIM), which is a new architecture for news recommendation that can tackle the above challenges",Theory Proposal,Theory Proposal
to the burgeoning body of research on using NLP techniques in key financial applications.,Theory Proposal,Theory Proposal
"We demonstrate that the user geolocation (especially) for the network-based methods, is largely dominated by the geographical locations of the 1- hop neighboring nodes",New Algorithm/ Method,New Algorithm/ Method
"We propose an attention-based, autoregressive model, bilingual attention language model (BALM), that not only learns the latent alignment from a parallel corpus for cross-lingual word embedding but also captures the word sequential dependency",Model Proposal,Model Proposal
"Adhering to the Matrix Language Frame theory (Myers-Scotton, 1997) and Equivalence Constraint theory (Poplack, 2000; Sankoff, 1998), we implement an objective function by jointly optimizing the cross-entropy loss as the monolingual constraint and the quasitranslation loss as the cross-lingual constraint",Theory Proposal,Theory Proposal
We show that BALM can learn from bilingual parallel data without the need for CS data,Dataset Creation,Dataset Creation
We propose a novel end-to-end trainable SpellGCN to integrate the pronunciation and shape similarities into the semantic space. Its essential components such as the specialized graph convolution and attentive combination operations are carefully investigated,Theory Proposal,Theory Proposal
We investigate the performance of SpellGCN both quantitatively and qualitatively. Experimental results indicate that our method achieves the best results on three benchmark datasets,New Algorithm/ Method,New Algorithm/ Method
proposal of the novel neural architecture Soft-Masked BERT for the CSC problem,Theory Proposal,Theory Proposal
empirical verification of the effectiveness of Soft-Masked BERT.,Theory Proposal,Theory Proposal
"We propose novel attention-based frame representation models, which take full advantage of LUs and F-to-F relations to model frames with attention schema",Model Proposal,Model Proposal
We propose a new Frame-based Sentence Representation (FSR) method that integrates multi-frame semantic information to obtain richer semantic aggregation for better sentence representation,New Algorithm/ Method,New Algorithm/ Method
Our experimental results demonstrate our proposed frame-based sentence representation (FSR) method is very effective on Machine Reading Comprehension (MRC) task,New Algorithm/ Method,New Algorithm/ Method
we propose a new procedure to increase the speed of the annotation process,Theory Proposal,Theory Proposal
"we propose a new procedure to increase the speed of the annotation process. For this, we first introduce an intermediate representation of the structured queries, which we call Operation Trees",Theory Proposal,Theory Proposal
It reduces the time needed for an annotation,Theory Proposal,Theory Proposal
we introduce a method to learn a Contextualized Sparse Representation (SPARC) for each phrase and show its effectiveness in opendomain QA under phrase retrieval setup,New Algorithm/ Method,New Algorithm/ Method
We introduce a dynamic sampling strategy that selects instances from a dataset with probability proportional to the gap between its current performance on some metric (like EM or F1 score) and measured single-task performance of the same model on that dataset,Dataset Creation,Dataset Creation
We design two novel auxiliary tasks in multitask fine-tuning to help improve the accuracy of answer span boundary detection for multilingual MRC model.,Model Proposal,Model Proposal
We propose a language-agnostic method to mine language-specific knowledge phrase from search engines. This method is lightweight and easy to scale to any language,New Algorithm/ Method,New Algorithm/ Method
"We conduct extensive experiments to prove the effectiveness of our proposed approach. In addition to an open benchmark dataset, we also create a new multilingual MRC dataset from real-scenario together with fine-grained answer type labels the in-depth impact analysis",Theory Proposal,Theory Proposal
"we propose a new framework of conversational machine reading with a novel Explicit Memory Tracker (EMT), which explicitly tracks each rule sentence to make decisions and generate follow-up questions",Theory Proposal,Theory Proposal
" A method for injecting skills into pre-trained LMs, given that automatic data generation is possible",New Algorithm/ Method,New Algorithm/ Method
"GENBERT, an architecture for pre-trained LM with generative and extractive abilities",Theory Proposal,Theory Proposal
A framework for generating numerical and textual synthetic data for numerical reasoning,Dataset Creation,Dataset Creation
We propose a new task for follow-up question identification in a conversational reading comprehension setting which supports automatic evaluation,Theory Proposal,Theory Proposal
"We present a new dataset, namely LIF, which is derived from the recently released conversational QA dataset QuAC",Dataset Creation,Dataset Creation
We propose a three-way attentive pooling network which aims to capture topic shift and topic continuity for follow-up question identification. The proposed model significantly outperforms all the baseline systems,Model Proposal,Model Proposal
we handle both constraints and multi-hop relations together for complex KBQA,Theory Proposal,Theory Proposal
We propose to modify the staged query graph generation method by allowing longer relation path,New Algorithm/ Method,New Algorithm/ Method
"We construct a diverse (in terms of lexicon usage), wide-coverage (in problem type), and publicly available1 MWP corpus, with annotations that can be used to assess the capability of different systems.",Theory Proposal,Theory Proposal
We propose a lexicon usage diversity metric to measure the diversity of an MWP corpus and use it to evaluate existing corpora,Performance Evaluation,Performance Evaluation
We show that the real performance of state-of-the-art (SOTA) systems is still far behind human performance if evaluated on a corpus that mimics a real human test,Performance Evaluation,Performance Evaluation
we add the concept of mismatch into cosine similarity by a threshold for mismatch detection and proper penalization,Theory Proposal,Theory Proposal
we attempt to deepen the understanding of cross-lingual word embeddings from the perspective of the choice of the context window through carefully designed experiments,Model Proposal,Model Proposal
We present a jointly optimized bi-encoder model (BEM) for WSD that improves performance on all-words English WSD,Model Optimization,Model Proposal
"We show that our model’s improvements come from better performance on LFS and zero-shot examples, without sacrificing accuracy on the most common senses",Algorithm/Method Optimization,New Algorithm/ Method
"We examine why our model performs well on LFS with a number of experiments, including an evaluation of the BEM in a few-shot learning setting demonstrating that the bi-encoder generalizes well from limited data",Model Optimization,Model Optimization
") humour detection (Khandelwal et al., 2018), (ii) sarcasm detection (Swami et al., 2018) and (iii) hate speech detection (Bohra et al., 2018) for HindiEnglish code-switched data",Dataset Creation,Dataset Creation
"We propose a transparent and interpretable scheme that incorporates decision tree model into co-attention networks, which not only discovers evidence for explainable claim verification but also provides interpretation for the discovery process of evidence through the decision conditions",Model Proposal,Model Proposal
"Designed co-attention networks promote the deep semantic interaction between evidence and claims, which can train DTE to obtain more powerful evidence and effectively focus on the false parts of claims",Model Proposal,Model Proposal
"Experiments on two public, widely used fake news datasets demonstrate that our DTCA achieves more excellent performance than previous state-of-the-art methods",New Algorithm/ Method,New Algorithm/ Method
We identify the task of conversational recommendation over multi-type dialogs.,Theory Proposal,Theory Proposal
"To facilitate the study of this task, we create a novel dialog dataset DuRecDial, with rich variability of dialog types and domains",Dataset Creation,Dataset Creation
We propose a conversation generation framework with a novel mixed-goal driven dialog policy mechanism.,Theory Proposal,Theory Proposal
We propose a semantic-enhanced Gaussian mixture model (SEG) for unknown intent detection by incorporating class semantic information into a Gaussian mixture distribution,Model Proposal,Model Proposal
"We explore to improve existing generalized zero-shot intent classification systems with an unknown intent identifier. To the best of our knowledge, this is the first attempt to apply unknown intent detection in this task",Model Optimization,Model Optimization
We conduct extensive experiments on three real-world datasets to validate the effectiveness of the proposed SEG model for unknown intent detection and its application in generalized zero-shot intent classification,Model Proposal,Model Proposal
"We propose the new task of expertise style transfer, which aims to facilitate communication between experts and laymen",Theory Proposal,Theory Proposal
We contribute a challenging dataset that requires knowledge-aware and structural modification techniques,Dataset Creation,Dataset Creation
"We establish benchmark performance and discuss key challenges of datasets, models and evaluation metrics",Model Optimization,Model Optimization
"we aim to overcome the above problems to automatically generate faithful texts from tables. In other words, we aim to produce the writing that a human without any external knowledge would do given the same table data as input",Performance Evaluation,Performance Evaluation
we propose Dynamic Memory Induction Networks (DMIN) to further tackle the above challenges.,Theory Proposal,Theory Proposal
"to our best knowledge, we are the first to design a hierarchical decoding process for the keyphrase generation problem",Model Proposal,Model Proposal
) we propose two novel exclusion mechanisms to avoid generating duplicated keyphrases as well as improve the generation accuracy,Theory Proposal,Theory Proposal
our method consistently outperforms all the SOTA sequential decoding methods on multiple benchmarks under the new setting,New Algorithm/ Method,New Algorithm/ Method
"With the prior hierarchy knowledge, we adopt typical structure encoders for modeling label dependencies in both top-down and bottomup manners, which has not been investigated for hierarchical text classification",Performance Evaluation,Performance Evaluation
We empirically demonstrate that both variants of HiAGM achieve consistent improvements on various datasets when using different structure encoders.,Dataset Creation,Dataset Creation
We release our code and experimental splits of Web-of-Science and NYTimes for reproducibility,Theory Proposal,Theory Proposal
"We report significant improvements for strong retrieval models on a standard benchmark collection, showing that keyphrases produced by state-of-the-art models are consistently helpful for document retrieval, even, to our surprise, when author keywords are provided",Resources,Resources
We introduce a new extrinsic evaluation framework for keyphrase generation that allows for a deeper understanding of the limitations of current models,Model Proposal,Model Optimization
"We present a derivational graph auto-encoder (DGA) that combines semantic and syntactic information with associative information from the mental lexicon, achieving very good results on MWF prediction and performing on par with a character-based LSTM at a fraction of the number of trainable parameters",Model Optimization,Model Proposal
", we present a new data set of about 1500 sentences randomly sampled from the romanized Algerian dialectal Arabic corpus of Cotterell et al.",Dataset Creation,Dataset Creation
") the Webis Gmane Email Crawl 2019, a crawl of more than 153 million emails from a wide range of mailing lists",Theory Proposal,Theory Proposal
"the Chipmunk email segmenter, a newly developed end-to-end neural model, a",Algorithm/Method Optimization,Model Optimization
"the complete preprocessing of the crawled emails using our model to construct the largest corpus of “ready-to-use” emails to date. Our corpus encompasses more than 20 years worth of discussions on a diverse set of topics, including important political and societal issues.",Model Proposal,Model Optimization
"We propose a language-neutral, fine-grained definition of cross-linguistic morphosyntactic divergences (CLMD) that allows for their extraction using a syntactically annotated, content-wordaligned parallel corpus.",Theory Proposal,Theory Proposal
data quality. Finding the right trade-off between the two is in fact a key element for an effective automatic CN generation.,Dataset Creation,Dataset Creation
To our understanding none of the collection strategies presented so far is able to fulfill this requirement.,Model Optimization,Model Proposal
KLEJ: A set of nine tasks constructed from both existing and newly introduced datasets used for the Polish language understanding evaluation,Dataset Creation,Dataset Creation
An online platform1 to evaluate and present the model results in the form of a leaderboard,Performance Evaluation,Performance Evaluation
"HerBERT: Transformer-based model for the Polish language understanding,",Model Proposal,Model Optimization
"Evaluation of several LSTM-based baselines, multilingual Transformer-based models and HerBERT.",Performance Evaluation,Performance Evaluation
We here propose an approach based on crosslingual distant supervision to generate almost arbitrarily large emotion lexicons for any target language and emotional variable,Theory Proposal,Theory Proposal
We study different ways to generate additional MT hypotheses by exploring uncertainty in NMT models,Model Optimization,Model Optimization
We devise methods to effectively explore multiple MT hypotheses to better evaluate MT output quality with existing evaluation metrics,Performance Evaluation,Performance Evaluation
"we introduce the task of Multimodal QE (MQE) for MT as an attempt to improve QE by using external sources of information, namely images",New Algorithm/ Method,New Algorithm/ Method
we propose several ways of incorporating visual information in neural-based and featurebased QE architectures,Theory Proposal,Theory Proposal
we achieve the state-of-the-art performance for such architectures in document and sentence-level QE,Applications,Applications
" this paper introduces a unique challenge, PuzzLing Machines, made up of ∼100 Rosetta Stone, a.k.a translation puzzles covering 81 languages from 39 different language families based on the Linguistic Olympiads",New Algorithm/ Method,New Algorithm/ Method
We develop an annotation scheme for marking information on materials-science experiments on scientific publications,Algorithm/Method Optimization,Algorithm/Method Optimization
" We provide a new corpus of 45 materialsscience publications in the research area of SOFCs, manually annotated by domain experts for information on experimental settings and results",Model Optimization,Model Optimization
We identify three sub-tasks of extracting experiment information and provide competitive baselines with state-of-the-art neural network approaches for them,Model Optimization,Model Optimization
We show the applicability of our findings to modeling the annotations of another materialsscience corpus,Performance Evaluation,Performance Evaluation
we present the iSarcasm dataset of tweets labelled for sarcasm by their authors,Dataset Creation,Dataset Creation
"We introduce a new approach tackling AMR parsing, following the incremental sequence-tograph transduction paradigm",New Algorithm/ Method,New Algorithm/ Method
" We present a new large-scale dataset for MDS, that is better aligned with several real-world industrial use cases",Dataset Creation,Dataset Creation
We provide an extensive analysis of the properties of this dataset,Dataset Creation,Dataset Creation
We provide empirical results for several baselines and state-of-the-art MDS methods aiming to facilitate future work on this datase,New Algorithm/ Method,New Algorithm/ Method
"We introduce a novel and efficient method which integrates the operation of attending, translating, and summarizing",Performance Evaluation,Performance Evaluation
We present three effective strategies to acquire the translation probability. It has shown that all these strategies can significantly improve the performance over the baseline,Model Proposal,Model Proposal
Experimental results demonstrate that our method can achieve remarkable improvements over baselines and achieve comparable performance with the state-of-the-art on both English-to-Chinese and Chinese-to-English cross-lingual summarization tasks,New Algorithm/ Method,New Algorithm/ Method
we examine existing strategies for the full TLS task and how well they actually work,Theory Proposal,Theory Proposal
We compare different TLS strategies side-byside using suitable evaluation metrics to provide a better picture for how well the full TLS task for news is solved so far,Performance Evaluation,Performance Evaluation
" We propose a simple addition to existing methods to significantly improve date-wise TLS, achieving new state-of-the-art results",New Algorithm/ Method,New Algorithm/ Method
We present a new TLS dataset that is larger than previous datasets and spans longer time ranges,Algorithm/Method Optimization,Algorithm/Method Optimization
", we explore improving the truthfulness in abstractive summarization on two datasets, English Gigaword and JApanese MUlti-Length Headline Corpus (JAMUL)",Dataset Creation,Dataset Creation
we analyze headlines generated by the state-of-the-art encoder-decoder model and show that the model sometimes generates unexpected words,Resources,Resources
"we conjecture that one of the reasons why the model sometimes exhibits such an untruthful behavior lies in untruthful article-headline pairs, which are used for training the model",Model Optimization,Model Optimization
"First, to better measure the semantic overlap between source documents and machine-generated summaries, we propose to use state-of-the-art contextualized text encoders and its variant SentenceBERT (SBERT) ",Model Optimization,Model Optimization
We present a guided copy mechanism based on source word centrality that is obtained by the indegree or outdegree centrality measures,Model Optimization,Model Proposal
We propose a centrality-aware attention and a guidance loss to encourage the model to pay attention to important source word,Model Proposal,Model Proposal
We achieve state-of-the-art on the public text summarization dataset,Dataset Creation,Dataset Creation
we take on the challenge of calibrating a large number of noisy self-reported user ratings to build better dialog evaluation models,Resources,Resources
"we focus on the static embedding, for it is flexible and efficient. The previous works learn the embedding from intra-sentence within a single space, which is not enough for dialog systems",Performance Evaluation,Performance Evaluation
we propose a new method to learn the conversational word embedding from human dialogue in two different vector spaces,New Algorithm/ Method,New Algorithm/ Method
We propose a few-shot CRF framework for slot tagging that computes emission score as wordlabel similarity and estimate transition score by transferring previously learned label dependencie,Algorithm/Method Optimization,Algorithm/Method Optimization
we address two key-questions that arise when training RL dialog agents with expert demonstrations:,Performance Evaluation,Performance Evaluation
"we can evaluate reasoning ability in chatbots, which can potentially allow us to bridge the gap between high performance on leader-board and unsatisfactory practical performance",Performance Evaluation,Performance Evaluation
we develop an open domain Multi-Turn dialogue reasoning dataset (MuTual) to facilitate conversation model reasoning capabilities,Model Optimization,Model Optimization
"we propose Persona Perception Bot (P 2 BOT), explicitly modeling the understanding between interlocutors with a transmitter-receiver framework",Model Proposal,Model Proposal
we formalize bridging anaphora resolution as a question answering problem and propose a QA model to solve the task,Model Proposal,Model Proposal
we explore a new method to generate a large amount of “quasi-bridging” training dataset and demonstrate its value for bridging anaphora resolution,New Algorithm/ Method,New Algorithm/ Method
we carefully carry out a series of experiments on two referential bridging corpora and provide some error analysis to verify the effectiveness of our QA model to resolve the context-dependent bridging anaphors in ISNotes,Model Optimization,Model Optimization
"proposing an MTL-based approach for dialogue coherence assessment using DAP as an auxiliary task, yielding more informative utterance representations for coherence assessment",Theory Proposal,Theory Proposal
alleviating the need for DA labels for dialogue coherence assessment during evaluations,Performance Evaluation,Performance Evaluation
"an empirical evaluation on two benchmark dialogue corpora, showing that our model substantially outperforms the state-of-theart coherence model on DailyDialog, and performs on par with it on SwitchBoard",Performance Evaluation,Performance Evaluation
" we tackle linearization decoding in a different way, by casting it as a Traveling Salesman Problem (TSP)",Theory Proposal,Theory Proposal
"we solve the dependency tree linearization task as a TSP. With the help of TreeLSTM to encode the tree and biaffine attention as a bigram language model, we can use a greedy TSP solver to linearize the tree effectively",Model Optimization,Model Optimization
" we propose the problem of Deep Question Generation (DQG), which aims to generate questions that require reasoning over multiple pieces of information in the passage",Theory Proposal,Theory Proposal
"the very first work, to the best of our knowledge, to investigate deep question generation",Performance Evaluation,Performance Evaluation
"a novel framework which combines a semantic graph with the input passage to generate deep questions, and",Model Proposal,Model Proposal
a novel graph encoder that incorporates attention into a GGNN approach,Model Proposal,Model Proposal
"We introduce a fresh perspective to revisit the relational triple extraction task with a principled problem formulation, which implies a general algorithmic framework that addresses the overlapping triple problem by design.",New Algorithm/ Method,New Algorithm/ Method
We instantiate the above framework as a novel cascade binary tagging model on top of a Transformer encoder,Model Proposal,Model Proposal
"Extensive experiments on two public datasets show that the proposed framework overwhelmingly outperforms state-of-the-art methods, achieving 17.5 and 30.2 absolute gain in F1-score on the two datasets respectively",New Algorithm/ Method,New Algorithm/ Method
"we propose the problem of Deep Question Generation (DQG), which aims to generate questions that require reasoning over multiple pieces of information in the passage.",Theory Proposal,Theory Proposal
a novel framework which combines a semantic graph with the input passage to generate deep questions,Model Proposal,Model Proposal
a novel graph encoder that incorporates attention into a GGNN approach.,Model Proposal,Model Proposal
" We introduce a fresh perspective to revisit the relational triple extraction task with a principled problem formulation, which implies a general algorithmic framework that addresses the overlapping triple problem by design",New Algorithm/ Method,New Algorithm/ Method
We instantiate the above framework as a novel cascade binary tagging model on top of a Transformer encode,Model Proposal,Model Proposal
". Extensive experiments on two public datasets show that the proposed framework overwhelmingly outperforms state-of-the-art methods, achieving 17.5 and 30.2 absolute gain in F1-score on the two datasets respectively",New Algorithm/ Method,New Algorithm/ Method
This work aims to enable rapid comprehension of a large scientific document by identifying a) the central concepts in a text,Theory Proposal,Theory Proposal
"We formulate a noisy sequence labeling problem, where the input undergoes an unknown noising process (§2.2), and we introduce a model to estimate the real error distribution",Model Proposal,Model Optimization
• We propose a data augmentation algorithm (§3.3) that directly induces noise in the input data to perform training of the neural model using a mixture of noisy and clean samples,Model Proposal,Model Proposal
"We implement a stability training method (Zheng et al., 2016), adapted to the sequence labeling scenario, which explicitly addresses the noisy input data problem by encouraging the model to produce a noise-invariant latent representation",New Algorithm/ Method,New Algorithm/ Method
We evaluate our methods on real OCR errors and misspellings against state-of-the-art baseline models,Performance Evaluation,Performance Evaluation
"To support future research in this area and to make our experiments reproducible, we make our code and data publicly available",Theory Proposal,Model Optimization
"A broad collection of labelling functions for NER, including neural models trained on various textual domains, gazetteers, heuristic functions, and document-level constraint",Model Optimization,Model Optimization
. A novel weak supervision model suited for sequence labelling tasks and able to include probabilistic labelling predictions,Model Proposal,Model Proposal
. An open-source implementation of these labelling functions and aggregation model that can scale to large datasets,Model Optimization,Dataset Creation
"Our goal in this paper is to understand which features of the input a model conditioned on relation extraction has learned as useful for the task, in order to be able to better interpret and explain model predictions",Theory Proposal,Model Optimization
"We construct a document-level graph for inference in an end-to-end fashion without relying on co-references or rules, which may not always yield optimal structures",Theory Proposal,Theory Proposal
"• We perform quantitative and qualitative analyses to compare with the state-of-the-art mod1Our model is implemented in PyTorch (Paszke et al., 2017) els in various settings",Algorithm/Method Optimization,New Algorithm/ Method
"We validate the 5k most challenging examples in the TACRED development and test sets, and provide a revised dataset2 that will improve the accuracy and reliability of future RE method evaluations",New Algorithm/ Method,New Algorithm/ Method
"We evaluate the most challenging, incorrectly predicted examples of the revised test set, and develop a set of 9 categories for common RE errors, that will also aid evaluation on other datasets",Performance Evaluation,Performance Evaluation
We verify our error hypotheses on three stateof-the-art RE models and show that two groups of ambiguous relations are responsible for most of the remaining errors and that models exploit cues in the dataset when entities are unmasked,Model Optimization,Model Optimization
"A novel MT task is proposed which can only use the ground-truth bilingual dictionary and monolingual corpora, while is independent on parallel sentences",Theory Proposal,Theory Proposal
AT is proposed as a solution to the task. AT uses the bilingual dictionary to place anchors that can encourage monolingual spaces of both languages to become closer so that translation becomes easier,Theory Proposal,Theory Proposal
" The detailed evaluation on various language pairs shows that AT, especially Bi-view AT, performs significantly better than various methods, including word-by-word translation, unsupervised MT, and cross-lingual embedding transformation",New Algorithm/ Method,New Algorithm/ Method
we perform an in-depth investigation of the suitability of self-attention models for character-level translation,Performance Evaluation,Performance Evaluation
• introducing PASCAL: an effective parameterfree local self-attention mechanism to incorporate source-side syntax into Transformer,New Algorithm/ Method,New Algorithm/ Method
"• adapting LISA (Strubell et al., 2018) to subword representations and applying it to NMT",Model Optimization,Model Optimization
"• similar to concurrent work (Pham et al., 2019), we find that modeling linguistic knowledge into the self-attention mechanism leads to better translations than other approaches",Performance Evaluation,Performance Evaluation
Increasing the capacity of multilingual NMT yields large improvements and narrows the performance gap with bilingual models. Lowresource translation benefits more from the increased capacity,Algorithm/Method Optimization,New Algorithm/ Method
Language-specific modeling and deep NMT architectures can slightly improve zero-sho,Model Optimization,Model Optimization
"Finetuning multilingual NMT with ROBT substantially reduces the proportion of offtarget translations (by ∼50%) and delivers an improvement of ∼10 BLEU in zero-shot settings, approaching the conventional pivotbased method",New Algorithm/ Method,New Algorithm/ Method
" we propose cross-mutual information (XMI), a new metric towards cross-linguistic comparability in NMT. In contrast to BLEU, this information-theoretic quantity no longer explicitly depends on language, model, and tokenization choices",Model Proposal,Model Proposal
we incorporate a language-aware Interlingua module into the Encoder-Decoder architecture,Theory Proposal,Theory Proposal
"we comparatively evaluate a number of reference-free MT evaluation metrics that build on the most recent developments in multilingual representation learning, namely cross-lingual contextualized embeddings (Devlin et al., 2019) and cross-lingual sentence encoders",Performance Evaluation,Performance Evaluation
"we use demographicallyrepresentative author samples from five languages (Dutch, English, French, German, Italian), and translate them with three commercially available machine translation systems",Model Optimization,Model Proposal
"This paper presents MMPE, the first translation environment combining standard mouse & keyboard input with touch, pen, and speech interactions for PE of MT",Model Optimization,Model Proposal
we study the trade-off between quantity and quality of data for training contextualized representations,Model Optimization,Model Proposal
" we offer initial answers to these questions, systematically assessing the syntactic generalization abilities of neural language models on 34 targeted test suites (33 adapted from previously published work, and 1 novel) covering a wide range of syntactic phenomena",Algorithm/Method Optimization,Algorithm/Method Optimization
"In this paper, we examine German number inflection, which has been identified as a crucial test case 1746 for connectionist modeling",Model Optimization,Model Optimization
"This paper aims to model suspense in computational terms, with the ultimate goal of making it deployable in NLP systems that analyze or generate narrative fiction",Resources,Resources
", we seek to address this area by building models to predict, understand, and interpret factors that could affect an article’s reading tim",Model Optimization,Model Optimization
"r, we propose a generative model for Joint natural language Understanding and Generation (JUG), which couples NLU and NLG with a latent variable representing the shared intent between natural language and formal representations",Model Proposal,Model Proposal
"we study three popular random decoding strategies—top-k, nucleus, and temperature sampling—applied to GPT-2",Theory Proposal,Theory Proposal
"• A comprehensive study of generated text detection systems’ sensitivity to model structure, decoding strategy, and excerpt length",Model Optimization,Model Optimization
"An analysis of human raters’ ability to identify machine-generated content, and how human raters differ from automatic detectors",Resources,Resources
"• This work is the first attempt that represents dialog transitions as a graph, and conducts graph grounded policy learning with RL",Resources,Resources
"we explore the possibility of directly fine-tuning a pre-trained transformer language model on a sequential representation of AMR graphs,",Model Proposal,Model Proposal
) the task of automatically updating an existing comment based on source code changes,Theory Proposal,Theory Proposal
novel approach for learning to relate edits between source code and natural language that outperforms multiple baselines on several automatic metrics and human evaluation,Model Proposal,Model Proposal
We introduce BPE-dropout – a simple and effective subword regularization method,New Algorithm/ Method,New Algorithm/ Method
We show that our method outperforms both BPE and previous subword regularization on a wide range of translation task,New Algorithm/ Method,New Algorithm/ Method
We analyze how training with BPE-dropout affects a model and show that it leads to a better quality of learned token embeddings and to a model being more robust to noisy inpu,Resources,Resources
We propose a novel seq2seq-based model to incorporate the salient clinical terms into the summarizer,Model Proposal,Model Proposal
Our model statistically significantly improves over the competitive baselines on MIMIC-CXR publicly available clinical dataset,Dataset Creation,Dataset Creation
" First, intrinsic and extrinsic hallucinations happen frequently – in more than 70% of single-sentence summaries",Theory Proposal,Theory Proposal
" the majority of hallucinations are extrinsic, which potentially could be valid abstractions that use background knowledge",Theory Proposal,Theory Proposal
"we are interested in summarizing longer narratives, i.e., screenplays, whose form and structure is far removed from newspaper articles",Theory Proposal,Theory Proposal
we develop methods for instilling knowledge about narrative structure into generic su- 1922 pervised and unsupervised summarization algorithms,New Algorithm/ Method,New Algorithm/ Method
we enable the use of supervised techniques for unsupervised summarization,Theory Proposal,Theory Proposal
The objective of the current study is to quantify the extent to which the differences between neural LMs trained on language produced by DAT patients and controls reflect known deficits in language use in this disease - in particular the loss of access to relatively infrequent terms that occurs with disease progression,Theory Proposal,Model Optimization
This paper introduces several novel probes for testing systematic generalization,New Algorithm/ Method,Theory Proposal
we investigate whether neural networks can in fact prioritize simultaneous interpretations in a human-like way,Performance Evaluation,Performance Evaluation
", we present a measure of relative word confusability based on both a language model and psychoacoustic data",Model Proposal,Model Proposal
"The paper concludes with a discussion of what our results tell us about adjective order and related issues, and a look towards future work",Theory Proposal,Theory Proposal
"The goal of this paper is to set a new direction for future task-oriented dialog system research: while retrieving the best candidate is crucial, it should be equally important to identify when the correct response (i.e. ground truth) is not present in the candidate set.",Model Proposal,Model Proposal
demonstrating that it is crucial to learn the relationship amongst the candidates as a set instead of looking at point-wise matching to solve the NOTA detection task,Performance Evaluation,Performance Evaluation
extensive experiments show that the raw output score (logits) is more informative in terms of representing model confidence than normalized probabilities after the Softmax laye,Model Proposal,Model Proposal
"we compare several ways to combine tasks designed to evaluate and improve a single conversational skill, ranging from multi-task training over several datasets to training a top-level classifier to play the role of a dialogue manage",Performance Evaluation,Performance Evaluation
we propose and explore the negative training framework to correct unwanted behaviors of a dialogue response generator,Theory Proposal,Theory Proposal
negative training is used to address the malicious response problem and the frequent response problem  in open-domain dialogue response generation,Theory Proposal,Theory Proposal
"We propose a recursive, hierarchical framebased representation that captures complex relationships between slots labels,",Theory Proposal,Theory Proposal
We formulate frame generation as a templatebased tree-decoding task,Theory Proposal,Theory Proposal
We extend (local) tree-based loss functions with global supervision optimize jointly for all loss functions end-to-end and show that this improves performance,Algorithm/Method Optimization,Applications
we study the task of SQL parse correction with natural language feedback to enable text-to-SQL systems to seek and leverage human feedback to further improve the overall performance and user experience,Algorithm/Method Optimization,Applications
we define the task of SQL parse correction with natural language feedback,Resources,Resources
We create a framework for explaining SQL parse in natural language to allow text-to-SQL users (who may have a good idea of what kind of information resides on their databases but are not proficient in SQL Hendrix et al. (1978)) to provide feedback to correct inaccurate SQL parses,Model Optimization,Model Optimization
"we demonstrate that neural network models show high calibration errors for NLP tasks such as POS, NER and QA",Performance Evaluation,Performance Evaluation
we explore using natural language explanations (Figure 1) to generate features that can augment modern neural representations,Theory Proposal,Theory Proposal
we extend the BERT training with unlabeled data in a generative adversarial setting,Dataset Creation,Dataset Creation
"we propose a novel framework, named Consensus Network (CONNET), for sequence labeling with multi-source supervisions",Theory Proposal,Theory Proposal
"we introduce a new data augmentation method, called TMix",New Algorithm/ Method,New Algorithm/ Method
"we propose MobileBERT to fill this gap. In practice, task-agnostic compression of BERT is desirable",Theory Proposal,Theory Proposal
"we seek to fill in this missing knowledge, and put this practice on more rigorous footing",Theory Proposal,Model Optimization
", we review the theory of importance sampling, providing proof that importance sampled perplexity estimates are stochastic upper bounds of the true perplexity—a previously unnoted justification for this evaluation technique",Algorithm/Method Optimization,Algorithm/Method Optimization
Our proposed neural model directly encodes the structural information from a noisy graph into the embedding space,Model Proposal,Model Proposal
"Our study suggests that collecting data and training on the target tasks is a solution worth considering, especially in production environments where accuracy is not the only considered factor, rather inference latency is often just as crucial",Dataset Creation,Dataset Creation
"First, we explain why BLI does not reflect downstream task accuracy",Theory Proposal,Theory Proposal
we introduce two post-processing methods to improve downstream models by fitting the training dictionary better,New Algorithm/ Method,New Algorithm/ Method
Method: We propose a distillation method leveraging internal representations and parameter projection that is agnostic of teacher architecture,New Algorithm/ Method,New Algorithm/ Method
"Inference: To learn model parameters, we propose stage wise optimization schedule with gradual unfreezing outperforming prior schemes",Model Proposal,Model Proposal
Experiments: We perform distillation for multilingual NER on 41 languages with massive compression and comparable performance to huge models,Algorithm/Method Optimization,Algorithm/Method Optimization
"Study: We study the influence of several factors on distillation like the availability of annotation resources for different languages, model architecture, quality of multilingual word embeddings, memory footprint and inference latency",Model Optimization,Model Proposal
we investigate the stealthiness of state-of-the-art authorship obfuscation methods,New Algorithm/ Method,New Algorithm/ Method
We study the problem of obfuscation detection for state-of-the-art authorship obfuscation method,New Algorithm/ Method,New Algorithm/ Method
We explore 160 distinct BERT and GPT-2 based neural language model architectures designed to leverage text smoothness for obfuscation detection,Model Proposal,Model Proposal
"We conduct a comprehensive evaluation of these architectures on 2 different datasets. Our best architecture achieves F1 of 0.87, on average, demonstrating the serious lack of stealthiness of existing authorship obfuscation methods",New Algorithm/ Method,New Algorithm/ Method
"we conduct experiments on BERT and RoBERTa with six GLUE datasets, showing that DeeBERT is capable of accelerating model inference by up to ∼40% with minimal model quality degradation on downstream tasks",Dataset Creation,Dataset Creation
"A robust model for HTC, with few parameters and short training time, that follows the paradigm of sequence-to-sequence learning",Model Optimization,Model Proposal
The practical application of an auxiliary (and not expensive) task that strengthens the model capacity for prediction in a bottom-up scheme,Model Optimization,Model Proposal
An exploration of strategies that take advantage of external information about textual definition of the classes,Theory Proposal,Theory Proposal
"We develop a multi-task framework that leverages inductive transfer between our main task (grading spoken language proficiency) and auxiliary objectives – predicting morphosyntactic labels, the learner’s first (‘native’) language (L1) and language modeling (LM)",New Algorithm/ Method,New Algorithm/ Method
"We investigate the performance of two encoder types for the speech scoring task: bidirectional recurrent neural networks, and bidirectional representations from transformers",Performance Evaluation,Performance Evaluation
"We analyze model performance under different conditions: namely, with and without filled pauses included in the transcriptions, with varying rates of word error in the ASR transcriptions, and according to the proficiency of the student response",Algorithm/Method Optimization,Algorithm/Method Optimization
We make our code publicly available for others to use for benchmarking and replication experiments,Performance Evaluation,Performance Evaluation
we introduce a new method for learning general-purpose vector representations of scientific documents,New Algorithm/ Method,New Algorithm/ Method
"we focus on the Search-based Pseudocode to Code (SPoC) dataset (Kulal et al., 2019) due to its challenging multiline programs and availability of input-output test suites to evaluate denotation accuracy",Performance Evaluation,Performance Evaluation
We propose the use of semantic scaffolds to add semantic constraints to models for longform language-to-code generation tasks,Model Proposal,Model Proposal
"We introduce a hierarchical beam search algorithm that incorporates these constraints, resulting in heightened efficiency, better coverage of the search space, and stronger performance when compared with the standard approach",Performance Evaluation,Performance Evaluation
We achieve a new state-of-the-art accuracy of 55.1% on the SPoC pseudocode-to-code dataset.,Dataset Creation,Dataset Creation
" We propose the OLP task, an OLP evaluation protocol, and a method to create an OLP benchmark dataset",New Algorithm/ Method,New Algorithm/ Method
"We propose a new English natural language inference dataset, INFOTABS, to study the problem of reasoning about semi-structured data",Theory Proposal,Theory Proposal
" To differentiate models’ ability to reason about the premises from their memorization of spurious patterns, we created three challenge test sets with controlled differences that employ similar reasoning as the training set",Performance Evaluation,Performance Evaluation
"We show that several existing approaches for NLI underperform on our dataset, suggesting the need for new modeling strategies",Dataset Creation,Dataset Creation
We describe a method to make MRC datasets interactive and formulate the new task as an RL problem,New Algorithm/ Method,New Algorithm/ Method
We develop a baseline agent that combines a top performing MRC model and two state-ofthe-art RL optimization algorithms and test it on iMRC task,New Algorithm/ Method,New Algorithm/ Method
We conduct experiments on several variants of iMRC and discuss the significant challenges posed by our setting,Resources,Resources
We constructed augmentation sets by applying syntactic transformations to a small number of examples from MNL,Applications,Applications
we aim to improve the generalization of the future frame prediction task by adding an auxiliary objective that serves as a regularization,Theory Proposal,Theory Proposal
we present a successful framework for fine-tuning BERT and XLNet for multimodal inpu,Theory Proposal,Model Proposal
We propose an efficient framework for finetuning BERT and XLNet for multimodal language data,Performance Evaluation,Performance Evaluation
MAG-BERT and MAG-XLNet set new state of the art in both CMU-MOSI and CMUMOSEI datasets,Dataset Creation,Dataset Creation
Our model utilizes the online temporal alignment between the input audio signal and its raw ASR transcription,Model Optimization,Model Proposal
We achieve consistent and significant improvements from learning jointly from the two modalities compared to ASR transcriptions and audio only,Applications,Applications
Our evaluation framework features a challenging real-world task with noisy inputs and realtime processing requirements,Performance Evaluation,Performance Evaluation
a fusion mechanism for audio and visual modalities based on the crossmodal scaled-dot product attention,Theory Proposal,Theory Proposal
an end to end training procedure for multimodal grounding in ASR,Theory Proposal,Theory Proposal
the use of a multiresolution training scheme for character and subword level recognition in a seq2seq setting without relying on explicit phonetic information,Theory Proposal,Theory Proposal
"We carefully curate Selected Pairs Of Learnable ImprovisatioN (SPOLIN), the first largescale corpus of yes-and dialogue acts, sourced from improv and movie dialogues",Theory Proposal,Theory Proposal
"We iteratively build a high-precision yes-and classifier, which we use to mine additional yesands from dialogue corpora with high volume but low yes-and densit",Theory Proposal,Theory Proposal
We fine-tune existing open-domain conversational models with our corpus and confirm via human evaluations that this approach improves creative grounding,Performance Evaluation,Performance Evaluation
"We release our models and data for public use, including a 64,000 turn pair extension of the core SPOLIN",Dataset Creation,Dataset Creation
" we take a step towards these goals by considering grounded dialogue involving openended discussion of a given image, a setting that is naturally fun for humans (Hu et al., 2014), and study neural conversational models for task",Model Proposal,Model Proposal
" a completely unsupervised unreferenced metric MAUDE (Metric for automatic Unreferenced dialogue evaluation), which leverages state-of-the-art pretrained language model combined with a novel discoursestructure aware text encoder and contrastive training approach",Model Proposal,Model Proposal
w We propose a neural model for generating these response timings in SDSs,Model Proposal,Model Proposal
"First, we present how our dataset is structured and our training objective",Dataset Creation,Dataset Creation
we present an overview of related work on automatic poetry generation,Theory Proposal,Theory Proposal
We propose a dual encoding method to narrow the structural gap between data encoder and text decoder for data-to-text generation,New Algorithm/ Method,New Algorithm/ Method
"We propose a neural planner, which is more efficient and effective than previous method",Performance Evaluation,Performance Evaluation
Experiments show that our method outperforms all baselines on a variety of measure,New Algorithm/ Method,New Algorithm/ Method
"In this work, we present infilling by language modeling (ILM), a simple framework which en- 2493 ables LMs to infill variable-length spans while preserving their aforementioned benefits: generation quality, efficient sampling, and conceptual simplicity",Performance Evaluation,Performance Evaluation
"We study the task of sentence infilling, which requires the model to handle inter-sentential correlation and to predict missing semantic information",Model Optimization,Model Optimization
"Our approach decouples understanding, planning, generation, and leverages existing largescale pre-trained understanding and generation models (BERT, GPT-2",Model Optimization,Model Optimization
Our model predicts a feature vector in the latent semantic space for the missing sentence and maps the vector to text,Model Optimization,Model Optimization
Our model allows the generation to be of arbitrary length,Model Proposal,Model Proposal
"Compared with directly processing text, our approach significantly reduces computation time and memory usage during training, as (after pre-computing sentence features) the sequence length is the number of sentences rather than that of tokens",Theory Proposal,Theory Proposal
we propose a model-based imitation-learning method to overcome the aforementioned issues in text-generation tasks,Model Proposal,Model Proposal
"we show that generation performance can be improved with a retrieve-edit-rerank approach that instead retrieves a set of outputs from  the training set, edits each independently",Applications,Applications
we investigate another crucial aspect of following the instructions: can a VLN agent generalize to following longer instructions by learning from shorter ones?,Performance Evaluation,Performance Evaluation
"We propose a new task, MultiMedia Event Extraction, and construct the first annotated news dataset as a benchmark to support deep analysis of cross-media events",Theory Proposal,Theory Proposal
"We develop a weakly supervised training framework, which utilizes existing singlemodal annotated corpora, and enables joint inference without cross-modal annotation",New Algorithm/ Method,Model Optimization
" Our proposed method, WASE, is the first to leverage structured representations and graph-based neural networks for multimedia common space embedding",New Algorithm/ Method,New Algorithm/ Method
" Generative models tend to do better than discriminative models of the same or similar model class at learning the full range of step types, which benefits action segmentation;",Model Optimization,Model Proposal
"Task structure affords strong, feature-agnostic baselines that are difficult for existing systems to surpass",Theory Proposal,Theory Proposal
Reporting multiple metrics is necessary to understand each model’s effectiveness for action segmentation,Resources,Resources
we explore models for building an automated Builder agent,Model Optimization,Model Proposal
we propose the MemoryAugmented Recurrent Transformer (MART) model  a transformer-based model that uses a shared encoder-decoder architecture augmented with an external memory module to enable the modeling of the previous history of video segments and sentences,Model Proposal,Model Proposal
we analyze the Visually Grounded Neural Syntax Learner (VG-NSL) model of Shi et al. (2019),Resources,Resources
In this paper we take a low-overhead approach to add limited interaction to intent classification,Theory Proposal,Model Optimization
study the effect of interaction on the system performance,Algorithm/Method Optimization,Applications
avoid the cost and complexities of interactive data collection,Dataset Creation,Dataset Creation
This work analyzes the contribution of various techniques proposed for transfer learning between languages for the task of sequence tagging,Theory Proposal,Theory Proposal
"Our work studies the corresponding pseudo-loglikelihood scores (PLLs) from MLMs (Wang and Cho, 2019), given by summing the conditional log probabilities log PMLM(wt | W\t ) of each sentence token (Shin et al., 2019",Theory Proposal,Theory Proposal
" a novel distance-based knowledge graph embedding called orthogonal transform embedding (OTE) with graph context is proposed to alleviate the 1-to-N, N-to-1 and N-to-N issues, while keeps the desired relation patterns as RotatE",Theory Proposal,Theory Proposal
"A new orthogonal transform embedding OTE, is proposed to extend RotatE from 2D space to high dimensional space, which also models symmetry/antisymmery, inversion and compositional relation patterns",Model Proposal,Model Optimization
A directed graph context modeling method is proposed to integrate knowledge graph context (including both neighboring entity nodes and relation edges) into the distance scoring function,Model Proposal,New Algorithm/ Method
"Experimental results of OTE on standard benchmark FB15k-237 and WN18RR datasets show consistent improvements over RotatE, the state of art distance-based embedding model, especially on FB15k-237 with many high in-degree nodes. On WN18RR our results achieve the new state-of-the-art performance",Dataset Creation,Dataset Creation
the classifier’s performance. We propose a simple but effective training technique called Posterior Calibrated (PosCal) training that optimizes the task objective while calibrating the posterior distribution in training,Theory Proposal,Theory Proposal
This work proposes a method for augmenting any neural decoder architecture to incorporate finegrained control states,New Algorithm/ Method,New Algorithm/ Method
"In this work, we systematically study the OOD robustness of various NLP models, such as word embeddings averages, LSTMs, pretrained Transformers, and more",Theory Proposal,Model Optimization
"Our primary contribution is robust encodings (RobEn), a framework to construct encodings that can make systems using any model robus",Theory Proposal,Theory Proposal
"Our main contributions are as follows: First, we prove that their estimator is biased under weak conditions and provide an unbiased solution",Model Optimization,Model Optimization
Our main contribution is a new framing for the sentence ordering task as a constraint solving problem,Theory Proposal,Theory Proposal
We also propose a new and simple approach for this task in this new framework,Theory Proposal,Theory Proposal
we raise a question about this trend from a different angle: “could widespread adoption of the practice of downloading publicly distributed weights pose a security threat?”,Theory Proposal,Theory Proposal
The ratio of the architecture design dimensions within a BERT encoder layer can be modified to obtain a layer with better performance. Transformer design dimensions suggested in Vaswani et al. (2017) are suboptimal,Algorithm/Method Optimization,Applications
"When we aim to obtain a computationally lighter model, using a ‘tall and narrow’ architecture provides better performance than a ‘wide and shallow’ architecture",Model Optimization,Model Optimization
The fully-connected component applied to each token separately plays a much more significant role in the top layers as compared to the bottom layer,Theory Proposal,Theory Proposal
", we propose to combine the beneficial effects of multilingual NMT with the selfsupervision from monolingual data",Theory Proposal,Theory Proposal
Our contribution is an extensive empirical evaluation of top-performing NMT systems to validate or disproof some of the above conjectures,Performance Evaluation,Performance Evaluation
"In this paper, we propose to achieve an adaptive policy via a much simpler heuristic composition of a set of wait-k policies (e.g., k = 1 ∼ 10)",Theory Proposal,Theory Proposal
". We introduce the novel structured logits mechanism, which enables the exploitation of concept relatedness as determined by LKB edges",New Algorithm/ Method,Theory Proposal
"We generalise the sense vector dot product technique from EWISE, showing that off-theshelf pretrained embeddings can be used",Theory Proposal,Theory Proposal
We show that the structured logits mechanism and the use of sense embeddings are orthogonal and can be exploited jointl,Theory Proposal,Theory Proposal
In this work we propose to learn a joint functionspecific word vector space that accounts for the study eat need food help assistance support subject art researcher science chicken scientist implementation cat chicken different roles and functions a word can take in text,Theory Proposal,Theory Proposal
"we first semi-automatically collect German specialized domain corpora to create a gold standard of term technicality across four domains: automotive, cooking, hunting and DIY",Theory Proposal,Theory Proposal
we focus on how identification of verbal metaphors can be helped by verbal MWEs,Theory Proposal,Theory Proposal
we aim to understand the bias in multilingual word embeddings,Theory Proposal,Theory Proposal
We build datasets for studying the gender bias in multilingual NLP systems,Dataset Creation,Dataset Creation
We analyze gender bias in multilingual word embeddings from both intrinsic and extrinsic perspectives,Resources,Resources
We show that simple mitigation methods can help to reduce the bias in multilingual word embeddings and discuss directions for future work to further study the problem,New Algorithm/ Method,New Algorithm/ Method
"This paper aims to investigate this issue, focused around the examination of a paper recently published at EMNLP 2019 on automatic prison term prediction by Chen et al. (2019)",Performance Evaluation,Performance Evaluation
"Propose MORPHEUS, a method for generating plausible and semantically similar adversaries by perturbing the inflections in the clean examples",New Algorithm/ Method,New Algorithm/ Method
"Demonstrate its effectiveness on multiple machine comprehension and translation models, including BERT and Transforme",Performance Evaluation,Performance Evaluation
"Show that adversarially fine-tuning the model on an adversarial training set generated via weighted random sampling is sufficient for it to acquire significant robustness, while preserving performance on clean examples",Applications,Applications
we conduct a systematic study to quantify the bias in the predicted distribution over labels,Resources,Resources
we present an evaluation framework to analyze social bias in NRE models,Theory Proposal,Model Proposal
"We create WikiGenderBias, a new dataset for evaluating gender bias in NRE systems",Dataset Creation,Dataset Creation
We present an evaluation framework to demonstrate that gender bias is exhibited in NRE model outputs,Theory Proposal,Model Proposal
We test several existing bias mitigation approaches to reducing gender bias in NRE system,Performance Evaluation,Performance Evaluation
we propose here a novel probabilistic generative model for analyzing extracted images of individual printed characters in historical document,Model Proposal,Model Optimization
. We draw from work on both deep generative modeling and interpretable models of the printing press to develop an approach that is both flexible and controllable – the later being a critical requirement for such analysis tool,New Algorithm/ Method,Model Optimization
we propose an Attentive Pooling with Learnable Norms (APLN) approach to enhance the learning of text representations,Theory Proposal,Theory Proposal
"Our contributions are a family of novel methods to compute the similarity of sequence tagging datasets, where the similarity values correlate with the change in multi-task learning performance when using one dataset as auxiliary data for training the other",New Algorithm/ Method,New Algorithm/ Method
SSANs can identify the improper word orders in both local (§4.1) and global (§4.2) ranges by learning to attend to the expected words,Theory Proposal,Theory Proposal
SSANs produce more syntactic representations (§5.1) with a better modeling of structure by selective attention,Theory Proposal,Model Proposal
The selective mechanism improves SANs by paying more attention to content words that posses semantic content and contribute to the meaning of the sentence,Theory Proposal,Theory Proposal
we design a new family of transformer models that follow a distinct sublayer ordering pattern: sandwich transformer,Model Proposal,Model Proposal
we propose a novel method that replicates the effects of the ensemble technique with a single model,Model Proposal,New Algorithm/ Method
We propose a self-training based method to leverage unlabeled data in zero-shot text classification,New Algorithm/ Method,New Algorithm/ Method
We propose a reinforcement learning framework to learn data selection policy automatically instead of using manually designed heuristic,Theory Proposal,Theory Proposal
Experimental results on both benchmarks and a real-world e-commerce dataset show that our method outperforms previous methods with a large margin of 15.4% and 5.4% on average in generalized and non-generalized ZSL respectively,New Algorithm/ Method,New Algorithm/ Method
we propose a novel graph-based multi-modal fusion encoder for NMT,Theory Proposal,Theory Proposal
" We propose a unified graph to represent the input sentence and image, where various semantic relationships between multi-modal semantic units can be captured for NMT",Theory Proposal,Theory Proposal
" We propose a graph-based multi-modal fusion encoder to conduct graph encoding based on the above graph. To the best of our knowledge, our work is the first attempt to explore multimodal graph neural network (GNN) for NMT",Theory Proposal,Theory Proposal
We conduct extensive experiments on Multi30k datasets of two language pairs,Dataset Creation,Dataset Creation
We release the code at https://github.com/ DeepLearnXMU/GMNMT,Theory Proposal,Theory Proposal
" Greedy algorithms: Wu et al. (2016) segment words by recursively selecting the longest subword prefix. Sennrich et al. (2016) recursively combine adjacent word fragments that co-occur most frequently, starting from characters",New Algorithm/ Method,New Algorithm/ Method
We view the subword segmentation of output sentences in machine translation as a latent variable that should be marginalized out to obtain the probability of the output sentence given the inpu,Theory Proposal,Theory Proposal
we take this point of view and learn cross-lingual word alignment by finding alignment between the second order statistics of the source and the target language embedding space,Theory Proposal,Theory Proposal
"We demonstrate the necessity of studying inference calibration for NMT, which can serve as useful indicators of translation error",Performance Evaluation,Performance Evaluation
"We reveal certain linguistic properties of miscalibrated predictions in NMT, which provides potentially useful information for the design of training procedures",Model Optimization,Model Optimization
"We revisit recent advances in architectures and regularization techniques, and provide variants that can boost translation performance by improving inference calibration",Model Optimization,Model Optimization
" We propose a SIGNAL model in the contex of Chinese text spam detection, to address the imbalance, efficiency, and text camouflage problems",Model Proposal,Model Optimization
") We develop an end-to-end framework, i.e., LADAN, to solve the LJP task. It addresses the confusing charges issue by mining similarities between fact descriptions and law articles as well as the distinctions between confusing law articles",New Algorithm/ Method,Model Proposal
We propose a novel graph distillation operator (GDO) to extract discriminative features for effectively distinguishing confusing law articles,Theory Proposal,Theory Proposal
We conduct extensive experiments on realworld datasets. The results show that our model outperforms all state-of-the-art methods,New Algorithm/ Method,New Algorithm/ Method
We propose a novel task of job posting generation that is defined as the conditional generation given a job description and basic company information to generate a job requiremen,Theory Proposal,Theory Proposal
A data-driven generation approach SAMA is proposed to model the complex mapping relationships and generate informative and accurate job requirements,Model Proposal,Model Optimization
We build a real-world job posting dataset and conducte extensive experiments to validate the effectiveness and superiority of our proposed approach,Theory Proposal,Theory Proposal
we propose a novel method termed as Hyperbolic and Co-graph Representation method (HyperCore,New Algorithm/ Method,New Algorithm/ Method
"We propose to connect CNN and RNN in parallel to simultaneously extract local and global contextual information, which would be complementary to each othe",Theory Proposal,Theory Proposal
"HYPERCAPS with HDR are formulated to aggregate features in a label-aware manner, and hyperbolic capsules benefits from the representation capacity of the hyperbolic space",Theory Proposal,Model Proposal
• Adaptive routing is furthermore presented to improve the scalability of HYPERCAPS and fit the large label set of MLC,Theory Proposal,Model Proposal
"Extensive experiments on four benchmark MLC datasets demonstrate the effectiveness of HYPERCAPS, especially on tail labels",Dataset Creation,Dataset Creation
"We introduce a novel task towards understanding technical support problems, which has implications on a variety of downstream application",New Algorithm/ Method,Theory Proposal
". We benchmark the performance of state of the art sequence labelling models on the task, studying their performance and limitations",Applications,Applications
"we present MOOCCube, a data repository that integrates courses, concepts, student behaviors, relationships, and external resources",Theory Proposal,Model Proposal
We propose a novel framework that stacks the Bayesian network ensembles on top of the entity-aware convolutional neural networks to bring interpretability into automatic diagnosis without compromising the accuracy of deep learning,Theory Proposal,Theory Proposal
We bring forward three variants of Bayesian Networks for disease inference that provides interpretability,Model Optimization,Model Optimization
We publish the Chinese medical knowledge graph of Gynaecology and Respiration used in our Bayesian Network for disease inference with this paper for reproducibility,Theory Proposal,Theory Proposal
This paper analyzes the persuasive effect of style in news editorial argumentation on readers with different political ideologies (conservative vs. liberal),Theory Proposal,Theory Proposal
"we propose a new end-to-end ECPE solution, called ECPE-Two-Dimensional (ECPE-2D), to represent the emotion-cause pairs by a 2D representation scheme, and integrate the emotion-cause pair representation, interaction and prediction into a joint framework",Theory Proposal,Theory Proposal
", we propose the first end-toend approach for emotion-cause pair extraction, which is a unified model to tackle this task from a ranking perspective",Model Proposal,Model Optimization
Our approach emphasizes inter-clause modeling by integrating inter-clause relationship modeling and kernel-based relative position enhanced clause pair ranking,Model Optimization,Model Proposal
"Experimental results demonstrate that our onestep approach significantly outperforms the current best-performing systems, especially in the condition of extracting multiple pairs in one document",Performance Evaluation,Performance Evaluation
"We construct a semantic-emotion heterogeneous graph from external semantic and emotion lexicons, and employ GCN to learn the semantic graph representation",Theory Proposal,Model Proposal
"We extend the standard LSTM cell with an additional memory unit, effectively integrating external knowledge into the classifier for stance detection",Theory Proposal,Theory Proposal
We conduct extensive experiments on a large dataset expanded from SemEval-2016 Task 6 to verify the effectiveness of our model for cross-domain stance detection,Dataset Creation,Dataset Creation
"We propose KinGDOM, a domain-adversarial framework that uses an external KB (ConceptNet) for unsupervised domain adaptation",Theory Proposal,Theory Proposal
"We demonstrate, through experiments, that KinGDOM surpasses state-of-the-art methods on the Amazon-reviews dataset (Blitzer et al., 2007b), thus validating our claim that external knowledge can aid the task of cross-domain SA",New Algorithm/ Method,New Algorithm/ Method
We propose the multi-channel CSAE model which distils grammatical aspects into contextualized features for improving sequential taggings,Model Proposal,Model Optimization
We contribute the LCFS-ASC which can analyze syntactical connections between words to better understand local contexts that are relevant to target aspect terms,Theory Proposal,Theory Proposal
We study the importance of the SRD by exploring the attention score in the LCF layer,Theory Proposal,Theory Proposal
", we propose to augment parallel data with three specific data augmentation methods to help improve the model’s generalization ability and reduce the overfitting risk",Model Proposal,New Algorithm/ Method
We propose an aspect-oriented tree structure by reshaping and pruning ordinary dependency trees to focus on the target aspects,Theory Proposal,Theory Proposal
We propose a new GAT model to encode the dependency relations and to establish the connections between aspects and opinion words,Model Proposal,Model Optimization
The source code of this work is released for future research,Theory Proposal,Theory Proposal
"We propose an end-to-end model for a new task PAOTE. To the best of our knowledge, it is the first end-to-end model that can jointly extract the AT/OT and the pair-wise relations between them",Model Proposal,Model Optimization
We design a novel span-based multi-task neural network for PAOTE. It can overcome the drawbacks of sequence tagging methods by taking advantage of the span-level information,New Algorithm/ Method,New Algorithm/ Method
We conduct extensive experiments and the results show that our proposed model outperforms the state-of-the-art methods,Model Proposal,New Algorithm/ Method
"r, we present a novel model for nontree argument mining",Theory Proposal,Model Proposal
", we propose a novel linearization of constituent trees tied on their span representations",Theory Proposal,Theory Proposal
"we propose three standardized experimental settings with respect to data preprocessing, post-processing, evaluation metrics, and tuning.",Theory Proposal,Theory Proposal
we propose a novel parsing approach that casts constituency parsing into a series of pointing problems,Theory Proposal,Theory Proposal
We for the first time propose second-order TreeCRF for neural dependency parsing,Theory Proposal,Theory Proposal
"We propose to batchify the inside algorithm via direct large tensor computation on GPUs, leading to very efficient TreeCRF loss computation",Performance Evaluation,Performance Evaluation
We conduct experiments on 27 datasets from 13 languages,Dataset Creation,Dataset Creation
"we explore dynamic conversation recommendation, which can model the change of user interests over time",Model Optimization,Model Proposal
"We design the model to capture user interests from both what they said in the past, and how they interacted with each other in the conversation structure",Performance Evaluation,Performance Evaluation
"• We propose a Multimodal Transformer model for the task of MNER, which empowers Transformer with a multimodal interaction module to capture the inter-modality dynamics between words and images",Model Proposal,Model Optimization
"we further design a unified architecture to incorporate a text-based entity span detection module, aiming to alleviate the bias of the visual context in MNER with the guidance of entity span predictions from this auxiliary module",Model Proposal,Model Proposal
"We model the leading political ideology (left, center or right bias) and the factuality of reporting (high, mixed, or low) of news media by modeling the textual content of what they publish vs. who reads it in social media (Twitter, Facebook, and YouTube)",Resources,Resources
" We combine a variety of information sources about the target medium, many of which have not been explored for our tasks, e.g., YouTube video channels, political bias estimates of their Facebook audience, and information from the profiles of the media followers on Twitter",Theory Proposal,Theory Proposal
"We use features from different data modalities: text, metadata, and speech. The latter two are novel for these tasks",Model Proposal,Model Proposal
We achieve sizeable improvements over the current state-of-the-art for both tasks,Applications,Applications
"We propose various ensembles to combine the different types of features, achieving further improvements, especially for bias detection",Theory Proposal,Theory Proposal
"We release the data, the features, and the code necessary to replicate our result",Dataset Creation,Dataset Creation
"we discuss some related work, followed by a description of our system’s architecture and the information sources we use",Theory Proposal,Theory Proposal
"In this paper, to obtain a new insight into the syntactic abilities of neural LMs, in particular RNNLMs, we perform a series of experiments under a different condition from the prior work",Theory Proposal,Model Optimization
We propose a novel approach to simulating various grammatical errors,Theory Proposal,Theory Proposal
We conduct a systematic analysis of the robustness of language encoders and enhance previous work by studying the performance of models on downstream tasks with various grammatical error type,Algorithm/Method Optimization,Algorithm/Method Optimization
We demonstrate the robustness of existing language encoders against grammatical errors varies,Performance Evaluation,Performance Evaluation
we suggest an analysis method which helps understand where linguistic properties are learned and represented along attention heads in transformer architectures,New Algorithm/ Method,New Algorithm/ Method
"we show that using analysis results, attention heads can be maximally utilized for performance gains during the fine-tuning process on the downstream tasks and for capturing linguistic properties",Applications,Applications
"We argue that the attention scores (rather than attention weights) are able to capture the global, absolute importance of word tokens within a corpus",Theory Proposal,Theory Proposal
We present R-MeN – a novel KG embedding model to memorize and encode the potential dependencies among relations and entities for two real applications of triple classification and search personalization,Theory Proposal,Model Proposal
"R-MeN obtains better performance than up-to-date embedding models, in which R-MeN produces new state-of-the-art results on SEARCH17 3430 for the search personalization task, and a new highest accuracy on WN11 and the secondhighest accuracy on FB13 for the triple classification task",Applications,Applications
"we propose MC-Tailor, which can tailor the resulting density of model distribution by cutting the probability mass of over-estimated zones to under-estimated zones, leading to more realistic model distribution after fine-tuning",Model Proposal,Model Optimization
"we propose a new architecture, named Multi-source Word Aligned Attention (MWA)",Theory Proposal,Theory Proposal
"we propose the Coupled-VAE approach, which couples the VAE model with a deterministic network with the same structure",Model Proposal,Model Optimization
We observe the encoder-decoder incompatibility in VAE and connect it to the posterior collapse problem,Theory Proposal,Theory Proposal
"We propose the Coupled-VAE, which helps the encoder and the decoder to learn better parameterizations of the data manifold with a coupled deterministic network, via encoder weight sharing and decoder signal matching",Model Optimization,Model Optimization
we propose a general co-teaching framework with three specific teaching strategies that cover both teaching with loss functions and teaching with data curriculum.,Theory Proposal,Theory Proposal
We present a novel inter-sentence relation extraction model that builds a labelled edge graph convolutional neural network model on a document-level graph,Model Proposal,Model Proposal
we observe that these KG embeddings treat triples independently and thus fail to cover the complex and hidden information that is inherently implicit in the local neighborhood surrounding a triple,Theory Proposal,Theory Proposal
"demonstrating that applying standard topic discovery algorithms such as NMF and LDA on aggregated
documents results in discovery of topics related
to the aggregation method.",Applications,Applications
"we propose using tree-structured semantic representations, like those used in traditional rule-based NLG systems, for better discourse-level structuring and sentence-level planning;",New Algorithm/ Method,New Algorithm/ Method
"Our work explores the possibility that limited
training data could be better exploited by including attentive collocation information.",Theory Proposal,Theory Proposal
"we compare the improvements
obtained on these two tasks in three South
Slavic languages (Slovenian, Croatian and Serbian) by moving from traditional approaches to
the neural ones.",Performance Evaluation,Performance Evaluation
"contextualized word embeddings have enhanced previous word embedding techniques by computing
word vector representations dependent on the
sentence they appear in.",Algorithm/Method Optimization,Algorithm/Method Optimization
"we propose
aspects of NLI and of the annotation task itself
that should be taken into account when designing future NLI corpora and annotation guidelines.",New Algorithm/ Method,New Algorithm/ Method
"We use distributed event representation based on the Role Factored Tensor Model
(RFTM) (Weber et al., 2018) to realize a robust
matching of event causality relations, even if these
causalities are not included in the extracted event
causality pairs",Model Proposal,Model Proposal
"we also
show that the neural encoder-decoder architecture trained to predict the minimum edit operations can produce considerably better results than the architecture trained to predict the
characters in lemmata directly as in previous
studies.",Algorithm/Method Optimization,Algorithm/Method Optimization
we propose a novel approach to estimate WER per sentence and to aggregate them to provide WER estimation per recording or for a whole test set,New Algorithm/ Method,New Algorithm/ Method
We investigate two pragmatic inferencetypes that are known to differ from classical en-tailment: scalar implicatures and presuppositions,Theory Proposal,Theory Proposal
we propose a novel approach to filter out noisysentence pairs from web-crawled corpora viapre-trained language models.,New Algorithm/ Method,New Algorithm/ Method
"we introduce a new loss func-tion for CVAEs that counteracts posterior collapse,motivated by our analysis of CVAE’s evidencelower bound objective (ELBO).",New Algorithm/ Method,New Algorithm/ Method
we make the first steps towards the adoption of MI as a measure of semantic similarity between dense word embeddings,Performance Evaluation,Performance Evaluation
"we propose two improvements to active learning for coreference resolution. First, we introduce the notion of discrete annotation. Second, we introduce mention clustering (Section 4).",Performance Evaluation,Performance Evaluation
The degree of correction of the neural GEC model can be controlled using the WER,Model Optimization,Model Optimization
"the basic ideas around how modern
NLP and NLG techniques could be applied to describe and summarize textual data in format that is
non-linguistic in nature or has some structure",Theory Proposal,Theory Proposal
"it focuses on the
copula construction and its negation and
the case-stacking phenomenon thereof.",Theory Proposal,Theory Proposal
"we show that facial expressions, voice, eyes
and body movements are the top three channels
among which the emotion is expressed,",Theory Proposal,Theory Proposal
"we present an analysis of user responses to a September 27, 2018 announcement about the quarantine policy on Reddit as
a case study of to what extent the discourse
on content moderation is polarized by users’
ideological viewpoint.",Applications,Applications
"we present an open
source desktop application for the annotation process.",Applications,Applications
we provide the introduction of a new NLP task: identifying actionable feedback in collaborative work conversations,Theory Proposal,Theory Proposal
We explore theusefulness ofinterpolatedtreebank vectors whichare computed via a weighted combination of thepredefined fixed ones,Performance Evaluation,Performance Evaluation
"we focus on investigating and re-ducing biases in the task of Natural Language In-ference (NLI), where the target of the model isto classify the relations between a pair of sen-tences into three categories: entailment, neutraland contradiction",New Algorithm/ Method,New Algorithm/ Method
"We show thatby preventing models from being overconfident onbiased examples, they are less likely to exploit thesimple cues from these examples",Model Optimization,Model Optimization
Our new system achievesstate-of-the-art resultsfor FEVERand we present anevaluation of ourmodelsincluding ablation studies (Section 6). Dataand code will be released to the community.,Model Proposal,Model Proposal
"we first present a comprehensiveanalysis of the trade-offs and limitations of multi-lingual language models at scale, inspired by re-cent monolingual scaling efforts (Liu et al., 2019).",Theory Proposal,Theory Proposal
"We present the first joint UDS parser, whichlearns to extract both graph structures and attributesfrom natural language input.",Model Proposal,Model Proposal
"we present TABERT, a pretrained LM that jointlylearns representations for NL sentences and(semi-)structured tables.",Model Proposal,Model Proposal
we present a structured tuning framework to improve models using softened constraints only at training time,Model Optimization,Model Optimization
we show that the benefits from taskadaptive pretraining increase when we have additional unlabeled data from the task distribution that has been manually curated by task designers or annotators,Performance Evaluation,Performance Evaluation
we complement previous work targeting possession existence with two attributes: duration (for how long does the possession hold true?) and co-possession (are there other possessors possessing the possessee concurrently?),Theory Proposal,Theory Proposal
"We conduct experiments across several sample selection algorithms using existing gold data for user labels and show that both of our contributions significantly improve performance on the CoNLL2012 dataset (Pradhan et al., 2012).",Algorithm/Method Optimization,Algorithm/Method Optimization
"we do so by building on Time Biased Gain (TBG, Smucker and Clarke, 2012),an IR evaluation measure that models the expected number of relevant items a user can find in a ranked list given a time budget",Performance Evaluation,Performance Evaluation
"I propose that our focus shifts towards interpreting the language together with its userdependent, contextual personal and social aspects, in order to truly process the “natural” language of a user",Theory Proposal,Theory Proposal
we conduct a corpus analysis of papers published in recent ACL venues to determine whether the community is collectively forgetting about older papers as it experiences a period of rapid growth,Dataset Creation,Dataset Creation
"We then investigate what happens when the input is an original sentence in the source language and the model’s output is also biased to be original, a scenario never observed in training.",Theory Proposal,Theory Proposal
"we introduce a procedure for generating synthetic training examples by recombining real ones, such that (2a) is assigned non-negligible probability because it already appears in the training dataset",New Algorithm/ Method,New Algorithm/ Method
"We collect a high-quality annotated dataset
for coreference resolution and information
completion in multi-turn dialogues, which
might benefit future related research.",Dataset Creation,Dataset Creation
we explore acoustic-prosodic and linguistic indicators of information concealment by collecting a unique corpus of professionals practicing for oral exams while concealing information.,Dataset Creation,Dataset Creation
"we introduce a new dataset constructed
from Wikipedia and Wikidata - DocRED annotates both named entities and relations, and is the largest humanannotated dataset for document-level RE from
plain text",Dataset Creation,Dataset Creation
introduce a challenging dataset using this representation for the weather domain,Dataset Creation,Dataset Creation
"we compare the parsing performances between Abstract Meaning Representation (AMR) and Minimal Recursion Semantics (MRS), and provide an in-depth analysis of what factors contributed to the discrepancy in their parsing accuracy.",Theory Proposal,Theory Proposal
"we propose a new type of semantic representation of Construction Grammar
that combines constructions with the vector
representations used in Distributional Semantics.",New Algorithm/ Method,New Algorithm/ Method
"we describe the creation of a data set
that contains news articles and corresponding
comments from Croatian news outlet 24 sata.",Dataset Creation,Dataset Creation
"We propose a way to effectively
construct a huge amount of silver data for the con-text reconstruction task.",New Algorithm/ Method,New Algorithm/ Method
we propose a straightforward active learning strategies for both traditional and overnight data collection that significantly reduce data annotation requirements,New Algorithm/ Method,New Algorithm/ Method
We propose a reinforcement learning based framework for medical DS. Experiment results on our dataset show that our dialogue system is able to collect symptoms from patients via conversation and improve the accuracy for automatic diagnosis,New Algorithm/ Method,New Algorithm/ Method
We use a neural generative model for slot filling on the data without word-level annotations which has received less attention,Model Proposal,Model Proposal
"We conclude that scalar annotation protocols should beadopted in future NLI-style dataset creation, whichshould enable new work in modeling a richer spaceof interesting inferences",Model Proposal,Model Proposal
we introduce the task of automati-cally generating To-Do items from email contextand meta-data to assist users with following up ontheir promised actions (also referred to as commit-ments in this work).,New Algorithm/ Method,New Algorithm/ Method
we investigate different end-to-end models tolearn label distributions on crowd-sourced dataand capture inter-subjectivity across all annota-tions,Model Proposal,Model Proposal
we focus on the task of mapping from natural language utterances to SQL queries executable in a database. Most prior work in mapping from natural language to SQL queries train and test the system on a single database,New Algorithm/ Method,New Algorithm/ Method
"We collect TVQA+, a large-scale spatiotemporal video question answering dataset, which augments the original TVQA dataset with frame-level bounding box annotations",Dataset Creation,Dataset Creation
"An innovative semantic parsing framework
based on dual learning is introduced, which
can fully exploit data (labeled or unlabeled)
and incorporate various prior-knowledge as
feedback signals. We are the first to introduce dual learning in semantic parsing to the
best of our knowledge",New Algorithm/ Method,New Algorithm/ Method
"We aim to further leverage this promising
methodology into more sophisticated and critical neural models, i.e., neural machine translation (NMT) models, since NMT models recently
play one of the central roles in the NLP research
community",Algorithm/Method Optimization,Algorithm/Method Optimization
"develop an adaptive inference scheme
for NMT ensembles by extending Bayesian Interpolation (BI) (Allauzen and Riley, 2011) to
sequence-to-sequence models.",New Algorithm/ Method,New Algorithm/ Method
"we propose an alternative to the
self-attention layer to reduce the computational
burden of a Transformer. Our layer learns its optimal context size, resulting in a network where
each attention layer gathers information on their
own context.",Algorithm/Method Optimization,Algorithm/Method Optimization
"We build on a state-of-theart convolutional neural encoder-decoder model
and incorporate cross-sentence context from previous sentences using an auxiliary encoder.",Model Proposal,Model Proposal
"our new loss function effectively reduces
gender bias in the language models during training by equalizing the probabilities of male and
female words in the output;",Theory Proposal,Theory Proposal
"A semantically-based framework for mention identification and coreference resolution
as a layer of UCCA (§3). Reusing UCCA
units as mentions facilitates efficient and consistent multilayer annotation.",New Algorithm/ Method,New Algorithm/ Method
"we propose models which generate
more diverse and interesting outputs by 
training models to focus attention on important keyphrases of the story",Model Proposal,Model Proposal
"The approach adopted consists
of two key components: fine-tuning the BERT
language representation model (Devlin et al.,
2018) and the usage of external datasets during the training process",New Algorithm/ Method,New Algorithm/ Method
"By developing a generic approach for restricting the predictions of a seq2seq model to grammatically permissible continuations, we arrive at a widely applicable technique for speeding up semantic parsing",New Algorithm/ Method,New Algorithm/ Method
"Twenty teams participated, developing a range of neural network
models, including some that successfully incorporated external data to boost performance.",Model Proposal,Model Proposal
we aim to improve topic quality with LDA by increasing the importance of named entities in the mode,Model Optimization,Model Optimization
"we proposed Se-qVAT, a variant of VAT that can be used alongwith CRF.",New Algorithm/ Method,New Algorithm/ Method
We propose two end-to-end debiasing techniquesthat can be used when the existing bias patterns areidentified.,New Algorithm/ Method,New Algorithm/ Method
we explore multilingual transfer learning to de-tect multiple frames from just the news head-line in a genuinely low-resource context wherethere are few/no frame annotations in the tar-get language,Resources,Resources
"we propose amethod for using the interpretable output of theattention layers of a neural AES for source-basedessay writing, with the goal of extracting TCs.",New Algorithm/ Method,New Algorithm/ Method
we propose two additional mea-sures for robustness which quantify the changesin translation when perturbations are added to theinput.,Algorithm/Method Optimization,Algorithm/Method Optimization
"we introduce “entity triggers,” aneffective proxy of human explanations for fa-cilitating label-efficient learning of NER mod-els.",Model Optimization,Model Optimization
"We propose a neural architecture for NER tai-lored to these three experimental setups, based onthe popular BiLSTM-CRF architecture (Lampleet al., 2016).",New Algorithm/ Method,New Algorithm/ Method
"we present a model for handling lexicalized and non-lexicalized features jointly. We use a sequence-to-sequence architecture, with different parameter sharing strategies at the encoder and decoder sides for the different features.",Model Proposal,Model Proposal
We propose to tackle this problem by leveraging richer training signals that can guide our model for preserving input information.,Model Proposal,Model Proposal
"we propose to use the recent, highly successful self-supervised pre-trained language models, e.g. Devlin et al. (2019); Liu et al. (2019) for domain data selection.",Model Proposal,Model Proposal
"we examine how pre-trained language models generalize on the Winograd Schema Challenge (WSC). Named after Terry Winograd, the WSC, in its current form, was proposed by Levesque et al. (2012) as an alternative to the Turing Test.",Performance Evaluation,Performance Evaluation
"We introduce a
novel weakly supervised learning approach, learning with partial labels, that exploits the modular
structure to reduce the supervision effort.",New Algorithm/ Method,New Algorithm/ Method
"we propose a goal-directed endto-end deep reinforcement learning framework to
resolve coreference.Specifically, we leverage the neural architecture",New Algorithm/ Method,New Algorithm/ Method
"We design a model controlling specificity of generated questions, unlike prior work on QA generation",Model Proposal,Model Proposal
We describe a neural PCFG inducer which employs context embeddings in a normalizing flow model to extend PCFG induction to use semantic and morphological information,Theory Proposal,Theory Proposal
"We propose to model reviewer biases from their review texts and rating distributions, and learn a bias-aware opinion representation.",Model Proposal,Model Proposal
"we present MoNoise, an easy-touse normalization system, consisting of an online
demo as well a more elaborate command line interface.",New Algorithm/ Method,New Algorithm/ Method
"We present a prototype coaching system, Level-Up, that applies the method to English learners’ essays in order to assist them in
writing and reading",New Algorithm/ Method,New Algorithm/ Method
"We demonstrate how fine-tuning large pretrained language models, the latest breakthrough in NLP, enhance state of the art on few
of the abusive language datasets, and show
that the domain shift isn’t considerable when
applied to abusive language datasets.",Model Optimization,Model Optimization
"introduces a theoretical
model for explaining aggressive online
comments from a sociological perspective",Theory Proposal,Theory Proposal
"Demonstrating that standard debiasing approaches like those introduced in (Bolukbasi et al., 2016) actually worsen the bias
of downstream tasks by providing a denoised
channel for communicating demographic information.",Theory Proposal,Theory Proposal
"We devise a fully data-driven neural conversational model that leverages conversation history and topic information in the response generation process through a hierarchical joint attention mechanism; making the
dialogue more diverse and engaging.",Model Proposal,Model Proposal
we propose a new model calledScriptWriterto address theproblem of script generation/selection with the helpof a narrative,Model Proposal,Model Proposal
We propose learning contextualized representa-tions that leverage both free text and informationfrom knowledge bases.,Theory Proposal,Theory Proposal
we pro-pose a framework forcategory-specificattributevalue extraction that is both efficient and effective.,Algorithm/Method Optimization,Algorithm/Method Optimization
we propose an architecture consisting of a candi-date generator and a list-wise ranker based onBERT.,New Algorithm/ Method,New Algorithm/ Method
We propose a concept normalization frame-work consisting of a candidate generator anda list-wise classifier.,New Algorithm/ Method,New Algorithm/ Method
"we therefore propose a neural framework, WMSEG, which uses memory networks to incorporate wordhood information with several popular encoder-decoder combinations for CWS.",New Algorithm/ Method,New Algorithm/ Method
"We propose Frugal Paradigm Completion, an approach that predicts all related forms in a morphological paradigm from as few manually provided forms as possible",New Algorithm/ Method,New Algorithm/ Method
"we show that competitive results on VisDial can indeed be achieved by replicating the top performing model for VQA (Yu et al., 2019b) – and effectively treating visual dialog as multiple rounds of question-answering, without taking history into account",Model Optimization,Model Optimization
"we propose a novel deep learning model for RE that uses the dependency trees to extract the syntax-based importance scores for the words, serving as a tree representation to introduce syntactic information into the models with greater generalization",Model Proposal,Model Proposal
We first propose a recurrent generative model that generates multiple keyphrases as delimiter-separated sequences. Generation diversity is further enhanced with two novel techniques by manipulating decoder hidden states,Model Proposal,Model Proposal
we suggest a new NLG task where a model is tasked with generating natural language statements that can be logically entailed by the facts in an open-domain semi-structured table.,New Algorithm/ Method,New Algorithm/ Method
"Our approach investigates a new direction for semantic parsing that models explaining a demonstration in a context, rather than mapping explanations to demonstrations.",New Algorithm/ Method,New Algorithm/ Method
"We introduce a new framework for incorporating first-order logic rules into neural network design in order to guide both training
and prediction.",New Algorithm/ Method,New Algorithm/ Method
"we use text simplification
methods to improve the understandability of clinical letters.",Performance Evaluation,Performance Evaluation
"we propose a novel attention-based graph decoder that walks an optimal path within a large commonsense KG (100K entities, 1.1M facts) to effectively prune unlikely candidate entities,",Algorithm/Method Optimization,Algorithm/Method Optimization
"We propose Two novel methods to train NLI models that
are more robust to dataset-specific artifacts.",Model Proposal,Model Proposal
"We propose a novel multi-hop retrieval approach, which we believe is imperative for
truly solving the open-domain multi-hop QA
task.",New Algorithm/ Method,New Algorithm/ Method
"describes the methods developed by team TMRLeiden for the
2019 Social Media Mining for Health Applications (SMM4H) Shared Task.",New Algorithm/ Method,New Algorithm/ Method
presents an effort to build a general purpose AMR-annotated corpus for Brazilian Portuguese by translating and adapting AMR English guidelines.,Dataset Creation,Dataset Creation
"We therefore propose Natural-language Infer-ence over Label-specific Explanations (NILE)1,which we train and evaluate on English languageexamples",Model Proposal,Model Proposal
we address this shortcoming by in-troducing a novel debiasing method that improvesmodels’ performance on the out-of-distribution ex-amples while preserves the in-distribution accu-racy,Algorithm/Method Optimization,Algorithm/Method Optimization
"we propose a novel multi-perspective cross-lingual neural framework forcode–text matching, inspired in part by a previ-ous model for monolingual text-to-text match-ing, to capture both global and local similari-ties",New Algorithm/ Method,New Algorithm/ Method
we propose a method that instead automatically learns howto weight training data through a data scorerthat is optimized to maximize performance onall test languages.,New Algorithm/ Method,New Algorithm/ Method
"We propose a novel methodthat takes the explicit ontology structure into ac-count, by amulti-level learning to rankapproachthat ranks the candidate types conditioned on thegiven entity mention.",New Algorithm/ Method,New Algorithm/ Method
we propose a novel iterative set expansion framework that leverages automatically generated class names to address the semantic drift issue,New Algorithm/ Method,New Algorithm/ Method
"we propose a method of “soft gazetteers” that incorporates ubiquitously available information from English knowledge bases, such as Wikipedia, into neural named entity recognition models through cross-lingual entity linking.",New Algorithm/ Method,New Algorithm/ Method
"we propose a joint neural framework, ONEIE, that aims to extract the globally optimal IE result as a graph from an input sentence",New Algorithm/ Method,New Algorithm/ Method
"We propose ESPRIT, a framework for commonsense reasoning about qualitative physics in natural language that generates interpretable descriptions of physical events",New Algorithm/ Method,New Algorithm/ Method
we propose to apply the VNMT framework to the state-of-the-art Transformer and introduce a more flexible approximate posterior based on normalizing flows.,Applications,Applications
"we reproduce a comparison of NMT and PBSMT in different data conditions, showing
that when following our best practices, NMT
outperforms PBSMT with as little as 100 000
words of parallel training data",Performance Evaluation,Performance Evaluation
"we propose and evaluate two strategies for automatically changing the gaps of a C-test
in order to reach a given target difficulty",Performance Evaluation,Performance Evaluation
"an approach determining the entire argument structure
based on just the relations between the four functional components of proposition across three heterogeneous corpora of which two are monological
and the other is dialogical",New Algorithm/ Method,New Algorithm/ Method
"We firstly propose to leverage both semantic and phonetic features of Chinese characters
in NLP tasks by introducing Pinyin Romanization and Wubi Input embeddings, which are easily accessible and effective in representing semantic and phonetic feature.",New Algorithm/ Method,New Algorithm/ Method
"We evaluate the proposed multiembedding scheme on Bakeoff2005 and CTB6
corpora.",Performance Evaluation,Performance Evaluation
"Conducting preliminaries experiments on
multi-label abusive language and hate speech
detection (including hate speech target, category, and level detection) in Indonesian Twitter using machine learning approaches.",Applications,Applications
"Comparing the efficacy of embedding based
debiasing techniques to manual word scrubbing techniques on both overall model performance and fairness.",Model Optimization,Model Optimization
the first study to introduce the shuffle languages to analyze the computational power of neural networks,Theory Proposal,Theory Proposal
"a metric definition, its validation with six real projects over the course of one year (2018.Q2 through 2019.Q1), as well as an extensible implementation1 and testing plan, which is described in “Metric Definition” below.",Theory Proposal,Theory Proposal
"We investigate the problem of choosing tree-bank embedding vectors for new, possibly out-of-domain, sentence",Theory Proposal,Theory Proposal
"Our evaluation demonstrates that Seq-VAT brings significant improvements in supervisedsettings, rather than marginal improvements re-ported from previous VAT-based approaches Clarket al..",Performance Evaluation,Performance Evaluation
we test rigorously the hypothesisof the utility of second-order features,Performance Evaluation,Performance Evaluation
We perform evaluation of the different strate-gies on the MWE-Aware English Dependency Cor-pus and treebanks for five additional languagesfrom the Universal Dependencies 2.2 corpus thathave frequent multi-word headless constructions,Performance Evaluation,Performance Evaluation
"We also show that neural representation sharingthrough MTL is an effective strategy, as it ac-counts for a large portion of our observed im-provements",Algorithm/Method Optimization,Algorithm/Method Optimization
"Finally, in our analysis we iden-tify general guidelines for strong cross-lingualembedding baselines, that extend to languagepairs that do not include English",Theory Proposal,Theory Proposal
"We evaluate the proposed method on the WMT2018 Parallel Corpus Filtering shared task, andon our own web-crawled Japanese-Chinese parallel corpus.",Performance Evaluation,Performance Evaluation
"we propose a more holistic analysis and evaluation setup for XSP. We propose to evaluate a semantic parsing system not only on evaluation data designed for XSP, but also on datasets that have only been studied in the SSP setting",Performance Evaluation,Performance Evaluation
We investigate the benefits of automatically learning related tasks to boost the performance of diacritic restoration,Performance Evaluation,Performance Evaluation
"We find that their improved accuracy does not actually emerge from proper visual grounding, but from regularization effects, where the model forgets the linguistic priors in the train set, thereby performing better on the test set.",Model Optimization,Model Optimization
"we advance the stateof-the-art on HTM by means of the design and evaluation of CluHTM, a novel nonprobabilistic hierarchical matrix factorization aimed at solving the specific issues of HTM",Model Proposal,Model Proposal
We propose two separate metrics to evaluate both the clustering of attested forms into paradigms and cells and the prediction of unseen inflected forms,New Algorithm/ Method,New Algorithm/ Method
a creation of a simple technique for integration of structured information into an ED system with graph embeddings,New Algorithm/ Method,New Algorithm/ Method
"The program is
used to generate a segmentation of a sentence
corpus, whose consistency is calculated and
compared with the current morpheme-based
segmentation of the same corpus",New Algorithm/ Method,New Algorithm/ Method
"As a benchmark model for scenario detection,
we present a two-stage model that combines
established methods from topic segmentation
and text classification",Model Proposal,Model Proposal
"we extract the task-specific features from the optimal layer of BERT for coreference resolution where we observe pronouns
strongly attend to the corresponding candidate entities.",New Algorithm/ Method,New Algorithm/ Method
"We demonstrate the effectiveness of NILEcompared to existing systems, in terms of la-bel and explanation accuracy",Model Optimization,Model Optimization
"we consider attribute value extrac-tion for real-world hierarchical taxonomies withthousands of product categories, where directly applying previous approaches presents limitations.",Theory Proposal,Theory Proposal
we experiment with neural networks to predict the focus of negation. Our main novelty is leveraging a scope detector to introduce the scope of negation as an additional input to the network,Algorithm/Method Optimization,Algorithm/Method Optimization
"We first investigate how end-toend neural sequence models (with pre-trained language model representations) perform on document-level role filler extraction, as well as how the length of context captured affects the models’ performance",Performance Evaluation,Performance Evaluation
"Our paper presents a concrete formalization of the PDP. Then, as a baseline for future work, we introduce a heuristic benchmark system.",New Algorithm/ Method,New Algorithm/ Method
"we propose to search for Hardware-Aware Transformers (HAT) by directly involving the latency feedback into the design loop. In this way, we do not need FLOPs as the latency proxy and can search specialized models for various hardware",Algorithm/Method Optimization,Algorithm/Method Optimization
"we deviate from learning a linear projection matrix (i.e., a parametric model) and propose a non-parametric model which translates vectors by estimating instance-specific geometric translations.",Model Proposal,Model Proposal
"The trained utterance rewriter, when integrated into two real-life online chatbots, is
shown to bring significant improvement over
the original system.",Algorithm/Method Optimization,Algorithm/Method Optimization
"to the best of our knowledge, we conduct the first systematic exploration on learning general-purpose binarized (memory-efficient)
sentence representations, and four different strategies are proposed;",New Algorithm/ Method,New Algorithm/ Method
"we study one type of domain adaptation for NER, denoted here heterogeneous tagsets.",Theory Proposal,Theory Proposal
"We adapt sequentially across two SpanishEnglish and three English-German tasks, comparing unregularized fine-tuning, L2 and Elastic Weight Consolidation",Performance Evaluation,Performance Evaluation
"we
focus on zero-shot generalization: training parsers
on a single treebank and evaluating
on a range of broad-coverage, out-of-domain treebanks , Genia the English Web Treebank",New Algorithm/ Method,New Algorithm/ Method
We demonstrate that automatic domain adaptation performs better at predicting financial outcomes than previous work based on manual domain adaptation,Performance Evaluation,Performance Evaluation
describe our work towards an unsupervised approach to classify documents into a set of categories described by a short sentence,New Algorithm/ Method,New Algorithm/ Method
"to study the
impact of both verbal and vocal features on financial markets, specifically, stock volatility.",Theory Proposal,Theory Proposal
we analyze gender stereotypes directly from writings under different metrics.,Theory Proposal,Theory Proposal
"This research
work proposes an improved framework for
social media feed pre-processing that
leverages on the combination of integrated
local knowledge bases and adapted Lesk
algorithm to facilitate pre-processing of
social media feeds",Algorithm/Method Optimization,Algorithm/Method Optimization
We have proposed an annotation scheme for EUs and its relations for ChangeMyView;,Theory Proposal,Theory Proposal
"describes the MARDY tool, an interactive annotation environment for political claims analysis in computational political science (see Pado et al. ´ (2019) for a task analysis and initial modeling results)",New Algorithm/ Method,New Algorithm/ Method
"A discussion of multilayer design principles
informed by existing semantically annotated
corpora",Theory Proposal,Theory Proposal
"diversification of the research on
hate speech by provision of a new dataset of
hate speech in another language than English,
namely Portuguese",Theory Proposal,Theory Proposal
"we want to show how to deal with
this problem for one of these languages: Polish, without having a large dedicated data set
and using solutions prepared for other NLP tasks",Theory Proposal,Theory Proposal
"a novel two-step procedure for eliciting
discourse connective insertions from na¨ıve
workers;",New Algorithm/ Method,New Algorithm/ Method
"Our focus is on
building an annotation schema which can help
writers recognise appropriate intentions in writing their Related Work section, and indicate when
these are missing.",New Algorithm/ Method,New Algorithm/ Method
"focused on
building an implementation using AllenNLP
with out-of-the-box methods to facilitate easy
operation and reuse",New Algorithm/ Method,New Algorithm/ Method
the identification of the schemes for which available tools are readily available for use;,Theory Proposal,Theory Proposal
we propose a novel simple abstract feature representation which is surprisingly effective,Theory Proposal,Theory Proposal
"We proposeUncertain Natural Language Infer-ence(UNLI), a refinement of NLI that capturesmore subtle distinctions in meaning by shiftingaway from categorical labels to the direct predic-tion of human subjective probability assessments",Theory Proposal,Theory Proposal
"We propose NILE, an NLI system which gen-erates and processes label-specific explana-tions to infer the task label, naturally provid-ing explanations for its decisions",New Algorithm/ Method,New Algorithm/ Method
we investigate the utilization ofnarratives in a special case of text generation –movie script generation.,Theory Proposal,Theory Proposal
we aim to learn associations be-tween visual attributes of fonts and the verbalcontext of the texts they are typically appliedto.,New Algorithm/ Method,New Algorithm/ Method
"We design a novel video question answering framework, Spatio-Temporal Answerer with Grounded Evidence (STAGE), to jointly localize moments, ground objects, and answer questions",New Algorithm/ Method,New Algorithm/ Method
"we study a relatively new setting in which we predict relations between entities based on the global co-occurrence statistics aggregated from a text corpus, and focus on medical relations and clinical texts in Electronic Medical Records (EMRs).",Theory Proposal,Theory Proposal
we propose the task of learning interpretable relationships from open domain facts to enrich and refine concept graphs,Theory Proposal,Theory Proposal
"we present BART, which pre-trains a model combining Bidirectional and Auto-Regressive Transformers. BART is a denoising autoencoder built with a sequence-to-sequence model that is applicable to a very wide range of end tasks",Theory Proposal,Theory Proposal
we investigate this capability of PLMs in the context of (1) negation and what we call (2) mispriming.,Theory Proposal,Theory Proposal
"we reflect on the progress of Automated Writing Evaluation (AWE), using Ellis Page’s seminal 1966 paper to frame the presentation",Theory Proposal,Theory Proposal
"We then investigate diagonal alignments with auto-encoders over real languages and randomly generated sequences, finding even randomly generated sequences as parents yield noticeable but smaller gains.",Theory Proposal,Theory Proposal
"we study the temporal aspects of text data, focusing on the information extraction task of named entity recognition in the Twitter domain.",Theory Proposal,Theory Proposal
"We further propose a novel validity reward focusing on the surface and semantics of logical forms, which is a feedback signal indicating whether the generated logical form is well-formed. It involves the prior- knowledge about structures of logical forms predefined in a domain.15:15",New Algorithm/ Method,New Algorithm/ Method
"We propose a GNN
architecture based on extending the self-attention
mechanism of the Transformer (Vaswani et al.,
2017) to make use of relations between input elements.",New Algorithm/ Method,New Algorithm/ Method
"we propose to approximate the
content information by bag-of-words (BoW) features, where we focus on style-neutral, nonstopwords. Along with traditional style-oriented
auxiliary losses, our BoW multi-task loss and
BoW adversarial loss enable better disentanglement of the style and content spaces",Algorithm/Method Optimization,Algorithm/Method Optimization
"We evaluate (i) by testing for
noise reduction in a control condition, (ii) on
large and controlled artificial data and (iii) on
a manually annotated LSC testset.",Performance Evaluation,Performance Evaluation
"In this paper, we fill a gap in the literature by proposing a thorough evaluation of Pereira et al. (2018), using previously untried evaluation metrics.",Theory Proposal,Theory Proposal
"The goal of this work is to study the use of deep neural models i.e., contextualized word represen-tation model BERT (Devlin et al., 2018) and Gated
Recurrent Units (GRU) (Cho et al., 2014) with
an attention mechanism, paired with word2vec
word embeddings and contextualized ELMo embeddings (Peters et al., 2018).",Applications,Applications
"we introduce an open-source
toolkit, NeuralClassifier4
, a neural hierarchical
multi-label text classification toolkit based on
PyTorch. It is designed for solving the hierarchical multi-label text classification problem
with effective and efficient neural models",Model Optimization,Model Optimization
"we introduce Parallax1
, a tool explicitly
designed for this task. Parallax allows the user
to use both state-of-the-art embedding analysis methods (PCA and t-SNE) and a simple yet
effective task-oriented approach where users
can explicitly define the axes of the projection
through algebraic formulae",New Algorithm/ Method,New Algorithm/ Method
"Conduct an empirical study to deepen our understanding of current datasets that focus on
different types of abusive language, which
are sometimes overlapping (racism, sexism,
hate speech, offensive language and personal
attacks). Show that our stacked Bidirectional
Long Short Term Memory architecture with
contextual attention is comparable to or out-performs state of the art approaches on all the
existing datasets.",Theory Proposal,Theory Proposal
"we investigate the
performance of a multi-dimension Capsule network as opposed to using a fixed dimension Capsule network for capturing a sentence representation and we shall discuss how well it captures
features necessary for classification of such sentences",Performance Evaluation,Performance Evaluation
"we manually reannotated the Turkish PUD treebank for consistency in the annotation. As we do not fully agree
with the annotation scheme of previous Turkish
treebanks, we had incorporated a more strict view
of the SD scheme and tried to balance the six directives of Manning’s Law",Model Optimization,Model Optimization
"Instead of only training models for each treebank separately, we use a two-stage training
process to incorporate cross-linguistic information present in other treebanks, training
multilingually over all treebanks in the first
stage and then monolingually using saved
multilingual weights in the second stage",Model Optimization,Model Optimization
we propose a hierarchical multi-scale language model in which short time-scale dependencies are encoded in the hidden state of a lower-level recurrent neural network while longer time-scale dependencies are encoded in the dynamic of the lower-level network by having a meta-learner update the weights of the lower-level neural network in an online meta-learning fashion,Model Proposal,Model Proposal
"we focus on the Multi-Genre Natural Language Inference (MNLI)dataset (Williams et al., 2018) in English, and ontwo specific kinds of dataset bias: Contradiction Word Bias (CWB) and Word Overlapping Bias (WOB)",Dataset Creation,Dataset Creation
"We propose a new neural structurefor this and name the resulting implementation s-QUASE, where “s” stands for “single;” in con-trast, we name the straightforward implementationmentioned above p-QUASEfor “paired.” Resultsshow that s-QUASEoutperforms p-QUASEsignif-icantly on3single-sentence tasks—SRL, NER, andsemantic dependency parsing (SDP)—indicatingthe importance of this distinction",New Algorithm/ Method,New Algorithm/ Method
"We evaluate our models on challenging bench-marks in textual entailment and fact verification, in-cluding HANS (Heuristic Analysis for NLI Systems)(McCoy et al., 2019b), hard NLI sets (Gururanganet al., 2018) of Stanford Natural Language Inference(SNLI) (Bowman et al., 2015) and MultiNLI (MNLI)(Williams et al., 2018), and FEVER Symmetric testset (Schuster et al., 2019).",Performance Evaluation,Performance Evaluation
"we attempt to explore the possibility of gaining plausi-ble judgments of how well an NLP model canperform under an experimental setting,with-out actually training or testing the model",Performance Evaluation,Performance Evaluation
we propose a probabilistic model whose loss function is derived from external su-pervision as regularization for the context gates.,Model Proposal,Model Proposal
"we study the problem in a real-world scenario where we crawl a large Japanese-Chinese parallel corpus from various websites and build open-domain machine translation systems between Japanese and Chinese, by filtering theweb crawled parallel corpus",New Algorithm/ Method,New Algorithm/ Method
"we focus on one class of meth-ods, subword regularization, which addresses NMT robustness without introducing any changes tothe architectures or to the training regime, solelythrough dynamic segmentation of input into sub-words (Kudo, 2018; Provilkov et al., 2019).",Theory Proposal,Theory Proposal
"we ask the question: “is it possi-ble tolearnan optimal strategy to automaticallybalance the usage of data in multilingual modeltraining?” To this effect, we propose a method that learns a language scorer that can be used through-out training to improve the model performanceonalllanguages.",New Algorithm/ Method,New Algorithm/ Method
"we focus on decoding informally romanized texts back into their original scripts. We view the task as a decipherment problem and propose an unsupervised approach, which allows us to save annotation effort since parallel data for informal transliteration does not occur naturally.",Model Proposal,Model Proposal
"we propose a neural model named TWASP for joint CWS and POS tagging following the character-based sequence labeling paradigm, where a two-way attention mechanism is used to incorporate both context feature and their corresponding syntactic knowledge for each input character.",Model Proposal,Model Proposal
"we improve the performance of diacritic restoration by building a multitask learning model (i.e. joint modeling). Multitask learning refers to models that learn more than one task at the same time, and has recently been shown to provide good solutions for a number of NLP tasks",Model Optimization,Model Optimization
we investigate how to utilize visual content for disambiguation and promoting latent space alignment in unsupervised MMT. Our model employs multimodal back-translation and features pseudo visual pivoting in which we learn a shared multilingual visual-semantic embedding space and incorporate visuallypivoted captioning as additional weak supervision,Applications,Applications
"we create PIXELHELP, a corpus that pairs English instructions with actions performed by people on a mobile UI emulator. To scale training, we decouple the language and action data by (a) annotating action phrase spans in HowTo instructions and (b) synthesizing grounded descriptions of actions for mobile user interfaces",Dataset Creation,Dataset Creation
"we propose a novel angle to further improve this representation learning, i.e., feature projection. This method projects existing features into the orthogonal space of the common features. The resulting projection is thus perpendicular to the common features and more discriminative for classification",Algorithm/Method Optimization,Algorithm/Method Optimization
"we propose a solution for “zero-shot” open-domain relation extraction from webpages with a previously unseen template, including from websites with little overlap with existing sources of knowledge for distant supervision and websites in entirely new subject verticals",Theory Proposal,Theory Proposal
we explore the sources of multilingual transfer in polyglot NER models and examine the weight structure of polyglot models compared to their monolingual counterparts. We find that polyglot models efficiently share many parameters across languages and that fine-tuning may utilize a large number of those parameters,Performance Evaluation,Performance Evaluation
"we approach event understanding as a form of linking, more akin to coreference resolution than sentence-level SRL. An event trigger evokes a set of roles regarded as latent arguments, with these implicit arguments then potentially linked to explicit mentions in the text",Applications,Applications
"we aim at adapting monolingual models to code-switched text in various tasks. Specifically, we transfer English knowledge from a pre-trained ELMo model to different code-switched language pairs (i.e., NepaliEnglish, Spanish-English, and Hindi-English) using the task of language identification",Model Optimization,Model Optimization
"We propose an unsupervised approach for sarcasm generation based on a non-sarcastic input sentence. Our method employs a retrieve-andedit framework to instantiate two major characteristics of sarcasm: reversal of valence and semantic incongruity with the context, which could include shared commonsense or world knowledge between the speaker and the listener",Algorithm/Method Optimization,Algorithm/Method Optimization
"We also introduce a new dataset (ST A C KEX) that expands beyond the only existing genre (i.e., academic writing) in keyphrase generation tasks.",Dataset Creation,Dataset Creation
We further propose two evaluation metrics tailored towards the variable-number generation.,New Algorithm/ Method,New Algorithm/ Method
We propose a novel neural CRF alignment model which not only leverages the sequential nature of sentences in parallel documents but also utilizes a neural sentence pair model to capture semantic similarity. Experiments demonstrate that our proposed approach outperforms all the previous work on monolingual sentence alignment task by more than 5 points in F1.,Model Proposal,Model Proposal
"we propose an iterative, editbased unsupervised sentence simplification approach, motivated by the shortcomings of existing work. We first design a scoring function that measures the quality of a candidate sentence based on the key characteristics of the simplification task, namely, fluency, simplicity, and meaning preservation.",New Algorithm/ Method,New Algorithm/ Method
"we present a novel approach, Conditional Masked Language Modeling (C-MLM), to enable the finetuning of BERT on target generation tasks. The finetuned BERT (teacher) is exploited as extra supervision to improve conventional Seq2Seq models (student) for better text generation performance.",Model Optimization,Model Optimization
"We propose BLEURT, a learned evaluation metric based on BERT that can model human judgments with a few thousand possibly biased training examples. A key aspect of our approach is a novel pre-training scheme that uses millions of synthetic examples to help the model generalize",Model Proposal,Model Proposal
we examine female first author percentages and the citations to their papers in Natural Language Processing (1965 to 2019). We determine aggregatelevel statistics using an existing manually curated author–gender list as well as first names strongly associated with a gender,Theory Proposal,Theory Proposal
"we make two key contributions. First, we argue that existing approaches do not adequately define comprehension; they are too unsystematic about what content is tested. Second, we present a detailed definition of comprehension—a TEMPLATE OF UNDERSTANDING—for a widely useful class of texts, namely short narratives",Theory Proposal,Theory Proposal
"We analyze the age of outgoing citations in papers published at selected ACL venues between 2010 and 2019, finding that there is indeed a tendency for recent papers to cite more recent work, but the rate at which papers older than 15 years are cited has remained relatively stable.",Theory Proposal,Theory Proposal
"We present the first statistical schwa deletion classifier for Hindi, which relies solely on the orthography as the input and outperforms previous approaches. We trained our model on a newly-compiled pronunciation lexicon extracted from various online dictionaries",Model Proposal,Model Proposal
We present Neural Machine Translation (NMT) training using document-level metrics with batch-level documents.,Model Proposal,Model Proposal
"We propose two methods to train translationese classifiers using only monolingual text, coupled with synthetic text produced by machine translation.Using only original→translationese and translationese→original training pairs, we apply techniques from zero-shot multilingual MT to enable original→original translation",New Algorithm/ Method,New Algorithm/ Method
"we propose treating gender debiasing as a domain adaptation problem, since NMT models can very quickly adapt to a new domain (Freitag and Al-Onaizan, 2016). To the best of our knowledge this work is the first to attempt NMT bias reduction by fine-tuning, rather than retraining",Model Proposal,Model Proposal
This paper presents a dynamic data selection method to multi-domain NMT. Things we do differently from previous work in mixing data are the choice of instance-level features and the employment of a multi-domain curriculum that is additionally able to denoise.,Algorithm/Method Optimization,Algorithm/Method Optimization
"we take this direction to an extreme by developing a variant of MHA without any learned parameters (Section 3). Concretely,we replace each attention head with a “hard-coded” version, which is simply a standard normal distribution centered around a particular position in the sequence (Figure 1).1",Resources,Resources
"we aim at making agents communicate with humans in natural language. Our starting point is a language model that has been trained on generic, not task-specific language data.",Model Proposal,Model Proposal
"we learn representations directly based on the relevance score inspired by the ideas from IR models. In contrast to the attention mechanism and Transformer models, we claim that the relevance patterns are as important. With proper alignment of the representation spaces of different input modalities, matching can be applied to those spaces.",Theory Proposal,Theory Proposal
"we propose GROLLA – a multitask evaluation framework for Grounded Language Learning with Attributes that expands a goal-oriented evaluation – based on the standard final task measure, with two auxiliary tasks: 1) Object attribute prediction (AP), and 2) Zero-shot evaluation (ZS).",New Algorithm/ Method,New Algorithm/ Method
"we try to leverage annotated training data from other domains. Motivated by the hypothesis that events, despite being domain/ task-specific, often occur in similar contextual patterns, we try to inject lexical domain-invariance into supervised models, improving generalization, while not overpredicting events",Theory Proposal,Theory Proposal
This work proposes an augmented pre-training for language models to improve their understanding of several important temporal phenomena. We address two kinds of reporting biases by effectively acquiring weak supervision from free-form text and utilizing it to learn multiple temporal dimensions jointly,Algorithm/Method Optimization,Algorithm/Method Optimization
"we present a unified framework, called RAT-SQL,1 for encoding relational structure in the database schema and a given question. It uses relation-aware self-attention to combine global reasoning over the schema entities and question words with structured reasoning over predefined schema relations.",New Algorithm/ Method,New Algorithm/ Method
"One of the challenges we face is to provide information on visual cues to the BERT model. We overcome this challenge by extracting densecap captions(densecaps) (Johnson et al., 2016) to provide textual information about the image objects, their properties, and interactions. This is motivated by the approaches of Visual Question Answering (VQA)",Model Optimization,Model Optimization
"we introduce a novel Information-theoretic Disentangled Embedding Learning method (IDEL) for text, based on guidance from information theory. Inspired by Variation of Information (VI), we introduce a novel information theoretic objective to measure how well the learned representations are disentangled.",New Algorithm/ Method,New Algorithm/ Method
"we investigate a simple question: can we use short-range attention for the majority of layers in the Transformer and recover the same performance? The hypothesis is that this should be possible, because many steps of reasoning will only involve short-range correlations, i.e. to piece characters together to form words or phrases. We find indeed it is possible.",Theory Proposal,Theory Proposal
"we propose to use minimal existing supervision for learning a commonsense-aware representation. Specifically, we provide the model with a supervision level identical to the test time of the Winograd challenge. For that, we introduce a self-supervised pre-training task, which only requires pair of sentences that differ in as few as one word (namely, “trigger” words).",Model Optimization,Model Optimization
"In this paper, we introduce SCIREX, a new comprehensive dataset for information extraction from scientific articles. Our dataset focuses on the task of identifying the main results of a scientific article as a tuple (Dataset, Metric, Task, Method) from raw text. It consists of three major subtasks, identifying individual entities, their document level relationships, and predicting their saliency in the document (i.e., entities that take part in the results of the article and are not merely, for example, mentioned in Related Work).",Dataset Creation,DataSet Creation
we present a simple URE approach relying only on entity types that can obtain improved performance compared to current methods,Algorithm/Method Optimization,Algorithm/Method Optimization
We address this issue by extending the recurrent units with multiple blocks along with a trainable routing network. T,Performance Evaluation,Performance Evaluation
